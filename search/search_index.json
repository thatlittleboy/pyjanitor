{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"pyjanitor pyjanitor is a Python implementation of the R package janitor , and provides a clean API for cleaning data. Quick start Installation: conda install -c conda-forge pyjanitor Check out the collection of general functions Why janitor? Originally a port of the R package, pyjanitor has evolved from a set of convenient data cleaning routines into an experiment with the method chaining paradigm. Data preprocessing usually consists of a series of steps that involve transforming raw data into an understandable/usable format. These series of steps need to be run in a certain sequence to achieve success. We take a base data file as the starting point, and perform actions on it, such as removing null/empty rows, replacing them with other values, adding/renaming/removing columns of data, filtering rows and others. More formally, these steps along with their relationships and dependencies are commonly referred to as a Directed Acyclic Graph (DAG). The pandas API has been invaluable for the Python data science ecosystem, and implements method chaining of a subset of methods as part of the API. For example, resetting indexes ( .reset_index() ), dropping null values ( .dropna() ), and more, are accomplished via the appropriate pd.DataFrame method calls. Inspired by the ease-of-use and expressiveness of the dplyr package of the R statistical language ecosystem, we have evolved pyjanitor into a language for expressing the data processing DAG for pandas users. To accomplish this, actions for which we would need to invoke imperative-style statements, can be replaced with method chains that allow one to read off the logical order of actions taken. Let us see the annotated example below. First off, here is the textual description of a data cleaning pathway: Create a DataFrame . Delete one column. Drop rows with empty values in two particular columns. Rename another two columns. Add a new column. Let's import some libraries and begin with some sample data for this example : # Libraries import numpy as np import pandas as pd import janitor # Sample Data curated for this example company_sales = { 'SalesMonth': ['Jan', 'Feb', 'Mar', 'April'], 'Company1': [150.0, 200.0, 300.0, 400.0], 'Company2': [180.0, 250.0, np.nan, 500.0], 'Company3': [400.0, 500.0, 600.0, 675.0] } In pandas code, most users might type something like this: # The Pandas Way # 1. Create a pandas DataFrame from the company_sales dictionary df = pd.DataFrame.from_dict(company_sales) # 2. Delete a column from the DataFrame. Say 'Company1' del df['Company1'] # 3. Drop rows that have empty values in columns 'Company2' and 'Company3' df = df.dropna(subset=['Company2', 'Company3']) # 4. Rename 'Company2' to 'Amazon' and 'Company3' to 'Facebook' df = df.rename( { 'Company2': 'Amazon', 'Company3': 'Facebook', }, axis=1, ) # 5. Let's add some data for another company. Say 'Google' df['Google'] = [450.0, 550.0, 800.0] # Output looks like this: # Out[15]: # SalesMonth Amazon Facebook Google # 0 Jan 180.0 400.0 450.0 # 1 Feb 250.0 500.0 550.0 # 3 April 500.0 675.0 800.0 Slightly more advanced users might take advantage of the functional API: df = ( pd.DataFrame(company_sales) .drop(columns=\"Company1\") .dropna(subset=['Company2', 'Company3']) .rename(columns={\"Company2\": \"Amazon\", \"Company3\": \"Facebook\"}) .assign(Google=[450.0, 550.0, 800.0]) ) # Output looks like this: # Out[15]: # SalesMonth Amazon Facebook Google # 0 Jan 180.0 400.0 450.0 # 1 Feb 250.0 500.0 550.0 # 3 April 500.0 675.0 800.0 With pyjanitor , we enable method chaining with method names that are explicitly named verbs , which describe the action taken. df = ( pd.DataFrame.from_dict(company_sales) .remove_columns(['Company1']) .dropna(subset=['Company2', 'Company3']) .rename_column('Company2', 'Amazon') .rename_column('Company3', 'Facebook') .add_column('Google', [450.0, 550.0, 800.0]) ) # Output looks like this: # Out[15]: # SalesMonth Amazon Facebook Google # 0 Jan 180.0 400.0 450.0 # 1 Feb 250.0 500.0 550.0 # 3 April 500.0 675.0 800.0 As such, pyjanitor's etymology has a two-fold relationship to \"cleanliness\". Firstly, it's about extending Pandas with convenient data cleaning routines. Secondly, it's about providing a cleaner, method-chaining, verb-based API for common pandas routines. Installation pyjanitor is currently installable from PyPI: pip install pyjanitor pyjanitor also can be installed by the conda package manager: conda install pyjanitor -c conda-forge pyjanitor can be installed by the pipenv environment manager too. This requires enabling prerelease dependencies: pipenv install --pre pyjanitor pyjanitor requires Python 3.6+. Functionality Current functionality includes: Cleaning columns name (multi-indexes are possible!) Removing empty rows and columns Identifying duplicate entries Encoding columns as categorical Splitting your data into features and targets (for machine learning) Adding, removing, and renaming columns Coalesce multiple columns into a single column Date conversions (from matlab, excel, unix) to Python datetime format Expand a single column that has delimited, categorical values into dummy-encoded variables Concatenating and deconcatenating columns, based on a delimiter Syntactic sugar for filtering the dataframe based on queries on a column Experimental submodules for finance, biology, chemistry, engineering, and pyspark API The idea behind the API is two-fold: Copy the R package function names, but enable Pythonic use with method chaining or pandas piping. Add other utility functions that make it easy to do data cleaning/preprocessing in pandas . Continuing with the company_sales dataframe previously used: import pandas as pd import numpy as np company_sales = { 'SalesMonth': ['Jan', 'Feb', 'Mar', 'April'], 'Company1': [150.0, 200.0, 300.0, 400.0], 'Company2': [180.0, 250.0, np.nan, 500.0], 'Company3': [400.0, 500.0, 600.0, 675.0] } As such, there are three ways to use the API. The first, and most strongly recommended one, is to use pyjanitor 's functions as if they were native to pandas. import janitor # upon import, functions are registered as part of pandas. # This cleans the column names as well as removes any duplicate rows df = pd.DataFrame.from_dict(company_sales).clean_names().remove_empty() The second is the functional API. from janitor import clean_names, remove_empty df = pd.DataFrame.from_dict(company_sales) df = clean_names(df) df = remove_empty(df) The final way is to use the pipe() method: from janitor import clean_names, remove_empty df = ( pd.DataFrame.from_dict(company_sales) .pipe(clean_names) .pipe(remove_empty) ) Contributing Follow contribution docs for a full description of the process of contributing to pyjanitor . Adding new functionality Keeping in mind the etymology of pyjanitor, contributing a new function to pyjanitor is a task that is not difficult at all. Define a function First off, you will need to define the function that expresses the data processing/cleaning routine, such that it accepts a dataframe as the first argument, and returns a modified dataframe: .. code-block:: python import pandas_flavor as pf @pf.register_dataframe_method def my_data_cleaning_function(df, arg1, arg2, ...): # Put data processing function here. return df We use pandas_flavor to register the function natively on a pandas.DataFrame . Add a test case Secondly, we ask that you contribute a test case, to ensure that it works as intended. Follow the contribution docs for further details. Feature requests If you have a feature request, please post it as an issue on the GitHub repository issue tracker. Even better, put in a PR for it! We are more than happy to guide you through the codebase so that you can put in a contribution to the codebase. Because pyjanitor is currently maintained by volunteers and has no fiscal support, any feature requests will be prioritized according to what maintainers encounter as a need in our day-to-day jobs. Please temper expectations accordingly. API Policy pyjanitor only extends or aliases the pandas API (and other dataframe APIs), but will never fix or replace them. Undesirable pandas behaviour should be reported upstream in the pandas issue tracker . We explicitly do not fix the pandas API. If at some point the pandas devs decide to take something from pyjanitor and internalize it as part of the official pandas API, then we will deprecate it from pyjanitor , while acknowledging the original contributors' contribution as part of the official deprecation record. Credits Test data for chemistry submodule can be found at Predictive Toxicology .","title":"Home"},{"location":"#pyjanitor","text":"pyjanitor is a Python implementation of the R package janitor , and provides a clean API for cleaning data.","title":"pyjanitor"},{"location":"#quick-start","text":"Installation: conda install -c conda-forge pyjanitor Check out the collection of general functions","title":"Quick start"},{"location":"#why-janitor","text":"Originally a port of the R package, pyjanitor has evolved from a set of convenient data cleaning routines into an experiment with the method chaining paradigm. Data preprocessing usually consists of a series of steps that involve transforming raw data into an understandable/usable format. These series of steps need to be run in a certain sequence to achieve success. We take a base data file as the starting point, and perform actions on it, such as removing null/empty rows, replacing them with other values, adding/renaming/removing columns of data, filtering rows and others. More formally, these steps along with their relationships and dependencies are commonly referred to as a Directed Acyclic Graph (DAG). The pandas API has been invaluable for the Python data science ecosystem, and implements method chaining of a subset of methods as part of the API. For example, resetting indexes ( .reset_index() ), dropping null values ( .dropna() ), and more, are accomplished via the appropriate pd.DataFrame method calls. Inspired by the ease-of-use and expressiveness of the dplyr package of the R statistical language ecosystem, we have evolved pyjanitor into a language for expressing the data processing DAG for pandas users. To accomplish this, actions for which we would need to invoke imperative-style statements, can be replaced with method chains that allow one to read off the logical order of actions taken. Let us see the annotated example below. First off, here is the textual description of a data cleaning pathway: Create a DataFrame . Delete one column. Drop rows with empty values in two particular columns. Rename another two columns. Add a new column. Let's import some libraries and begin with some sample data for this example : # Libraries import numpy as np import pandas as pd import janitor # Sample Data curated for this example company_sales = { 'SalesMonth': ['Jan', 'Feb', 'Mar', 'April'], 'Company1': [150.0, 200.0, 300.0, 400.0], 'Company2': [180.0, 250.0, np.nan, 500.0], 'Company3': [400.0, 500.0, 600.0, 675.0] } In pandas code, most users might type something like this: # The Pandas Way # 1. Create a pandas DataFrame from the company_sales dictionary df = pd.DataFrame.from_dict(company_sales) # 2. Delete a column from the DataFrame. Say 'Company1' del df['Company1'] # 3. Drop rows that have empty values in columns 'Company2' and 'Company3' df = df.dropna(subset=['Company2', 'Company3']) # 4. Rename 'Company2' to 'Amazon' and 'Company3' to 'Facebook' df = df.rename( { 'Company2': 'Amazon', 'Company3': 'Facebook', }, axis=1, ) # 5. Let's add some data for another company. Say 'Google' df['Google'] = [450.0, 550.0, 800.0] # Output looks like this: # Out[15]: # SalesMonth Amazon Facebook Google # 0 Jan 180.0 400.0 450.0 # 1 Feb 250.0 500.0 550.0 # 3 April 500.0 675.0 800.0 Slightly more advanced users might take advantage of the functional API: df = ( pd.DataFrame(company_sales) .drop(columns=\"Company1\") .dropna(subset=['Company2', 'Company3']) .rename(columns={\"Company2\": \"Amazon\", \"Company3\": \"Facebook\"}) .assign(Google=[450.0, 550.0, 800.0]) ) # Output looks like this: # Out[15]: # SalesMonth Amazon Facebook Google # 0 Jan 180.0 400.0 450.0 # 1 Feb 250.0 500.0 550.0 # 3 April 500.0 675.0 800.0 With pyjanitor , we enable method chaining with method names that are explicitly named verbs , which describe the action taken. df = ( pd.DataFrame.from_dict(company_sales) .remove_columns(['Company1']) .dropna(subset=['Company2', 'Company3']) .rename_column('Company2', 'Amazon') .rename_column('Company3', 'Facebook') .add_column('Google', [450.0, 550.0, 800.0]) ) # Output looks like this: # Out[15]: # SalesMonth Amazon Facebook Google # 0 Jan 180.0 400.0 450.0 # 1 Feb 250.0 500.0 550.0 # 3 April 500.0 675.0 800.0 As such, pyjanitor's etymology has a two-fold relationship to \"cleanliness\". Firstly, it's about extending Pandas with convenient data cleaning routines. Secondly, it's about providing a cleaner, method-chaining, verb-based API for common pandas routines.","title":"Why janitor?"},{"location":"#installation","text":"pyjanitor is currently installable from PyPI: pip install pyjanitor pyjanitor also can be installed by the conda package manager: conda install pyjanitor -c conda-forge pyjanitor can be installed by the pipenv environment manager too. This requires enabling prerelease dependencies: pipenv install --pre pyjanitor pyjanitor requires Python 3.6+.","title":"Installation"},{"location":"#functionality","text":"Current functionality includes: Cleaning columns name (multi-indexes are possible!) Removing empty rows and columns Identifying duplicate entries Encoding columns as categorical Splitting your data into features and targets (for machine learning) Adding, removing, and renaming columns Coalesce multiple columns into a single column Date conversions (from matlab, excel, unix) to Python datetime format Expand a single column that has delimited, categorical values into dummy-encoded variables Concatenating and deconcatenating columns, based on a delimiter Syntactic sugar for filtering the dataframe based on queries on a column Experimental submodules for finance, biology, chemistry, engineering, and pyspark","title":"Functionality"},{"location":"#api","text":"The idea behind the API is two-fold: Copy the R package function names, but enable Pythonic use with method chaining or pandas piping. Add other utility functions that make it easy to do data cleaning/preprocessing in pandas . Continuing with the company_sales dataframe previously used: import pandas as pd import numpy as np company_sales = { 'SalesMonth': ['Jan', 'Feb', 'Mar', 'April'], 'Company1': [150.0, 200.0, 300.0, 400.0], 'Company2': [180.0, 250.0, np.nan, 500.0], 'Company3': [400.0, 500.0, 600.0, 675.0] } As such, there are three ways to use the API. The first, and most strongly recommended one, is to use pyjanitor 's functions as if they were native to pandas. import janitor # upon import, functions are registered as part of pandas. # This cleans the column names as well as removes any duplicate rows df = pd.DataFrame.from_dict(company_sales).clean_names().remove_empty() The second is the functional API. from janitor import clean_names, remove_empty df = pd.DataFrame.from_dict(company_sales) df = clean_names(df) df = remove_empty(df) The final way is to use the pipe() method: from janitor import clean_names, remove_empty df = ( pd.DataFrame.from_dict(company_sales) .pipe(clean_names) .pipe(remove_empty) )","title":"API"},{"location":"#contributing","text":"Follow contribution docs for a full description of the process of contributing to pyjanitor .","title":"Contributing"},{"location":"#adding-new-functionality","text":"Keeping in mind the etymology of pyjanitor, contributing a new function to pyjanitor is a task that is not difficult at all.","title":"Adding new functionality"},{"location":"#define-a-function","text":"First off, you will need to define the function that expresses the data processing/cleaning routine, such that it accepts a dataframe as the first argument, and returns a modified dataframe: .. code-block:: python import pandas_flavor as pf @pf.register_dataframe_method def my_data_cleaning_function(df, arg1, arg2, ...): # Put data processing function here. return df We use pandas_flavor to register the function natively on a pandas.DataFrame .","title":"Define a function"},{"location":"#add-a-test-case","text":"Secondly, we ask that you contribute a test case, to ensure that it works as intended. Follow the contribution docs for further details.","title":"Add a test case"},{"location":"#feature-requests","text":"If you have a feature request, please post it as an issue on the GitHub repository issue tracker. Even better, put in a PR for it! We are more than happy to guide you through the codebase so that you can put in a contribution to the codebase. Because pyjanitor is currently maintained by volunteers and has no fiscal support, any feature requests will be prioritized according to what maintainers encounter as a need in our day-to-day jobs. Please temper expectations accordingly.","title":"Feature requests"},{"location":"#api-policy","text":"pyjanitor only extends or aliases the pandas API (and other dataframe APIs), but will never fix or replace them. Undesirable pandas behaviour should be reported upstream in the pandas issue tracker . We explicitly do not fix the pandas API. If at some point the pandas devs decide to take something from pyjanitor and internalize it as part of the official pandas API, then we will deprecate it from pyjanitor , while acknowledging the original contributors' contribution as part of the official deprecation record.","title":"API Policy"},{"location":"#credits","text":"Test data for chemistry submodule can be found at Predictive Toxicology .","title":"Credits"},{"location":"AUTHORS/","text":"Contributors Once you have added your contribution to pyjanitor , please add your name using this markdown template: [@githubname](https://github.com/githubname) | [contributions](https://github.com/pyjanitor-devs/pyjanitor/issues?q=is%3Aclosed+mentions%3Agithubname) You can copy/paste the template and replace githubname with your username. Contributions that did not leave a commit trace are indicated in bullet points below each user's username. Leads @ericmjl | contributions @szuckerman | contributions @zbarry | contributions Co-led sprint at SciPy 2019. @hectormz | contributions @jk3587 | contributions Tagged issues at SciPy 2019. @sallyhong | contributions Tagged issues at SciPy 2019. @zjpoh | contributions Started pyspark sub-module. @anzelpwj | contributions @samukweku | contributions @loganthomas | contributions Helped others with git issues at SciPy 2019. @nvamsikrishna05 | contributions Contributors @JoshuaC3 | contributions @cduvallet | contributions @shantanuo | contributions @jcvall | contributions @CWen001 | contributions @bhallaY | contributions @jekwatt | contributions Helped other sprinters with git issues at SciPy 2019. @kurtispinkney | contributions @lphk92 | contributions @jonnybazookatone | contributions @SorenFrohlich | contributions @dave-frazzetto | contributions @dsouzadaniel | contributions @Eidhagen | contributions @mdini | contributions @kimt33 | contributions @jack-kessler-88 | user no longer found @NapsterInBlue | contributions @ricky-lim | contributions @catherinedevlin | contributions @StephenSchroed | contributions @Rajat-181 | contributions @dendrondal | contributions @rahosbach | contributions @asearfos | contributions @emnemnemnem | contributions @rebeccawperry | contributions @TomMonks | contributions @benjaminjack | contributions @kulini | contributions @dwgoltra | contributions @shandou | contributions @samwalkow | contributions @portc13 | contributions @DSNortsev | contributions @qtson | contributions @keoghdata | contributions @cjmayers | contributions @gjlynx | contributions @aopisco | contributions @gaworecki5 | contributions @puruckertom | contributions @thomasjpfan | contributions @jiafengkevinchen | contributions @mralbu | contributions @Ram-N | contributions @eyaltrabelsi | contributions @gddcunh | contributions @DollofCuty | contributions @bdice | contributions @evan-anderson | contributions @smu095 | contributions @VPerrollaz | contributions @UGuntupalli | contributions @mphirke | contributions @sauln | contributions @richardqiu | contributions @MinchinWeb | contributions @BaritoneBeard | contributions @Sousa8697 | contributions @MollyCroke | contributions @ericclessantostv | contributions @fireddd | contributions @Zeroto521 | contributions","title":"Authors"},{"location":"AUTHORS/#contributors","text":"Once you have added your contribution to pyjanitor , please add your name using this markdown template: [@githubname](https://github.com/githubname) | [contributions](https://github.com/pyjanitor-devs/pyjanitor/issues?q=is%3Aclosed+mentions%3Agithubname) You can copy/paste the template and replace githubname with your username. Contributions that did not leave a commit trace are indicated in bullet points below each user's username.","title":"Contributors"},{"location":"AUTHORS/#leads","text":"@ericmjl | contributions @szuckerman | contributions @zbarry | contributions Co-led sprint at SciPy 2019. @hectormz | contributions @jk3587 | contributions Tagged issues at SciPy 2019. @sallyhong | contributions Tagged issues at SciPy 2019. @zjpoh | contributions Started pyspark sub-module. @anzelpwj | contributions @samukweku | contributions @loganthomas | contributions Helped others with git issues at SciPy 2019. @nvamsikrishna05 | contributions","title":"Leads"},{"location":"AUTHORS/#contributors_1","text":"@JoshuaC3 | contributions @cduvallet | contributions @shantanuo | contributions @jcvall | contributions @CWen001 | contributions @bhallaY | contributions @jekwatt | contributions Helped other sprinters with git issues at SciPy 2019. @kurtispinkney | contributions @lphk92 | contributions @jonnybazookatone | contributions @SorenFrohlich | contributions @dave-frazzetto | contributions @dsouzadaniel | contributions @Eidhagen | contributions @mdini | contributions @kimt33 | contributions @jack-kessler-88 | user no longer found @NapsterInBlue | contributions @ricky-lim | contributions @catherinedevlin | contributions @StephenSchroed | contributions @Rajat-181 | contributions @dendrondal | contributions @rahosbach | contributions @asearfos | contributions @emnemnemnem | contributions @rebeccawperry | contributions @TomMonks | contributions @benjaminjack | contributions @kulini | contributions @dwgoltra | contributions @shandou | contributions @samwalkow | contributions @portc13 | contributions @DSNortsev | contributions @qtson | contributions @keoghdata | contributions @cjmayers | contributions @gjlynx | contributions @aopisco | contributions @gaworecki5 | contributions @puruckertom | contributions @thomasjpfan | contributions @jiafengkevinchen | contributions @mralbu | contributions @Ram-N | contributions @eyaltrabelsi | contributions @gddcunh | contributions @DollofCuty | contributions @bdice | contributions @evan-anderson | contributions @smu095 | contributions @VPerrollaz | contributions @UGuntupalli | contributions @mphirke | contributions @sauln | contributions @richardqiu | contributions @MinchinWeb | contributions @BaritoneBeard | contributions @Sousa8697 | contributions @MollyCroke | contributions @ericclessantostv | contributions @fireddd | contributions @Zeroto521 | contributions","title":"Contributors"},{"location":"CHANGELOG/","text":"Changelog Unreleased v0.22.0 - 2021-11-21 [BUG] Fix conditional join issue for multiple conditions, where pd.eval fails to evaluate if numexpr is installed. #898 @samukweku [ENH] Added case_when to handle multiple conditionals and replacement values. Issue #736. @robertmitchellv [ENH] Deprecate new_column_names and merge_frame from process_text . Only existing columns are supported. @samukweku [ENH] complete uses pd.merge internally, providing a simpler logic, with some speed improvements in certain cases over pd.reindex . @samukweku [ENH] expand_grid returns a MultiIndex DataFrame, allowing the user to decide how to manipulate the columns. @samukweku [INF] Simplify a bit linting, use pre-commit as the CI linting checker. @Zeroto521 [ENH] Fix bug in pivot_longer for wrong output when names_pattern is a sequence with a single value. Issue #885 @samukweku [ENH] Deprecate aggfunc from pivot_wider ; aggregation can be chained with pandas' groupby . [ENH] As_Categorical deprecated from encode_categorical ; a tuple of (categories, order) suffices for **kwargs. @samukweku [ENH] Deprecate names_sort from pivot_wider .@samukweku [ENH] Add softmax to math module. Issue #902. @loganthomas v0.21.2 - 2021-09-01 [ENH] Fix warning message in coalesce , from bfill/fill; coalesce now uses variable arguments. Issue #882 @samukweku [INF] Add SciPy as explicit dependency in base.in . Issue #895 @ericmjl v0.21.1 - 2021-08-29 [DOC] Fix references and broken links in AUTHORS.rst. @loganthomas [DOC] Updated Broken links in the README and contributing docs. @nvamsikrishna05 [INF] Update pre-commit hooks and remove mutable references. Issue #844. @loganthomas [INF] Add GitHub Release pointer to auto-release script. Issue #818. @loganthomas [INF] Updated black version in github actions code-checks to match pre-commit hooks. @nvamsikrishna05 [ENH] Add reset_index flag to row_to_names function. @fireddd [ENH] Updated label_encode to use pandas factorize instead of scikit-learn LabelEncoder. @nvamsikrishna05 [INF] Removed the scikit-learn package from the dependencies from environment-dev.yml and base.in files. @nvamsikrishna05 [ENH] Add function to remove constant columns. @fireddd [ENH] Added factorize_columns method which will deprecate the label_encode method in future release. @nvamsikrishna05 [DOC] Delete Read the Docs project and remove all readthedocs.io references from the repo. Issue #863. @loganthomas [DOC] Updated various documentation sources to reflect pyjanitor-dev ownership. @loganthomas [INF] Fix isort automatic checks. Issue #845. @loganthomas [ENH] complete function now uses variable args (*args) - @samukweku [EHN] Set expand_column 's sep default is \"|\" , same to pandas.Series.str.get_dummies . Issue #876. @Zeroto521 [ENH] Deprecate limit from fill_direction. fill_direction now uses kwargs. @samukweku [ENH] Added conditional_join function that supports joins on non-equi operators. @samukweku [INF] Speed up pytest via -n (pytest-xdist) option. Issue #881. @Zeroto521 [DOC] Add list mark to keep select_columns 's example same style. @Zeroto521 [ENH] Updated rename_columns to take optional function argument for mapping. @nvamsikrishna05 v0.21.0 - 2021-07-16 [ENH] Drop fill_value parameter from complete . Users can use fillna instead. @samukweku [BUG] Fix bug in pivot_longer with single level columns. @samukweku [BUG] Disable exchange rates API until we can find another one to hit. @ericmjl [ENH] Change coalesce to return columns; also use bfill , ffill , which is faster than combine_first @samukweku [ENH] Use eval for string conditions in update_where . @samukweku [ENH] Add clearer error messages for pivot_longer . h/t to @tdhock for the observation. Issue #836 @samukweku [ENH] select_columns now uses variable arguments (*args), to provide a simpler selection without the need for lists. - @samukweku [ENH] encode_categoricals refactored to use generic functions via functools.dispatch . - @samukweku [ENH] Updated convert_excel_date to throw meaningful error when values contain non-numeric. @nvamsikrishna05 v0.20.14 - 2021-03-25 [ENH] Add dropna parameter to groupby_agg. @samukweku [ENH] complete adds a by parameter to expose explicit missing values per group, via groupby. @samukweku [ENH] Fix check_column to support single inputs - fixes label_encode . @zbarry v0.20.13 - 2021-02-25 [ENH] Performance improvements to expand_grid . @samukweku [HOTFIX] Add multipledispatch to pip requirements. @ericmjl v0.20.12 - 2021-02-25 [INF] Auto-release GitHub action maintenance. @loganthomas v0.20.11 - 2021-02-24 [INF] Setup auto-release GitHub action. @loganthomas [INF] Deploy darglint package for docstring linting. Issue #745. @loganthomas [ENH] Added optional truncation to clean_names function. Issue #753. @richardqiu [ENH] Added timeseries.flag_jumps() function. Issue #711. @loganthomas [ENH] pivot_longer can handle multiple values in paired columns, and can reshape using a list/tuple of regular expressions in names_pattern . @samukweku [ENH] Replaced default numeric conversion of dataframe with a dtypes parameter, allowing the user to control the data types. - @samukweku [INF] Loosen dependency specifications. Switch to pip-tools for managing dependencies. Issue #760. @MinchinWeb [DOC] added pipenv installation instructions @evan-anderson [ENH] Add pivot_wider function, which is the inverse of the pivot_longer function. @samukweku [INF] Add openpyxl to environment-dev.yml . @samukweku [ENH] Reduce code by reusing existing functions for fill_direction. @samukweku [ENH] Improvements to pivot_longer function, with improved speed and cleaner code. dtypes parameter dropped; user can change dtypes with pandas' astype method, or pyjanitor's change_type method. @samukweku [ENH] Add kwargs to encode_categorical function, to create ordered categorical columns, or categorical columns with explicit categories. @samukweku [ENH] Improvements to complete method. Use pd.merge to handle duplicates and null values. @samukweku [ENH] Add new_column_names parameter to process_text , allowing a user to create a new column name after processing a text column. Also added a merge_frame parameter, allowing dataframe merging, if the result of the text processing is a dataframe.@samukweku [ENH] Add aggfunc parameter to pivot_wider. @samukweku [ENH] Modified the check function in utils to verify if a value is a callable. @samukweku [ENH] Add a base _select_column function, using functools.singledispatch , to allow for flexible columns selection. @samukweku [ENH] pivot_longer and pivot_wider now support janitor.select_columns syntax, allowing for more flexible and dynamic column selection. @samukweku v0.20.10 [ENH] Added function sort_timestamps_monotonically to timeseries functions @UGuntupalli [ENH] Added the complete function for converting implicit missing values to explicit ones. @samukweku [ENH] Further simplification of expand_grid. @samukweku [BUGFIX] Added copy() method to original dataframe, to avoid mutation. Issue #729. @samukweku [ENH] Added also method for running functions in chain with no return values. [DOC] Added a timeseries module section to website docs. Issue #742. @loganthomas [ENH] Added a pivot_longer function, a wrapper around pd.melt and similar to tidyr's pivot_longer function. Also added an example notebook. @samukweku [ENH] Fixed code to returns error if fill_value is not a dictionary. @samukweku [INF] Welcome bot (.github/config.yml) for new users added. Issue #739. @samukweku v0.20.9 [ENH] Updated groupby_agg function to account for null entries in the by argument. @samukweku [ENH] Added function groupby_topk to janitor functions @mphirke v0.20.8 [ENH] Upgraded update_where function to use either the pandas query style, or boolean indexing via the loc method. Also updated find_replace function to use the loc method directly, instead of routing it through the update_where function. @samukweku [INF] Update pandas minimum version to 1.0.0. @hectormz [DOC] Updated the general functions API page to show all available functions. @samukweku [DOC] Fix the few lacking type annotations of functions. @VPerrollaz [DOC] Changed the signature from str to Optional[str] when initialized by None. @VPerrollaz [DOC] Add the Optional type for all signatures of the API. @VPerrollaz [TST] Updated test_expand_grid to account for int dtype difference in Windows OS @samukweku [TST] Make importing pandas testing functions follow uniform pattern. @hectormz [ENH] Added process_text wrapper function for all Pandas string methods. @samukweku [TST] Only skip tests for non-installed libraries on local machine. @hectormz [DOC] Fix minor issues in documentation. @hectormz [ENH] Added fill_direction function for forward/backward fills on missing values for selected columns in a dataframe. @samukweku [ENH] Simpler logic and less lines of code for expand_grid function @samukweku v0.20.7 [TST] Add a test for transform_column to check for nonmutation. @VPerrollaz [ENH] Contributed expand_grid function by @samukweku v0.20.6 [DOC] Pep8 all examples. @VPerrollaz [TST] Add docstrings to tests @hectormz [INF] Add debug-statements , requirements-txt-fixer , and interrogate to pre-commit . @hectormz [ENH] Upgraded transform_column to use df.assign underneath the hood, and also added option to transform column elementwise (via apply) or columnwise (thus operating on a series). @ericmjl v0.20.5 [INF] Replace pycodestyle with flake8 in order to add pandas-vet linter @hectormz [ENH] select_columns() now raises NameError if column label in search_columns_labels is missing from DataFrame columns. @smu095 v0.20.1 [DOC] Added an example for groupby_agg in general functions @samukweku [ENH] Contributed sort_naturally() function. @ericmjl v0.20.0 [DOC] Edited transform_column dest_column_name kwarg description to be clearer on defaults by @evan-anderson. [ENH] Replace apply() in favor of pandas functions in several functions. @hectormz [ENH] Add ecdf() Series function by @ericmjl. [DOC] Update API policy for clarity. @ericmjl [ENH] Enforce string conversion when cleaning names. @ericmjl [ENH] Change find_replace implementation to use keyword arguments to specify columns to perform find and replace on. @ericmjl [ENH] Add jitter() dataframe function by @rahosbach v0.19.0 [ENH] Add xarray support and clone_using / convert_datetime_to_number funcs by @zbarry. v0.18.3 [ENH] Series toset() functionality #570 @eyaltrabelsi [ENH] Added option to coalesce function to not delete coalesced columns. @gddcunh [ENH] Added functionality to deconcatenate tuple/list/collections in a column to deconcatenate_column @zbarry [ENH] Fix error message when length of new_column_names is wrong @DollofCutty [DOC] Fixed several examples of functional syntax in functions.py . @bdice [DOC] Fix #noqa comments showing up in docs by @hectormz [ENH] Add unionizing a group of dataframes' categoricals. @zbarry [DOC] Fix contributions hyperlinks in AUTHORS.rst and contributions by @hectormz [INF] Add pre-commit hooks to repository by @ericmjl [DOC] Fix formatting code in CONTRIBUTING.rst by @hectormz [DOC] Changed the typing for most \"column_name(s)\" to Hashable rather than enforcing strings, to more closely match Pandas API by @dendrondal [INF] Edited pycodestyle and Black parameters to avoid venvs by @dendrondal v0.18.2 [INF] Make requirements.txt smaller @eyaltrabelsi [ENH] Add a reset_index parameter to shuffle @eyaltrabelsi [DOC] Added contribution page link to readme @eyaltrabelsi [DOC] fix example for update_where , provide a bit more detail, and expand the bad_values example notebook to demonstrate its use by @anzelpwj. [INF] Fix pytest marks by @ericmjl (issue #520) [ENH] add example notebook with use of finance submodule methods by @rahosbach [DOC] added a couple of admonitions for Windows users. h/t @anzelpwj for debugging help when a few tests failed for win32 @Ram-N [ENH] Pyjanitor for PySpark @zjpoh [ENH] Add pyspark clean_names @zjpoh [ENH] Convert asserts to raise exceptions by @hectormz [ENH] Add decorator functions for missing and error handling @jiafengkevinchen [DOC] Update README with functional pandas API example. @ericmjl [INF] Move get_features_targets() to new ml.py module by @hectormz [ENH] Add chirality to morgan fingerprints in janitor.chemistry submodule by @Clayton-Springer [INF] import_message suggests python dist. appropriate installs by @hectormz [ENH] Add count_cumulative_unique() method to janitor.functions submodule by @rahosbach [ENH] Add update_where() method to janitor.spark.functions submodule by @zjpoh v0.18.1 [ENH] extend find_replace functionality to allow both exact match and regular-expression-based fuzzy match by @shandou [ENH] add preserve_position kwarg to deconcatenate_column with tests by @shandou and @ericmjl [DOC] add contributions that did not leave git traces by @ericmjl [ENH] add inflation adjustment in finance submodule by @rahosbach [DOC] clarified how new functions should be implemented by @shandou [ENH] add optional removal of accents on functions.clean_names, enabled by default by @mralbu [ENH] add camelCase conversion to snake_case on clean_names by @ericmjl, h/t @jtaylor for sharing original [ENH] Added null_flag function which can mark null values in rows. Implemented by @anzelpwj [ENH] add engineering submodule with unit conversion method by @rahosbach [DOC] add PyPI project description [ENH] add example notebook with use of finance submodule methods by @rahosbach For changes that happened prior to v0.18.1, please consult the closed PRs, which can be found here . We thank all contributors who have helped make pyjanitor the package that it is today.","title":"Changelog"},{"location":"CHANGELOG/#changelog","text":"","title":"Changelog"},{"location":"CHANGELOG/#unreleased","text":"","title":"Unreleased"},{"location":"CHANGELOG/#v0220-2021-11-21","text":"[BUG] Fix conditional join issue for multiple conditions, where pd.eval fails to evaluate if numexpr is installed. #898 @samukweku [ENH] Added case_when to handle multiple conditionals and replacement values. Issue #736. @robertmitchellv [ENH] Deprecate new_column_names and merge_frame from process_text . Only existing columns are supported. @samukweku [ENH] complete uses pd.merge internally, providing a simpler logic, with some speed improvements in certain cases over pd.reindex . @samukweku [ENH] expand_grid returns a MultiIndex DataFrame, allowing the user to decide how to manipulate the columns. @samukweku [INF] Simplify a bit linting, use pre-commit as the CI linting checker. @Zeroto521 [ENH] Fix bug in pivot_longer for wrong output when names_pattern is a sequence with a single value. Issue #885 @samukweku [ENH] Deprecate aggfunc from pivot_wider ; aggregation can be chained with pandas' groupby . [ENH] As_Categorical deprecated from encode_categorical ; a tuple of (categories, order) suffices for **kwargs. @samukweku [ENH] Deprecate names_sort from pivot_wider .@samukweku [ENH] Add softmax to math module. Issue #902. @loganthomas","title":"v0.22.0 - 2021-11-21"},{"location":"CHANGELOG/#v0212-2021-09-01","text":"[ENH] Fix warning message in coalesce , from bfill/fill; coalesce now uses variable arguments. Issue #882 @samukweku [INF] Add SciPy as explicit dependency in base.in . Issue #895 @ericmjl","title":"v0.21.2 - 2021-09-01"},{"location":"CHANGELOG/#v0211-2021-08-29","text":"[DOC] Fix references and broken links in AUTHORS.rst. @loganthomas [DOC] Updated Broken links in the README and contributing docs. @nvamsikrishna05 [INF] Update pre-commit hooks and remove mutable references. Issue #844. @loganthomas [INF] Add GitHub Release pointer to auto-release script. Issue #818. @loganthomas [INF] Updated black version in github actions code-checks to match pre-commit hooks. @nvamsikrishna05 [ENH] Add reset_index flag to row_to_names function. @fireddd [ENH] Updated label_encode to use pandas factorize instead of scikit-learn LabelEncoder. @nvamsikrishna05 [INF] Removed the scikit-learn package from the dependencies from environment-dev.yml and base.in files. @nvamsikrishna05 [ENH] Add function to remove constant columns. @fireddd [ENH] Added factorize_columns method which will deprecate the label_encode method in future release. @nvamsikrishna05 [DOC] Delete Read the Docs project and remove all readthedocs.io references from the repo. Issue #863. @loganthomas [DOC] Updated various documentation sources to reflect pyjanitor-dev ownership. @loganthomas [INF] Fix isort automatic checks. Issue #845. @loganthomas [ENH] complete function now uses variable args (*args) - @samukweku [EHN] Set expand_column 's sep default is \"|\" , same to pandas.Series.str.get_dummies . Issue #876. @Zeroto521 [ENH] Deprecate limit from fill_direction. fill_direction now uses kwargs. @samukweku [ENH] Added conditional_join function that supports joins on non-equi operators. @samukweku [INF] Speed up pytest via -n (pytest-xdist) option. Issue #881. @Zeroto521 [DOC] Add list mark to keep select_columns 's example same style. @Zeroto521 [ENH] Updated rename_columns to take optional function argument for mapping. @nvamsikrishna05","title":"v0.21.1 - 2021-08-29"},{"location":"CHANGELOG/#v0210-2021-07-16","text":"[ENH] Drop fill_value parameter from complete . Users can use fillna instead. @samukweku [BUG] Fix bug in pivot_longer with single level columns. @samukweku [BUG] Disable exchange rates API until we can find another one to hit. @ericmjl [ENH] Change coalesce to return columns; also use bfill , ffill , which is faster than combine_first @samukweku [ENH] Use eval for string conditions in update_where . @samukweku [ENH] Add clearer error messages for pivot_longer . h/t to @tdhock for the observation. Issue #836 @samukweku [ENH] select_columns now uses variable arguments (*args), to provide a simpler selection without the need for lists. - @samukweku [ENH] encode_categoricals refactored to use generic functions via functools.dispatch . - @samukweku [ENH] Updated convert_excel_date to throw meaningful error when values contain non-numeric. @nvamsikrishna05","title":"v0.21.0 - 2021-07-16"},{"location":"CHANGELOG/#v02014-2021-03-25","text":"[ENH] Add dropna parameter to groupby_agg. @samukweku [ENH] complete adds a by parameter to expose explicit missing values per group, via groupby. @samukweku [ENH] Fix check_column to support single inputs - fixes label_encode . @zbarry","title":"v0.20.14 - 2021-03-25"},{"location":"CHANGELOG/#v02013-2021-02-25","text":"[ENH] Performance improvements to expand_grid . @samukweku [HOTFIX] Add multipledispatch to pip requirements. @ericmjl","title":"v0.20.13 - 2021-02-25"},{"location":"CHANGELOG/#v02012-2021-02-25","text":"[INF] Auto-release GitHub action maintenance. @loganthomas","title":"v0.20.12 - 2021-02-25"},{"location":"CHANGELOG/#v02011-2021-02-24","text":"[INF] Setup auto-release GitHub action. @loganthomas [INF] Deploy darglint package for docstring linting. Issue #745. @loganthomas [ENH] Added optional truncation to clean_names function. Issue #753. @richardqiu [ENH] Added timeseries.flag_jumps() function. Issue #711. @loganthomas [ENH] pivot_longer can handle multiple values in paired columns, and can reshape using a list/tuple of regular expressions in names_pattern . @samukweku [ENH] Replaced default numeric conversion of dataframe with a dtypes parameter, allowing the user to control the data types. - @samukweku [INF] Loosen dependency specifications. Switch to pip-tools for managing dependencies. Issue #760. @MinchinWeb [DOC] added pipenv installation instructions @evan-anderson [ENH] Add pivot_wider function, which is the inverse of the pivot_longer function. @samukweku [INF] Add openpyxl to environment-dev.yml . @samukweku [ENH] Reduce code by reusing existing functions for fill_direction. @samukweku [ENH] Improvements to pivot_longer function, with improved speed and cleaner code. dtypes parameter dropped; user can change dtypes with pandas' astype method, or pyjanitor's change_type method. @samukweku [ENH] Add kwargs to encode_categorical function, to create ordered categorical columns, or categorical columns with explicit categories. @samukweku [ENH] Improvements to complete method. Use pd.merge to handle duplicates and null values. @samukweku [ENH] Add new_column_names parameter to process_text , allowing a user to create a new column name after processing a text column. Also added a merge_frame parameter, allowing dataframe merging, if the result of the text processing is a dataframe.@samukweku [ENH] Add aggfunc parameter to pivot_wider. @samukweku [ENH] Modified the check function in utils to verify if a value is a callable. @samukweku [ENH] Add a base _select_column function, using functools.singledispatch , to allow for flexible columns selection. @samukweku [ENH] pivot_longer and pivot_wider now support janitor.select_columns syntax, allowing for more flexible and dynamic column selection. @samukweku","title":"v0.20.11 - 2021-02-24"},{"location":"CHANGELOG/#v02010","text":"[ENH] Added function sort_timestamps_monotonically to timeseries functions @UGuntupalli [ENH] Added the complete function for converting implicit missing values to explicit ones. @samukweku [ENH] Further simplification of expand_grid. @samukweku [BUGFIX] Added copy() method to original dataframe, to avoid mutation. Issue #729. @samukweku [ENH] Added also method for running functions in chain with no return values. [DOC] Added a timeseries module section to website docs. Issue #742. @loganthomas [ENH] Added a pivot_longer function, a wrapper around pd.melt and similar to tidyr's pivot_longer function. Also added an example notebook. @samukweku [ENH] Fixed code to returns error if fill_value is not a dictionary. @samukweku [INF] Welcome bot (.github/config.yml) for new users added. Issue #739. @samukweku","title":"v0.20.10"},{"location":"CHANGELOG/#v0209","text":"[ENH] Updated groupby_agg function to account for null entries in the by argument. @samukweku [ENH] Added function groupby_topk to janitor functions @mphirke","title":"v0.20.9"},{"location":"CHANGELOG/#v0208","text":"[ENH] Upgraded update_where function to use either the pandas query style, or boolean indexing via the loc method. Also updated find_replace function to use the loc method directly, instead of routing it through the update_where function. @samukweku [INF] Update pandas minimum version to 1.0.0. @hectormz [DOC] Updated the general functions API page to show all available functions. @samukweku [DOC] Fix the few lacking type annotations of functions. @VPerrollaz [DOC] Changed the signature from str to Optional[str] when initialized by None. @VPerrollaz [DOC] Add the Optional type for all signatures of the API. @VPerrollaz [TST] Updated test_expand_grid to account for int dtype difference in Windows OS @samukweku [TST] Make importing pandas testing functions follow uniform pattern. @hectormz [ENH] Added process_text wrapper function for all Pandas string methods. @samukweku [TST] Only skip tests for non-installed libraries on local machine. @hectormz [DOC] Fix minor issues in documentation. @hectormz [ENH] Added fill_direction function for forward/backward fills on missing values for selected columns in a dataframe. @samukweku [ENH] Simpler logic and less lines of code for expand_grid function @samukweku","title":"v0.20.8"},{"location":"CHANGELOG/#v0207","text":"[TST] Add a test for transform_column to check for nonmutation. @VPerrollaz [ENH] Contributed expand_grid function by @samukweku","title":"v0.20.7"},{"location":"CHANGELOG/#v0206","text":"[DOC] Pep8 all examples. @VPerrollaz [TST] Add docstrings to tests @hectormz [INF] Add debug-statements , requirements-txt-fixer , and interrogate to pre-commit . @hectormz [ENH] Upgraded transform_column to use df.assign underneath the hood, and also added option to transform column elementwise (via apply) or columnwise (thus operating on a series). @ericmjl","title":"v0.20.6"},{"location":"CHANGELOG/#v0205","text":"[INF] Replace pycodestyle with flake8 in order to add pandas-vet linter @hectormz [ENH] select_columns() now raises NameError if column label in search_columns_labels is missing from DataFrame columns. @smu095","title":"v0.20.5"},{"location":"CHANGELOG/#v0201","text":"[DOC] Added an example for groupby_agg in general functions @samukweku [ENH] Contributed sort_naturally() function. @ericmjl","title":"v0.20.1"},{"location":"CHANGELOG/#v0200","text":"[DOC] Edited transform_column dest_column_name kwarg description to be clearer on defaults by @evan-anderson. [ENH] Replace apply() in favor of pandas functions in several functions. @hectormz [ENH] Add ecdf() Series function by @ericmjl. [DOC] Update API policy for clarity. @ericmjl [ENH] Enforce string conversion when cleaning names. @ericmjl [ENH] Change find_replace implementation to use keyword arguments to specify columns to perform find and replace on. @ericmjl [ENH] Add jitter() dataframe function by @rahosbach","title":"v0.20.0"},{"location":"CHANGELOG/#v0190","text":"[ENH] Add xarray support and clone_using / convert_datetime_to_number funcs by @zbarry.","title":"v0.19.0"},{"location":"CHANGELOG/#v0183","text":"[ENH] Series toset() functionality #570 @eyaltrabelsi [ENH] Added option to coalesce function to not delete coalesced columns. @gddcunh [ENH] Added functionality to deconcatenate tuple/list/collections in a column to deconcatenate_column @zbarry [ENH] Fix error message when length of new_column_names is wrong @DollofCutty [DOC] Fixed several examples of functional syntax in functions.py . @bdice [DOC] Fix #noqa comments showing up in docs by @hectormz [ENH] Add unionizing a group of dataframes' categoricals. @zbarry [DOC] Fix contributions hyperlinks in AUTHORS.rst and contributions by @hectormz [INF] Add pre-commit hooks to repository by @ericmjl [DOC] Fix formatting code in CONTRIBUTING.rst by @hectormz [DOC] Changed the typing for most \"column_name(s)\" to Hashable rather than enforcing strings, to more closely match Pandas API by @dendrondal [INF] Edited pycodestyle and Black parameters to avoid venvs by @dendrondal","title":"v0.18.3"},{"location":"CHANGELOG/#v0182","text":"[INF] Make requirements.txt smaller @eyaltrabelsi [ENH] Add a reset_index parameter to shuffle @eyaltrabelsi [DOC] Added contribution page link to readme @eyaltrabelsi [DOC] fix example for update_where , provide a bit more detail, and expand the bad_values example notebook to demonstrate its use by @anzelpwj. [INF] Fix pytest marks by @ericmjl (issue #520) [ENH] add example notebook with use of finance submodule methods by @rahosbach [DOC] added a couple of admonitions for Windows users. h/t @anzelpwj for debugging help when a few tests failed for win32 @Ram-N [ENH] Pyjanitor for PySpark @zjpoh [ENH] Add pyspark clean_names @zjpoh [ENH] Convert asserts to raise exceptions by @hectormz [ENH] Add decorator functions for missing and error handling @jiafengkevinchen [DOC] Update README with functional pandas API example. @ericmjl [INF] Move get_features_targets() to new ml.py module by @hectormz [ENH] Add chirality to morgan fingerprints in janitor.chemistry submodule by @Clayton-Springer [INF] import_message suggests python dist. appropriate installs by @hectormz [ENH] Add count_cumulative_unique() method to janitor.functions submodule by @rahosbach [ENH] Add update_where() method to janitor.spark.functions submodule by @zjpoh","title":"v0.18.2"},{"location":"CHANGELOG/#v0181","text":"[ENH] extend find_replace functionality to allow both exact match and regular-expression-based fuzzy match by @shandou [ENH] add preserve_position kwarg to deconcatenate_column with tests by @shandou and @ericmjl [DOC] add contributions that did not leave git traces by @ericmjl [ENH] add inflation adjustment in finance submodule by @rahosbach [DOC] clarified how new functions should be implemented by @shandou [ENH] add optional removal of accents on functions.clean_names, enabled by default by @mralbu [ENH] add camelCase conversion to snake_case on clean_names by @ericmjl, h/t @jtaylor for sharing original [ENH] Added null_flag function which can mark null values in rows. Implemented by @anzelpwj [ENH] add engineering submodule with unit conversion method by @rahosbach [DOC] add PyPI project description [ENH] add example notebook with use of finance submodule methods by @rahosbach For changes that happened prior to v0.18.1, please consult the closed PRs, which can be found here . We thank all contributors who have helped make pyjanitor the package that it is today.","title":"v0.18.1"},{"location":"devguide/","text":"Development Guide For those of you who are interested in contributing code to the project, many previous contributors have wrestled with a variety of ways of getting set up. While we can't cover every single last configuration, we could cover some of the more common cases. Here they are for your benefit! Development Containers with VSCode As of 29 May 2020, development containers are supported! This is the preferred way to get you started up and running, as it creates a uniform setup environment that is much easier for the maintainers to debug, because you are provided with a pre-built and clean development environment free of any assumptions of your own system. You don't have to wrestle with conda wait times if you don't want to! To get started: Fork the repository. Ensure you have Docker running on your local machine. Ensure you have VSCode running on your local machine. In Visual Studio Code, click on the quick actions Status Bar item in the lower left corner. Then select \"Remote Containers: Clone Repository In Container Volume\". Enter in the URL of your fork of pyjanitor . VSCode will pull down the prebuilt Docker container, git clone the repository for you inside an isolated Docker volume, and mount the repository directory inside your Docker container. Follow best practices to submit a pull request by making a feature branch. Now, hack away, and submit in your pull request! You shouln't be able to access the cloned repo on your local hard drive. If you do want local access, then clone the repo locally first before selecting \"Remote Containers: Open Folder In Container\". If you find something is broken because a utility is missing in the container, submit a PR with the appropriate build command inserted in the Dockerfile. Care has been taken to document what each step does, so please read the in-line documentation in the Dockerfile carefully. Manual Setup Fork the repository Firstly, begin by forking the pyjanitor repo on GitHub. Then, clone your fork locally: git clone git@github.com:your_name_here/pyjanitor.git Setup the conda environment Now, install your local copy into a conda environment. Assuming you have conda installed, this is how you set up your fork for local development cd pyjanitor/ make install This also installs your new conda environment as a Jupyter-accessible kernel. If you plan to write any notebooks, to run correctly inside the environment, make sure you select the correct kernel from the top right corner of JupyterLab! Windows Users If you are on Windows, you may need to install make before you can run the install. You can get it from conda-forge :: bash conda install -c defaults -c conda-forge make You should be able to run make now. The command above installs make to the ~/Anaconda3/Library/bin directory. PyCharm Users For PyCharm users, here are some instructions <PYCHARM_USERS.html> __ to get your Conda environment set up. Install the pre-commit hooks. pre-commit hooks are available to run code formatting checks automagically before git commits happen. If you did not have these installed before, run the following commands: # Update your environment to install pre-commit conda env update -f environment-dev.yml # Install pre-commit hooks pre-commit install Build docs locally You should also be able to build the docs locally. To do this, from the main pyjanitor directory: make docs The command above allows you to view the documentation locally in your browser. Sphinx (a python documentation generator) <http://www.sphinx-doc.org/en/stable/usage/quickstart.html> _ builds and renders the html for you, and you can find the html files by navigating to pyjanitor/docs/_build , and then you can find the correct html file. To see the main pyjanitor page, open the index.html file. Errors with documentation builds If you get any errors about importing modules when running make docs , first activate the development environment: bash source activate pyjanitor-dev || conda activate pyjanitor-dev Sphinx uses rst files (restructured text) <http://www.sphinx-doc.org/en/master/usage/restructuredtext/basics.html> _ as its markdown language. To edit documentation, go to the rst file that corresponds to the html file you would like to edit. Make the changes directly in the rst file with the correct markup. Save the file and rebuild the html pages using the same commands as above to see what your changes look like in html. Plan out the change you'd like to contribute The old adage rings true: failing to plan means planning to fail. We'd encourage you to flesh out the idea you'd like to contribute on the GitHub issue tracker before embarking on a contribution. Submitting new code, in particular, is one where the maintainers will need more consideration, after all, any new code submitted introduces a new maintenance burden, unless you the contributor would like to join the maintainers team! To kickstart the discussion, submit an issue to the pyjanitor GitHub issue tracker describing your planned changes. The issue tracker also helps us keep track of who is working on what. Create a branch for local development New contributions to pyjanitor should be done in a new branch that you have based off the latest version of the dev branch. To create a new branch: git checkout -b name-of-your-bugfix-or-feature dev Now you can make your changes locally. Check your code When you're done making changes, check that your changes are properly formatted and that all tests still pass:: make check If any of the checks fail, you can apply the checks individually (to save time): Automated code formatting: make style Code styling problems check: make lint Code unit testing: make test Styling problems must be resolved before the pull request can be accepted. make test runs all pyjanitor 's unit tests to probe whether changes to the source code have potentially introduced bugs. These tests must also pass before the pull request is accepted, and the continuous integration system up on GitHub Actions will help run all of the tests before they are committed to the repository. When you run the test locally, the tests in chemistry.py , biology.py , spark.py are automatically skipped if you don't have the optional dependencies (e.g. rdkit ) installed. Commit your changes Now you can commit your changes and push your branch to GitHub: git add . git commit -m \"Your detailed description of your changes.\" git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website Congratulations, you've made it to the penultimate step; your code is ready to be checked and reviewed by the maintainers! Head over to the GitHub website and create a pull request. When you are picking out which branch to merge into, a.k.a. the target branch, be sure to select dev (not master ). Fix any remaining issues It's rare, but you might at this point still encounter issues, as the continuous integration (CI) system on GitHub Actions checks your code. Some of these might not be your fault; rather, it might well be the case that your code fell a little bit out of date as others' pull requests are merged into the repository. In any case, if there are any issues, the pipeline will fail out. We check for code style, docstring coverage, test coverage, and doc discovery. If you're comfortable looking at the pipeline logs, feel free to do so; they are open to all to view. Otherwise, one of the dev team members can help you with reviewing the code checks. Code Compatibility pyjanitor supports Python 3.6+, so all contributed code must maintain this compatibility. Tips To run a subset of tests: pytest tests.test_functions","title":"Development Guide"},{"location":"devguide/#development-guide","text":"For those of you who are interested in contributing code to the project, many previous contributors have wrestled with a variety of ways of getting set up. While we can't cover every single last configuration, we could cover some of the more common cases. Here they are for your benefit!","title":"Development Guide"},{"location":"devguide/#development-containers-with-vscode","text":"As of 29 May 2020, development containers are supported! This is the preferred way to get you started up and running, as it creates a uniform setup environment that is much easier for the maintainers to debug, because you are provided with a pre-built and clean development environment free of any assumptions of your own system. You don't have to wrestle with conda wait times if you don't want to! To get started: Fork the repository. Ensure you have Docker running on your local machine. Ensure you have VSCode running on your local machine. In Visual Studio Code, click on the quick actions Status Bar item in the lower left corner. Then select \"Remote Containers: Clone Repository In Container Volume\". Enter in the URL of your fork of pyjanitor . VSCode will pull down the prebuilt Docker container, git clone the repository for you inside an isolated Docker volume, and mount the repository directory inside your Docker container. Follow best practices to submit a pull request by making a feature branch. Now, hack away, and submit in your pull request! You shouln't be able to access the cloned repo on your local hard drive. If you do want local access, then clone the repo locally first before selecting \"Remote Containers: Open Folder In Container\". If you find something is broken because a utility is missing in the container, submit a PR with the appropriate build command inserted in the Dockerfile. Care has been taken to document what each step does, so please read the in-line documentation in the Dockerfile carefully.","title":"Development Containers with VSCode"},{"location":"devguide/#manual-setup","text":"","title":"Manual Setup"},{"location":"devguide/#fork-the-repository","text":"Firstly, begin by forking the pyjanitor repo on GitHub. Then, clone your fork locally: git clone git@github.com:your_name_here/pyjanitor.git","title":"Fork the repository"},{"location":"devguide/#setup-the-conda-environment","text":"Now, install your local copy into a conda environment. Assuming you have conda installed, this is how you set up your fork for local development cd pyjanitor/ make install This also installs your new conda environment as a Jupyter-accessible kernel. If you plan to write any notebooks, to run correctly inside the environment, make sure you select the correct kernel from the top right corner of JupyterLab! Windows Users If you are on Windows, you may need to install make before you can run the install. You can get it from conda-forge :: bash conda install -c defaults -c conda-forge make You should be able to run make now. The command above installs make to the ~/Anaconda3/Library/bin directory. PyCharm Users For PyCharm users, here are some instructions <PYCHARM_USERS.html> __ to get your Conda environment set up.","title":"Setup the conda environment"},{"location":"devguide/#install-the-pre-commit-hooks","text":"pre-commit hooks are available to run code formatting checks automagically before git commits happen. If you did not have these installed before, run the following commands: # Update your environment to install pre-commit conda env update -f environment-dev.yml # Install pre-commit hooks pre-commit install","title":"Install the pre-commit hooks."},{"location":"devguide/#build-docs-locally","text":"You should also be able to build the docs locally. To do this, from the main pyjanitor directory: make docs The command above allows you to view the documentation locally in your browser. Sphinx (a python documentation generator) <http://www.sphinx-doc.org/en/stable/usage/quickstart.html> _ builds and renders the html for you, and you can find the html files by navigating to pyjanitor/docs/_build , and then you can find the correct html file. To see the main pyjanitor page, open the index.html file. Errors with documentation builds If you get any errors about importing modules when running make docs , first activate the development environment: bash source activate pyjanitor-dev || conda activate pyjanitor-dev Sphinx uses rst files (restructured text) <http://www.sphinx-doc.org/en/master/usage/restructuredtext/basics.html> _ as its markdown language. To edit documentation, go to the rst file that corresponds to the html file you would like to edit. Make the changes directly in the rst file with the correct markup. Save the file and rebuild the html pages using the same commands as above to see what your changes look like in html.","title":"Build docs locally"},{"location":"devguide/#plan-out-the-change-youd-like-to-contribute","text":"The old adage rings true: failing to plan means planning to fail. We'd encourage you to flesh out the idea you'd like to contribute on the GitHub issue tracker before embarking on a contribution. Submitting new code, in particular, is one where the maintainers will need more consideration, after all, any new code submitted introduces a new maintenance burden, unless you the contributor would like to join the maintainers team! To kickstart the discussion, submit an issue to the pyjanitor GitHub issue tracker describing your planned changes. The issue tracker also helps us keep track of who is working on what.","title":"Plan out the change you'd like to contribute"},{"location":"devguide/#create-a-branch-for-local-development","text":"New contributions to pyjanitor should be done in a new branch that you have based off the latest version of the dev branch. To create a new branch: git checkout -b name-of-your-bugfix-or-feature dev Now you can make your changes locally.","title":"Create a branch for local development"},{"location":"devguide/#check-your-code","text":"When you're done making changes, check that your changes are properly formatted and that all tests still pass:: make check If any of the checks fail, you can apply the checks individually (to save time): Automated code formatting: make style Code styling problems check: make lint Code unit testing: make test Styling problems must be resolved before the pull request can be accepted. make test runs all pyjanitor 's unit tests to probe whether changes to the source code have potentially introduced bugs. These tests must also pass before the pull request is accepted, and the continuous integration system up on GitHub Actions will help run all of the tests before they are committed to the repository. When you run the test locally, the tests in chemistry.py , biology.py , spark.py are automatically skipped if you don't have the optional dependencies (e.g. rdkit ) installed.","title":"Check your code"},{"location":"devguide/#commit-your-changes","text":"Now you can commit your changes and push your branch to GitHub: git add . git commit -m \"Your detailed description of your changes.\" git push origin name-of-your-bugfix-or-feature","title":"Commit your changes"},{"location":"devguide/#submit-a-pull-request-through-the-github-website","text":"Congratulations, you've made it to the penultimate step; your code is ready to be checked and reviewed by the maintainers! Head over to the GitHub website and create a pull request. When you are picking out which branch to merge into, a.k.a. the target branch, be sure to select dev (not master ).","title":"Submit a pull request through the GitHub website"},{"location":"devguide/#fix-any-remaining-issues","text":"It's rare, but you might at this point still encounter issues, as the continuous integration (CI) system on GitHub Actions checks your code. Some of these might not be your fault; rather, it might well be the case that your code fell a little bit out of date as others' pull requests are merged into the repository. In any case, if there are any issues, the pipeline will fail out. We check for code style, docstring coverage, test coverage, and doc discovery. If you're comfortable looking at the pipeline logs, feel free to do so; they are open to all to view. Otherwise, one of the dev team members can help you with reviewing the code checks.","title":"Fix any remaining issues"},{"location":"devguide/#code-compatibility","text":"pyjanitor supports Python 3.6+, so all contributed code must maintain this compatibility.","title":"Code Compatibility"},{"location":"devguide/#tips","text":"To run a subset of tests: pytest tests.test_functions","title":"Tips"},{"location":"api/biology/","text":"Biology Biology and bioinformatics-oriented data cleaning functions. join_fasta(df, filename, id_col, column_name) Convenience method to join in a FASTA file as a column. This allows us to add the string sequence of a FASTA file as a new column of data in the dataframe. This method only attaches the string representation of the SeqRecord.Seq object from Biopython. Does not attach the full SeqRecord. Alphabet is also not stored, under the assumption that the data scientist has domain knowledge of what kind of sequence is being read in (nucleotide vs. amino acid.) This method mutates the original DataFrame. For more advanced functions, please use phylopandas. Functional usage example: import pandas as pd import janitor.biology df = pd.DataFrame(...) df = janitor.biology.join_fasta( df=df, filename='fasta_file.fasta', id_col='sequence_accession', column_name='sequence', ) Method chaining usage example: import pandas as pd import janitor.biology df = pd.DataFrame(...) df = df.join_fasta( filename='fasta_file.fasta', id_col='sequence_accession', column_name='sequence', ) Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required filename str Path to the FASTA file. required id_col str The column in the DataFrame that houses sequence IDs. required column_name str The name of the new column. required Returns: Type Description DataFrame A pandas DataFrame with new FASTA string sequence column. Source code in janitor/biology.py @pf.register_dataframe_method @deprecated_alias(col_name=\"column_name\") def join_fasta( df: pd.DataFrame, filename: str, id_col: str, column_name: str ) -> pd.DataFrame: \"\"\" Convenience method to join in a FASTA file as a column. This allows us to add the string sequence of a FASTA file as a new column of data in the dataframe. This method only attaches the string representation of the SeqRecord.Seq object from Biopython. Does not attach the full SeqRecord. Alphabet is also not stored, under the assumption that the data scientist has domain knowledge of what kind of sequence is being read in (nucleotide vs. amino acid.) This method mutates the original DataFrame. For more advanced functions, please use phylopandas. Functional usage example: ```python import pandas as pd import janitor.biology df = pd.DataFrame(...) df = janitor.biology.join_fasta( df=df, filename='fasta_file.fasta', id_col='sequence_accession', column_name='sequence', ) ``` Method chaining usage example: ```python import pandas as pd import janitor.biology df = pd.DataFrame(...) df = df.join_fasta( filename='fasta_file.fasta', id_col='sequence_accession', column_name='sequence', ) ``` :param df: A pandas DataFrame. :param filename: Path to the FASTA file. :param id_col: The column in the DataFrame that houses sequence IDs. :param column_name: The name of the new column. :returns: A pandas DataFrame with new FASTA string sequence column. \"\"\" seqrecords = { x.id: x.seq.__str__() for x in SeqIO.parse(filename, \"fasta\") } seq_col = [seqrecords[i] for i in df[id_col]] df[column_name] = seq_col return df","title":"Biology"},{"location":"api/biology/#biology","text":"Biology and bioinformatics-oriented data cleaning functions.","title":"Biology"},{"location":"api/biology/#janitor.biology.join_fasta","text":"Convenience method to join in a FASTA file as a column. This allows us to add the string sequence of a FASTA file as a new column of data in the dataframe. This method only attaches the string representation of the SeqRecord.Seq object from Biopython. Does not attach the full SeqRecord. Alphabet is also not stored, under the assumption that the data scientist has domain knowledge of what kind of sequence is being read in (nucleotide vs. amino acid.) This method mutates the original DataFrame. For more advanced functions, please use phylopandas. Functional usage example: import pandas as pd import janitor.biology df = pd.DataFrame(...) df = janitor.biology.join_fasta( df=df, filename='fasta_file.fasta', id_col='sequence_accession', column_name='sequence', ) Method chaining usage example: import pandas as pd import janitor.biology df = pd.DataFrame(...) df = df.join_fasta( filename='fasta_file.fasta', id_col='sequence_accession', column_name='sequence', ) Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required filename str Path to the FASTA file. required id_col str The column in the DataFrame that houses sequence IDs. required column_name str The name of the new column. required Returns: Type Description DataFrame A pandas DataFrame with new FASTA string sequence column. Source code in janitor/biology.py @pf.register_dataframe_method @deprecated_alias(col_name=\"column_name\") def join_fasta( df: pd.DataFrame, filename: str, id_col: str, column_name: str ) -> pd.DataFrame: \"\"\" Convenience method to join in a FASTA file as a column. This allows us to add the string sequence of a FASTA file as a new column of data in the dataframe. This method only attaches the string representation of the SeqRecord.Seq object from Biopython. Does not attach the full SeqRecord. Alphabet is also not stored, under the assumption that the data scientist has domain knowledge of what kind of sequence is being read in (nucleotide vs. amino acid.) This method mutates the original DataFrame. For more advanced functions, please use phylopandas. Functional usage example: ```python import pandas as pd import janitor.biology df = pd.DataFrame(...) df = janitor.biology.join_fasta( df=df, filename='fasta_file.fasta', id_col='sequence_accession', column_name='sequence', ) ``` Method chaining usage example: ```python import pandas as pd import janitor.biology df = pd.DataFrame(...) df = df.join_fasta( filename='fasta_file.fasta', id_col='sequence_accession', column_name='sequence', ) ``` :param df: A pandas DataFrame. :param filename: Path to the FASTA file. :param id_col: The column in the DataFrame that houses sequence IDs. :param column_name: The name of the new column. :returns: A pandas DataFrame with new FASTA string sequence column. \"\"\" seqrecords = { x.id: x.seq.__str__() for x in SeqIO.parse(filename, \"fasta\") } seq_col = [seqrecords[i] for i in df[id_col]] df[column_name] = seq_col return df","title":"join_fasta()"},{"location":"api/chemistry/","text":"Chemistry Chemistry and cheminformatics-oriented data cleaning functions. maccs_keys_fingerprint(df, mols_column_name) Convert a column of RDKIT mol objects into MACCS Keys Fingerprints. Returns a new dataframe without any of the original data. This is intentional to leave the user with the data requested. This method does not mutate the original DataFrame. Functional usage example: import pandas as pd import janitor.chemistry df = pd.DataFrame(...) maccs = janitor.chemistry.maccs_keys_fingerprint( df=df.smiles2mol('smiles', 'mols'), mols_column_name='mols' ) Method chaining usage example: import pandas as pd import janitor.chemistry df = pd.DataFrame(...) maccs = ( df.smiles2mol('smiles', 'mols') .maccs_keys_fingerprint(mols_column_name='mols') ) If you wish to join the maccs keys fingerprints back into the original dataframe, this can be accomplished by doing a join , because the indices are preserved: joined = df.join(maccs_keys_fingerprint) Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required mols_column_name Hashable The name of the column that has the RDKIT mol objects. required Returns: Type Description DataFrame A new pandas DataFrame of MACCS keys fingerprints. Source code in janitor/chemistry.py @pf.register_dataframe_method @deprecated_alias(mols_col=\"mols_column_name\") def maccs_keys_fingerprint( df: pd.DataFrame, mols_column_name: Hashable ) -> pd.DataFrame: \"\"\" Convert a column of RDKIT mol objects into MACCS Keys Fingerprints. Returns a new dataframe without any of the original data. This is intentional to leave the user with the data requested. This method does not mutate the original DataFrame. Functional usage example: ```python import pandas as pd import janitor.chemistry df = pd.DataFrame(...) maccs = janitor.chemistry.maccs_keys_fingerprint( df=df.smiles2mol('smiles', 'mols'), mols_column_name='mols' ) ``` Method chaining usage example: ```python import pandas as pd import janitor.chemistry df = pd.DataFrame(...) maccs = ( df.smiles2mol('smiles', 'mols') .maccs_keys_fingerprint(mols_column_name='mols') ) ``` If you wish to join the maccs keys fingerprints back into the original dataframe, this can be accomplished by doing a `join`, because the indices are preserved: ```python joined = df.join(maccs_keys_fingerprint) ``` :param df: A pandas DataFrame. :param mols_column_name: The name of the column that has the RDKIT mol objects. :returns: A new pandas DataFrame of MACCS keys fingerprints. \"\"\" maccs = [GetMACCSKeysFingerprint(m) for m in df[mols_column_name]] np_maccs = [] for macc in maccs: arr = np.zeros((1,)) DataStructs.ConvertToNumpyArray(macc, arr) np_maccs.append(arr) np_maccs = np.vstack(np_maccs) fmaccs = pd.DataFrame(np_maccs) fmaccs.index = df.index return fmaccs molecular_descriptors(df, mols_column_name) Convert a column of RDKIT mol objects into a Pandas DataFrame of molecular descriptors. Returns a new dataframe without any of the original data. This is intentional to leave the user only with the data requested. This method does not mutate the original DataFrame. The molecular descriptors are from the rdkit.Chem.rdMolDescriptors: Chi0n, Chi0v, Chi1n, Chi1v, Chi2n, Chi2v, Chi3n, Chi3v, Chi4n, Chi4v, ExactMolWt, FractionCSP3, HallKierAlpha, Kappa1, Kappa2, Kappa3, LabuteASA, NumAliphaticCarbocycles, NumAliphaticHeterocycles, NumAliphaticRings, NumAmideBonds, NumAromaticCarbocycles, NumAromaticHeterocycles, NumAromaticRings, NumAtomStereoCenters, NumBridgeheadAtoms, NumHBA, NumHBD, NumHeteroatoms, NumHeterocycles, NumLipinskiHBA, NumLipinskiHBD, NumRings, NumSaturatedCarbocycles, NumSaturatedHeterocycles, NumSaturatedRings, NumSpiroAtoms, NumUnspecifiedAtomStereoCenters, TPSA. Functional usage example: import pandas as pd import janitor.chemistry df = pd.DataFrame(...) mol_desc = ( janitor.chemistry.molecular_descriptors( df=df.smiles2mol('smiles', 'mols'), mols_column_name='mols' ) ) Method chaining usage example: import pandas as pd import janitor.chemistry df = pd.DataFrame(...) mol_desc = ( df.smiles2mol('smiles', 'mols') .molecular_descriptors(mols_column_name='mols') ) If you wish to join the molecular descriptors back into the original dataframe, this can be accomplished by doing a join , because the indices are preserved: joined = df.join(mol_desc) Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required mols_column_name Hashable The name of the column that has the RDKIT mol objects. required Returns: Type Description DataFrame A new pandas DataFrame of molecular descriptors. Source code in janitor/chemistry.py @pf.register_dataframe_method @deprecated_alias(mols_col=\"mols_column_name\") def molecular_descriptors( df: pd.DataFrame, mols_column_name: Hashable ) -> pd.DataFrame: \"\"\" Convert a column of RDKIT mol objects into a Pandas DataFrame of molecular descriptors. Returns a new dataframe without any of the original data. This is intentional to leave the user only with the data requested. This method does not mutate the original DataFrame. The molecular descriptors are from the rdkit.Chem.rdMolDescriptors: ``` Chi0n, Chi0v, Chi1n, Chi1v, Chi2n, Chi2v, Chi3n, Chi3v, Chi4n, Chi4v, ExactMolWt, FractionCSP3, HallKierAlpha, Kappa1, Kappa2, Kappa3, LabuteASA, NumAliphaticCarbocycles, NumAliphaticHeterocycles, NumAliphaticRings, NumAmideBonds, NumAromaticCarbocycles, NumAromaticHeterocycles, NumAromaticRings, NumAtomStereoCenters, NumBridgeheadAtoms, NumHBA, NumHBD, NumHeteroatoms, NumHeterocycles, NumLipinskiHBA, NumLipinskiHBD, NumRings, NumSaturatedCarbocycles, NumSaturatedHeterocycles, NumSaturatedRings, NumSpiroAtoms, NumUnspecifiedAtomStereoCenters, TPSA. ``` Functional usage example: ```python import pandas as pd import janitor.chemistry df = pd.DataFrame(...) mol_desc = ( janitor.chemistry.molecular_descriptors( df=df.smiles2mol('smiles', 'mols'), mols_column_name='mols' ) ) ``` Method chaining usage example: ```python import pandas as pd import janitor.chemistry df = pd.DataFrame(...) mol_desc = ( df.smiles2mol('smiles', 'mols') .molecular_descriptors(mols_column_name='mols') ) ``` If you wish to join the molecular descriptors back into the original dataframe, this can be accomplished by doing a `join`, because the indices are preserved: ```python joined = df.join(mol_desc) ``` :param df: A pandas DataFrame. :param mols_column_name: The name of the column that has the RDKIT mol objects. :returns: A new pandas DataFrame of molecular descriptors. \"\"\" descriptors = [ CalcChi0n, CalcChi0v, CalcChi1n, CalcChi1v, CalcChi2n, CalcChi2v, CalcChi3n, CalcChi3v, CalcChi4n, CalcChi4v, CalcExactMolWt, CalcFractionCSP3, CalcHallKierAlpha, CalcKappa1, CalcKappa2, CalcKappa3, CalcLabuteASA, CalcNumAliphaticCarbocycles, CalcNumAliphaticHeterocycles, CalcNumAliphaticRings, CalcNumAmideBonds, CalcNumAromaticCarbocycles, CalcNumAromaticHeterocycles, CalcNumAromaticRings, CalcNumAtomStereoCenters, CalcNumBridgeheadAtoms, CalcNumHBA, CalcNumHBD, CalcNumHeteroatoms, CalcNumHeterocycles, CalcNumLipinskiHBA, CalcNumLipinskiHBD, CalcNumRings, CalcNumSaturatedCarbocycles, CalcNumSaturatedHeterocycles, CalcNumSaturatedRings, CalcNumSpiroAtoms, CalcNumUnspecifiedAtomStereoCenters, CalcTPSA, ] descriptors_mapping = {f.__name__.strip(\"Calc\"): f for f in descriptors} feats = dict() for name, func in descriptors_mapping.items(): feats[name] = [func(m) for m in df[mols_column_name]] return pd.DataFrame(feats) morgan_fingerprint(df, mols_column_name, radius=3, nbits=2048, kind='counts') Convert a column of RDKIT Mol objects into Morgan Fingerprints. Returns a new dataframe without any of the original data. This is intentional, as Morgan fingerprints are usually high-dimensional features. This method does not mutate the original DataFrame. Functional usage example: import pandas as pd import janitor.chemistry df = pd.DataFrame(...) # For \"counts\" kind morgans = janitor.chemistry.morgan_fingerprint( df=df.smiles2mol('smiles', 'mols'), mols_column_name='mols', radius=3, # Defaults to 3 nbits=2048, # Defaults to 2048 kind='counts' # Defaults to \"counts\" ) # For \"bits\" kind morgans = janitor.chemistry.morgan_fingerprint( df=df.smiles2mol('smiles', 'mols'), mols_column_name='mols', radius=3, # Defaults to 3 nbits=2048, # Defaults to 2048 kind='bits' # Defaults to \"counts\" ) Method chaining usage example: import pandas as pd import janitor.chemistry df = pd.DataFrame(...) # For \"counts\" kind morgans = ( df.smiles2mol('smiles', 'mols') .morgan_fingerprint( mols_column_name='mols', radius=3, # Defaults to 3 nbits=2048, # Defaults to 2048 kind='counts' # Defaults to \"counts\" ) ) # For \"bits\" kind morgans = ( df .smiles2mol('smiles', 'mols') .morgan_fingerprint( mols_column_name='mols', radius=3, # Defaults to 3 nbits=2048, # Defaults to 2048 kind='bits' # Defaults to \"counts\" ) ) If you wish to join the morgan fingerprints back into the original dataframe, this can be accomplished by doing a join , because the indices are preserved: joined = df.join(morgans) Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required mols_column_name str The name of the column that has the RDKIT mol objects required radius int Radius of Morgan fingerprints. Defaults to 3. 3 nbits int The length of the fingerprints. Defaults to 2048. 2048 kind str Whether to return counts or bits. Defaults to counts. 'counts' Returns: Type Description DataFrame A new pandas DataFrame of Morgan fingerprints. Exceptions: Type Description ValueError if kind is not one of \"counts\" or `\"bits\"``. Source code in janitor/chemistry.py @pf.register_dataframe_method @deprecated_alias(mols_col=\"mols_column_name\") def morgan_fingerprint( df: pd.DataFrame, mols_column_name: str, radius: int = 3, nbits: int = 2048, kind: str = \"counts\", ) -> pd.DataFrame: \"\"\" Convert a column of RDKIT Mol objects into Morgan Fingerprints. Returns a new dataframe without any of the original data. This is intentional, as Morgan fingerprints are usually high-dimensional features. This method does not mutate the original DataFrame. Functional usage example: ```python import pandas as pd import janitor.chemistry df = pd.DataFrame(...) # For \"counts\" kind morgans = janitor.chemistry.morgan_fingerprint( df=df.smiles2mol('smiles', 'mols'), mols_column_name='mols', radius=3, # Defaults to 3 nbits=2048, # Defaults to 2048 kind='counts' # Defaults to \"counts\" ) # For \"bits\" kind morgans = janitor.chemistry.morgan_fingerprint( df=df.smiles2mol('smiles', 'mols'), mols_column_name='mols', radius=3, # Defaults to 3 nbits=2048, # Defaults to 2048 kind='bits' # Defaults to \"counts\" ) ``` Method chaining usage example: ```python import pandas as pd import janitor.chemistry df = pd.DataFrame(...) # For \"counts\" kind morgans = ( df.smiles2mol('smiles', 'mols') .morgan_fingerprint( mols_column_name='mols', radius=3, # Defaults to 3 nbits=2048, # Defaults to 2048 kind='counts' # Defaults to \"counts\" ) ) # For \"bits\" kind morgans = ( df .smiles2mol('smiles', 'mols') .morgan_fingerprint( mols_column_name='mols', radius=3, # Defaults to 3 nbits=2048, # Defaults to 2048 kind='bits' # Defaults to \"counts\" ) ) ``` If you wish to join the morgan fingerprints back into the original dataframe, this can be accomplished by doing a `join`, because the indices are preserved: ```python joined = df.join(morgans) ``` :param df: A pandas DataFrame. :param mols_column_name: The name of the column that has the RDKIT mol objects :param radius: Radius of Morgan fingerprints. Defaults to 3. :param nbits: The length of the fingerprints. Defaults to 2048. :param kind: Whether to return counts or bits. Defaults to counts. :returns: A new pandas DataFrame of Morgan fingerprints. :raises ValueError: if `kind` is not one of `\"counts\"` or `\"bits\"``. \"\"\" acceptable_kinds = [\"counts\", \"bits\"] if kind not in acceptable_kinds: raise ValueError(f\"`kind` must be one of {acceptable_kinds}\") if kind == \"bits\": fps = [ GetMorganFingerprintAsBitVect(m, radius, nbits, useChirality=True) for m in df[mols_column_name] ] elif kind == \"counts\": fps = [ GetHashedMorganFingerprint(m, radius, nbits, useChirality=True) for m in df[mols_column_name] ] np_fps = [] for fp in fps: arr = np.zeros((1,)) DataStructs.ConvertToNumpyArray(fp, arr) np_fps.append(arr) np_fps = np.vstack(np_fps) fpdf = pd.DataFrame(np_fps) fpdf.index = df.index return fpdf smiles2mol(df, smiles_column_name, mols_column_name, drop_nulls=True, progressbar=None) Convert a column of SMILES strings into RDKit Mol objects. Automatically drops invalid SMILES, as determined by RDKIT. This method mutates the original DataFrame. Functional usage example: import pandas as pd import janitor.chemistry df = pd.DataFrame(...) df = janitor.chemistry.smiles2mol( df=df, smiles_column_name='smiles', mols_column_name='mols' ) Method chaining usage example: import pandas as pd import janitor.chemistry df = pd.DataFrame(...) df = df.smiles2mol( smiles_column_name='smiles', mols_column_name='mols' ) A progressbar can be optionally used. Pass in \"notebook\" to show a tqdm notebook progressbar. ( ipywidgets must be enabled with your Jupyter installation.) Pass in \"terminal\" to show a tqdm progressbar. Better suited for use with scripts. \"none\" is the default value - progress bar will be not be shown. Parameters: Name Type Description Default df DataFrame pandas DataFrame. required smiles_column_name Hashable Name of column that holds the SMILES strings. required mols_column_name Hashable Name to be given to the new mols column. required drop_nulls bool Whether to drop rows whose mols failed to be constructed. True progressbar Optional[str] Whether to show a progressbar or not. None Returns: Type Description DataFrame A pandas DataFrame with new RDKIT Mol objects column. Exceptions: Type Description ValueError if progressbar is not one of \"notebook\"``, \"terminal\" , or `None . Source code in janitor/chemistry.py @pf.register_dataframe_method @deprecated_alias(smiles_col=\"smiles_column_name\", mols_col=\"mols_column_name\") def smiles2mol( df: pd.DataFrame, smiles_column_name: Hashable, mols_column_name: Hashable, drop_nulls: bool = True, progressbar: Optional[str] = None, ) -> pd.DataFrame: \"\"\" Convert a column of SMILES strings into RDKit Mol objects. Automatically drops invalid SMILES, as determined by RDKIT. This method mutates the original DataFrame. Functional usage example: ```python import pandas as pd import janitor.chemistry df = pd.DataFrame(...) df = janitor.chemistry.smiles2mol( df=df, smiles_column_name='smiles', mols_column_name='mols' ) ``` Method chaining usage example: ```python import pandas as pd import janitor.chemistry df = pd.DataFrame(...) df = df.smiles2mol( smiles_column_name='smiles', mols_column_name='mols' ) ``` A progressbar can be optionally used. - Pass in \"notebook\" to show a `tqdm` notebook progressbar. (`ipywidgets` must be enabled with your Jupyter installation.) - Pass in \"terminal\" to show a `tqdm` progressbar. Better suited for use with scripts. - \"none\" is the default value - progress bar will be not be shown. :param df: pandas DataFrame. :param smiles_column_name: Name of column that holds the SMILES strings. :param mols_column_name: Name to be given to the new mols column. :param drop_nulls: Whether to drop rows whose mols failed to be constructed. :param progressbar: Whether to show a progressbar or not. :returns: A pandas DataFrame with new RDKIT Mol objects column. :raises ValueError: if `progressbar` is not one of `\"notebook\"``, `\"terminal\"``, or `None``. \"\"\" valid_progress = [\"notebook\", \"terminal\", None] if progressbar not in valid_progress: raise ValueError(f\"progressbar kwarg must be one of {valid_progress}\") if progressbar is None: df[mols_column_name] = df[smiles_column_name].apply( lambda x: Chem.MolFromSmiles(x) ) else: if progressbar == \"notebook\": tqdmn().pandas(desc=\"mols\") elif progressbar == \"terminal\": tqdm.pandas(desc=\"mols\") df[mols_column_name] = df[smiles_column_name].progress_apply( lambda x: Chem.MolFromSmiles(x) ) if drop_nulls: df = df.dropna(subset=[mols_column_name]) df = df.reset_index(drop=True) return df","title":"Chemistry"},{"location":"api/chemistry/#chemistry","text":"Chemistry and cheminformatics-oriented data cleaning functions.","title":"Chemistry"},{"location":"api/chemistry/#janitor.chemistry.maccs_keys_fingerprint","text":"Convert a column of RDKIT mol objects into MACCS Keys Fingerprints. Returns a new dataframe without any of the original data. This is intentional to leave the user with the data requested. This method does not mutate the original DataFrame. Functional usage example: import pandas as pd import janitor.chemistry df = pd.DataFrame(...) maccs = janitor.chemistry.maccs_keys_fingerprint( df=df.smiles2mol('smiles', 'mols'), mols_column_name='mols' ) Method chaining usage example: import pandas as pd import janitor.chemistry df = pd.DataFrame(...) maccs = ( df.smiles2mol('smiles', 'mols') .maccs_keys_fingerprint(mols_column_name='mols') ) If you wish to join the maccs keys fingerprints back into the original dataframe, this can be accomplished by doing a join , because the indices are preserved: joined = df.join(maccs_keys_fingerprint) Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required mols_column_name Hashable The name of the column that has the RDKIT mol objects. required Returns: Type Description DataFrame A new pandas DataFrame of MACCS keys fingerprints. Source code in janitor/chemistry.py @pf.register_dataframe_method @deprecated_alias(mols_col=\"mols_column_name\") def maccs_keys_fingerprint( df: pd.DataFrame, mols_column_name: Hashable ) -> pd.DataFrame: \"\"\" Convert a column of RDKIT mol objects into MACCS Keys Fingerprints. Returns a new dataframe without any of the original data. This is intentional to leave the user with the data requested. This method does not mutate the original DataFrame. Functional usage example: ```python import pandas as pd import janitor.chemistry df = pd.DataFrame(...) maccs = janitor.chemistry.maccs_keys_fingerprint( df=df.smiles2mol('smiles', 'mols'), mols_column_name='mols' ) ``` Method chaining usage example: ```python import pandas as pd import janitor.chemistry df = pd.DataFrame(...) maccs = ( df.smiles2mol('smiles', 'mols') .maccs_keys_fingerprint(mols_column_name='mols') ) ``` If you wish to join the maccs keys fingerprints back into the original dataframe, this can be accomplished by doing a `join`, because the indices are preserved: ```python joined = df.join(maccs_keys_fingerprint) ``` :param df: A pandas DataFrame. :param mols_column_name: The name of the column that has the RDKIT mol objects. :returns: A new pandas DataFrame of MACCS keys fingerprints. \"\"\" maccs = [GetMACCSKeysFingerprint(m) for m in df[mols_column_name]] np_maccs = [] for macc in maccs: arr = np.zeros((1,)) DataStructs.ConvertToNumpyArray(macc, arr) np_maccs.append(arr) np_maccs = np.vstack(np_maccs) fmaccs = pd.DataFrame(np_maccs) fmaccs.index = df.index return fmaccs","title":"maccs_keys_fingerprint()"},{"location":"api/chemistry/#janitor.chemistry.molecular_descriptors","text":"Convert a column of RDKIT mol objects into a Pandas DataFrame of molecular descriptors. Returns a new dataframe without any of the original data. This is intentional to leave the user only with the data requested. This method does not mutate the original DataFrame. The molecular descriptors are from the rdkit.Chem.rdMolDescriptors: Chi0n, Chi0v, Chi1n, Chi1v, Chi2n, Chi2v, Chi3n, Chi3v, Chi4n, Chi4v, ExactMolWt, FractionCSP3, HallKierAlpha, Kappa1, Kappa2, Kappa3, LabuteASA, NumAliphaticCarbocycles, NumAliphaticHeterocycles, NumAliphaticRings, NumAmideBonds, NumAromaticCarbocycles, NumAromaticHeterocycles, NumAromaticRings, NumAtomStereoCenters, NumBridgeheadAtoms, NumHBA, NumHBD, NumHeteroatoms, NumHeterocycles, NumLipinskiHBA, NumLipinskiHBD, NumRings, NumSaturatedCarbocycles, NumSaturatedHeterocycles, NumSaturatedRings, NumSpiroAtoms, NumUnspecifiedAtomStereoCenters, TPSA. Functional usage example: import pandas as pd import janitor.chemistry df = pd.DataFrame(...) mol_desc = ( janitor.chemistry.molecular_descriptors( df=df.smiles2mol('smiles', 'mols'), mols_column_name='mols' ) ) Method chaining usage example: import pandas as pd import janitor.chemistry df = pd.DataFrame(...) mol_desc = ( df.smiles2mol('smiles', 'mols') .molecular_descriptors(mols_column_name='mols') ) If you wish to join the molecular descriptors back into the original dataframe, this can be accomplished by doing a join , because the indices are preserved: joined = df.join(mol_desc) Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required mols_column_name Hashable The name of the column that has the RDKIT mol objects. required Returns: Type Description DataFrame A new pandas DataFrame of molecular descriptors. Source code in janitor/chemistry.py @pf.register_dataframe_method @deprecated_alias(mols_col=\"mols_column_name\") def molecular_descriptors( df: pd.DataFrame, mols_column_name: Hashable ) -> pd.DataFrame: \"\"\" Convert a column of RDKIT mol objects into a Pandas DataFrame of molecular descriptors. Returns a new dataframe without any of the original data. This is intentional to leave the user only with the data requested. This method does not mutate the original DataFrame. The molecular descriptors are from the rdkit.Chem.rdMolDescriptors: ``` Chi0n, Chi0v, Chi1n, Chi1v, Chi2n, Chi2v, Chi3n, Chi3v, Chi4n, Chi4v, ExactMolWt, FractionCSP3, HallKierAlpha, Kappa1, Kappa2, Kappa3, LabuteASA, NumAliphaticCarbocycles, NumAliphaticHeterocycles, NumAliphaticRings, NumAmideBonds, NumAromaticCarbocycles, NumAromaticHeterocycles, NumAromaticRings, NumAtomStereoCenters, NumBridgeheadAtoms, NumHBA, NumHBD, NumHeteroatoms, NumHeterocycles, NumLipinskiHBA, NumLipinskiHBD, NumRings, NumSaturatedCarbocycles, NumSaturatedHeterocycles, NumSaturatedRings, NumSpiroAtoms, NumUnspecifiedAtomStereoCenters, TPSA. ``` Functional usage example: ```python import pandas as pd import janitor.chemistry df = pd.DataFrame(...) mol_desc = ( janitor.chemistry.molecular_descriptors( df=df.smiles2mol('smiles', 'mols'), mols_column_name='mols' ) ) ``` Method chaining usage example: ```python import pandas as pd import janitor.chemistry df = pd.DataFrame(...) mol_desc = ( df.smiles2mol('smiles', 'mols') .molecular_descriptors(mols_column_name='mols') ) ``` If you wish to join the molecular descriptors back into the original dataframe, this can be accomplished by doing a `join`, because the indices are preserved: ```python joined = df.join(mol_desc) ``` :param df: A pandas DataFrame. :param mols_column_name: The name of the column that has the RDKIT mol objects. :returns: A new pandas DataFrame of molecular descriptors. \"\"\" descriptors = [ CalcChi0n, CalcChi0v, CalcChi1n, CalcChi1v, CalcChi2n, CalcChi2v, CalcChi3n, CalcChi3v, CalcChi4n, CalcChi4v, CalcExactMolWt, CalcFractionCSP3, CalcHallKierAlpha, CalcKappa1, CalcKappa2, CalcKappa3, CalcLabuteASA, CalcNumAliphaticCarbocycles, CalcNumAliphaticHeterocycles, CalcNumAliphaticRings, CalcNumAmideBonds, CalcNumAromaticCarbocycles, CalcNumAromaticHeterocycles, CalcNumAromaticRings, CalcNumAtomStereoCenters, CalcNumBridgeheadAtoms, CalcNumHBA, CalcNumHBD, CalcNumHeteroatoms, CalcNumHeterocycles, CalcNumLipinskiHBA, CalcNumLipinskiHBD, CalcNumRings, CalcNumSaturatedCarbocycles, CalcNumSaturatedHeterocycles, CalcNumSaturatedRings, CalcNumSpiroAtoms, CalcNumUnspecifiedAtomStereoCenters, CalcTPSA, ] descriptors_mapping = {f.__name__.strip(\"Calc\"): f for f in descriptors} feats = dict() for name, func in descriptors_mapping.items(): feats[name] = [func(m) for m in df[mols_column_name]] return pd.DataFrame(feats)","title":"molecular_descriptors()"},{"location":"api/chemistry/#janitor.chemistry.morgan_fingerprint","text":"Convert a column of RDKIT Mol objects into Morgan Fingerprints. Returns a new dataframe without any of the original data. This is intentional, as Morgan fingerprints are usually high-dimensional features. This method does not mutate the original DataFrame. Functional usage example: import pandas as pd import janitor.chemistry df = pd.DataFrame(...) # For \"counts\" kind morgans = janitor.chemistry.morgan_fingerprint( df=df.smiles2mol('smiles', 'mols'), mols_column_name='mols', radius=3, # Defaults to 3 nbits=2048, # Defaults to 2048 kind='counts' # Defaults to \"counts\" ) # For \"bits\" kind morgans = janitor.chemistry.morgan_fingerprint( df=df.smiles2mol('smiles', 'mols'), mols_column_name='mols', radius=3, # Defaults to 3 nbits=2048, # Defaults to 2048 kind='bits' # Defaults to \"counts\" ) Method chaining usage example: import pandas as pd import janitor.chemistry df = pd.DataFrame(...) # For \"counts\" kind morgans = ( df.smiles2mol('smiles', 'mols') .morgan_fingerprint( mols_column_name='mols', radius=3, # Defaults to 3 nbits=2048, # Defaults to 2048 kind='counts' # Defaults to \"counts\" ) ) # For \"bits\" kind morgans = ( df .smiles2mol('smiles', 'mols') .morgan_fingerprint( mols_column_name='mols', radius=3, # Defaults to 3 nbits=2048, # Defaults to 2048 kind='bits' # Defaults to \"counts\" ) ) If you wish to join the morgan fingerprints back into the original dataframe, this can be accomplished by doing a join , because the indices are preserved: joined = df.join(morgans) Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required mols_column_name str The name of the column that has the RDKIT mol objects required radius int Radius of Morgan fingerprints. Defaults to 3. 3 nbits int The length of the fingerprints. Defaults to 2048. 2048 kind str Whether to return counts or bits. Defaults to counts. 'counts' Returns: Type Description DataFrame A new pandas DataFrame of Morgan fingerprints. Exceptions: Type Description ValueError if kind is not one of \"counts\" or `\"bits\"``. Source code in janitor/chemistry.py @pf.register_dataframe_method @deprecated_alias(mols_col=\"mols_column_name\") def morgan_fingerprint( df: pd.DataFrame, mols_column_name: str, radius: int = 3, nbits: int = 2048, kind: str = \"counts\", ) -> pd.DataFrame: \"\"\" Convert a column of RDKIT Mol objects into Morgan Fingerprints. Returns a new dataframe without any of the original data. This is intentional, as Morgan fingerprints are usually high-dimensional features. This method does not mutate the original DataFrame. Functional usage example: ```python import pandas as pd import janitor.chemistry df = pd.DataFrame(...) # For \"counts\" kind morgans = janitor.chemistry.morgan_fingerprint( df=df.smiles2mol('smiles', 'mols'), mols_column_name='mols', radius=3, # Defaults to 3 nbits=2048, # Defaults to 2048 kind='counts' # Defaults to \"counts\" ) # For \"bits\" kind morgans = janitor.chemistry.morgan_fingerprint( df=df.smiles2mol('smiles', 'mols'), mols_column_name='mols', radius=3, # Defaults to 3 nbits=2048, # Defaults to 2048 kind='bits' # Defaults to \"counts\" ) ``` Method chaining usage example: ```python import pandas as pd import janitor.chemistry df = pd.DataFrame(...) # For \"counts\" kind morgans = ( df.smiles2mol('smiles', 'mols') .morgan_fingerprint( mols_column_name='mols', radius=3, # Defaults to 3 nbits=2048, # Defaults to 2048 kind='counts' # Defaults to \"counts\" ) ) # For \"bits\" kind morgans = ( df .smiles2mol('smiles', 'mols') .morgan_fingerprint( mols_column_name='mols', radius=3, # Defaults to 3 nbits=2048, # Defaults to 2048 kind='bits' # Defaults to \"counts\" ) ) ``` If you wish to join the morgan fingerprints back into the original dataframe, this can be accomplished by doing a `join`, because the indices are preserved: ```python joined = df.join(morgans) ``` :param df: A pandas DataFrame. :param mols_column_name: The name of the column that has the RDKIT mol objects :param radius: Radius of Morgan fingerprints. Defaults to 3. :param nbits: The length of the fingerprints. Defaults to 2048. :param kind: Whether to return counts or bits. Defaults to counts. :returns: A new pandas DataFrame of Morgan fingerprints. :raises ValueError: if `kind` is not one of `\"counts\"` or `\"bits\"``. \"\"\" acceptable_kinds = [\"counts\", \"bits\"] if kind not in acceptable_kinds: raise ValueError(f\"`kind` must be one of {acceptable_kinds}\") if kind == \"bits\": fps = [ GetMorganFingerprintAsBitVect(m, radius, nbits, useChirality=True) for m in df[mols_column_name] ] elif kind == \"counts\": fps = [ GetHashedMorganFingerprint(m, radius, nbits, useChirality=True) for m in df[mols_column_name] ] np_fps = [] for fp in fps: arr = np.zeros((1,)) DataStructs.ConvertToNumpyArray(fp, arr) np_fps.append(arr) np_fps = np.vstack(np_fps) fpdf = pd.DataFrame(np_fps) fpdf.index = df.index return fpdf","title":"morgan_fingerprint()"},{"location":"api/chemistry/#janitor.chemistry.smiles2mol","text":"Convert a column of SMILES strings into RDKit Mol objects. Automatically drops invalid SMILES, as determined by RDKIT. This method mutates the original DataFrame. Functional usage example: import pandas as pd import janitor.chemistry df = pd.DataFrame(...) df = janitor.chemistry.smiles2mol( df=df, smiles_column_name='smiles', mols_column_name='mols' ) Method chaining usage example: import pandas as pd import janitor.chemistry df = pd.DataFrame(...) df = df.smiles2mol( smiles_column_name='smiles', mols_column_name='mols' ) A progressbar can be optionally used. Pass in \"notebook\" to show a tqdm notebook progressbar. ( ipywidgets must be enabled with your Jupyter installation.) Pass in \"terminal\" to show a tqdm progressbar. Better suited for use with scripts. \"none\" is the default value - progress bar will be not be shown. Parameters: Name Type Description Default df DataFrame pandas DataFrame. required smiles_column_name Hashable Name of column that holds the SMILES strings. required mols_column_name Hashable Name to be given to the new mols column. required drop_nulls bool Whether to drop rows whose mols failed to be constructed. True progressbar Optional[str] Whether to show a progressbar or not. None Returns: Type Description DataFrame A pandas DataFrame with new RDKIT Mol objects column. Exceptions: Type Description ValueError if progressbar is not one of \"notebook\"``, \"terminal\" , or `None . Source code in janitor/chemistry.py @pf.register_dataframe_method @deprecated_alias(smiles_col=\"smiles_column_name\", mols_col=\"mols_column_name\") def smiles2mol( df: pd.DataFrame, smiles_column_name: Hashable, mols_column_name: Hashable, drop_nulls: bool = True, progressbar: Optional[str] = None, ) -> pd.DataFrame: \"\"\" Convert a column of SMILES strings into RDKit Mol objects. Automatically drops invalid SMILES, as determined by RDKIT. This method mutates the original DataFrame. Functional usage example: ```python import pandas as pd import janitor.chemistry df = pd.DataFrame(...) df = janitor.chemistry.smiles2mol( df=df, smiles_column_name='smiles', mols_column_name='mols' ) ``` Method chaining usage example: ```python import pandas as pd import janitor.chemistry df = pd.DataFrame(...) df = df.smiles2mol( smiles_column_name='smiles', mols_column_name='mols' ) ``` A progressbar can be optionally used. - Pass in \"notebook\" to show a `tqdm` notebook progressbar. (`ipywidgets` must be enabled with your Jupyter installation.) - Pass in \"terminal\" to show a `tqdm` progressbar. Better suited for use with scripts. - \"none\" is the default value - progress bar will be not be shown. :param df: pandas DataFrame. :param smiles_column_name: Name of column that holds the SMILES strings. :param mols_column_name: Name to be given to the new mols column. :param drop_nulls: Whether to drop rows whose mols failed to be constructed. :param progressbar: Whether to show a progressbar or not. :returns: A pandas DataFrame with new RDKIT Mol objects column. :raises ValueError: if `progressbar` is not one of `\"notebook\"``, `\"terminal\"``, or `None``. \"\"\" valid_progress = [\"notebook\", \"terminal\", None] if progressbar not in valid_progress: raise ValueError(f\"progressbar kwarg must be one of {valid_progress}\") if progressbar is None: df[mols_column_name] = df[smiles_column_name].apply( lambda x: Chem.MolFromSmiles(x) ) else: if progressbar == \"notebook\": tqdmn().pandas(desc=\"mols\") elif progressbar == \"terminal\": tqdm.pandas(desc=\"mols\") df[mols_column_name] = df[smiles_column_name].progress_apply( lambda x: Chem.MolFromSmiles(x) ) if drop_nulls: df = df.dropna(subset=[mols_column_name]) df = df.reset_index(drop=True) return df","title":"smiles2mol()"},{"location":"api/engineering/","text":"Engineering Engineering-specific data cleaning functions. convert_units(df, column_name=None, existing_units=None, to_units=None, dest_column_name=None) Converts a column of numeric values from one unit to another. Functional usage example: import pandas as pd import janitor.engineering df = pd.DataFrame(...) df = janitor.engineering.convert_units( df=df, column_name='temp_F', existing_units='degF', to_units='degC', dest_column_name='temp_C' ) Method chaining usage example: import pandas as pd import janitor.engineering df = pd.DataFrame(...) df = df.convert_units( column_name='temp_F', existing_units='degF', to_units='degC', dest_column_name='temp_C' ) Unit conversion can only take place if the existing_units and to_units are of the same type (e.g., temperature or pressure). The provided unit types can be any unit name or alternate name provided in the unyt package's Listing of Units table . Volume units are not provided natively in unyt . However, exponents are supported, and therefore some volume units can be converted. For example, a volume in cubic centimeters can be converted to cubic meters using existing_units='cm**3' and to_units='m**3' . Note : This method mutates the original DataFrame. Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name str Name of the column containing numeric values that are to be converted from one set of units to another. None existing_units str The unit type to convert from. None to_units str The unit type to convert to. None dest_column_name str The name of the new column containing the converted values that will be created. None Returns: Type Description DataFrame A pandas DataFrame with a new column of unit-converted values. Exceptions: Type Description TypeError if column is not numeric. Source code in janitor/engineering.py @pf.register_dataframe_method def convert_units( df: pd.DataFrame, column_name: str = None, existing_units: str = None, to_units: str = None, dest_column_name: str = None, ) -> pd.DataFrame: \"\"\" Converts a column of numeric values from one unit to another. Functional usage example: ```python import pandas as pd import janitor.engineering df = pd.DataFrame(...) df = janitor.engineering.convert_units( df=df, column_name='temp_F', existing_units='degF', to_units='degC', dest_column_name='temp_C' ) ``` Method chaining usage example: ```python import pandas as pd import janitor.engineering df = pd.DataFrame(...) df = df.convert_units( column_name='temp_F', existing_units='degF', to_units='degC', dest_column_name='temp_C' ) ``` Unit conversion can only take place if the `existing_units` and `to_units` are of the same type (e.g., temperature or pressure). The provided unit types can be any unit name or alternate name provided in the `unyt` package's [Listing of Units table]( https://unyt.readthedocs.io/en/stable/unit_listing.html#unit-listing). Volume units are not provided natively in `unyt`. However, exponents are supported, and therefore some volume units can be converted. For example, a volume in cubic centimeters can be converted to cubic meters using `existing_units='cm**3'` and `to_units='m**3'`. **Note**: This method mutates the original DataFrame. :param df: A pandas DataFrame. :param column_name: Name of the column containing numeric values that are to be converted from one set of units to another. :param existing_units: The unit type to convert from. :param to_units: The unit type to convert to. :param dest_column_name: The name of the new column containing the converted values that will be created. :returns: A pandas DataFrame with a new column of unit-converted values. :raises TypeError: if column is not numeric. \"\"\" # Check all inputs are correct data type check(\"column_name\", column_name, [str]) check(\"existing_units\", existing_units, [str]) check(\"to_units\", to_units, [str]) check(\"dest_column_name\", dest_column_name, [str]) # Check that column_name is a numeric column if not np.issubdtype(df[column_name].dtype, np.number): raise TypeError(f\"{column_name} must be a numeric column.\") original_vals = df[column_name].to_numpy() * unyt.Unit(existing_units) converted_vals = original_vals.to(to_units) df[dest_column_name] = np.array(converted_vals) return df","title":"Engineering"},{"location":"api/engineering/#engineering","text":"Engineering-specific data cleaning functions.","title":"Engineering"},{"location":"api/engineering/#janitor.engineering.convert_units","text":"Converts a column of numeric values from one unit to another. Functional usage example: import pandas as pd import janitor.engineering df = pd.DataFrame(...) df = janitor.engineering.convert_units( df=df, column_name='temp_F', existing_units='degF', to_units='degC', dest_column_name='temp_C' ) Method chaining usage example: import pandas as pd import janitor.engineering df = pd.DataFrame(...) df = df.convert_units( column_name='temp_F', existing_units='degF', to_units='degC', dest_column_name='temp_C' ) Unit conversion can only take place if the existing_units and to_units are of the same type (e.g., temperature or pressure). The provided unit types can be any unit name or alternate name provided in the unyt package's Listing of Units table . Volume units are not provided natively in unyt . However, exponents are supported, and therefore some volume units can be converted. For example, a volume in cubic centimeters can be converted to cubic meters using existing_units='cm**3' and to_units='m**3' . Note : This method mutates the original DataFrame. Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name str Name of the column containing numeric values that are to be converted from one set of units to another. None existing_units str The unit type to convert from. None to_units str The unit type to convert to. None dest_column_name str The name of the new column containing the converted values that will be created. None Returns: Type Description DataFrame A pandas DataFrame with a new column of unit-converted values. Exceptions: Type Description TypeError if column is not numeric. Source code in janitor/engineering.py @pf.register_dataframe_method def convert_units( df: pd.DataFrame, column_name: str = None, existing_units: str = None, to_units: str = None, dest_column_name: str = None, ) -> pd.DataFrame: \"\"\" Converts a column of numeric values from one unit to another. Functional usage example: ```python import pandas as pd import janitor.engineering df = pd.DataFrame(...) df = janitor.engineering.convert_units( df=df, column_name='temp_F', existing_units='degF', to_units='degC', dest_column_name='temp_C' ) ``` Method chaining usage example: ```python import pandas as pd import janitor.engineering df = pd.DataFrame(...) df = df.convert_units( column_name='temp_F', existing_units='degF', to_units='degC', dest_column_name='temp_C' ) ``` Unit conversion can only take place if the `existing_units` and `to_units` are of the same type (e.g., temperature or pressure). The provided unit types can be any unit name or alternate name provided in the `unyt` package's [Listing of Units table]( https://unyt.readthedocs.io/en/stable/unit_listing.html#unit-listing). Volume units are not provided natively in `unyt`. However, exponents are supported, and therefore some volume units can be converted. For example, a volume in cubic centimeters can be converted to cubic meters using `existing_units='cm**3'` and `to_units='m**3'`. **Note**: This method mutates the original DataFrame. :param df: A pandas DataFrame. :param column_name: Name of the column containing numeric values that are to be converted from one set of units to another. :param existing_units: The unit type to convert from. :param to_units: The unit type to convert to. :param dest_column_name: The name of the new column containing the converted values that will be created. :returns: A pandas DataFrame with a new column of unit-converted values. :raises TypeError: if column is not numeric. \"\"\" # Check all inputs are correct data type check(\"column_name\", column_name, [str]) check(\"existing_units\", existing_units, [str]) check(\"to_units\", to_units, [str]) check(\"dest_column_name\", dest_column_name, [str]) # Check that column_name is a numeric column if not np.issubdtype(df[column_name].dtype, np.number): raise TypeError(f\"{column_name} must be a numeric column.\") original_vals = df[column_name].to_numpy() * unyt.Unit(existing_units) converted_vals = original_vals.to(to_units) df[dest_column_name] = np.array(converted_vals) return df","title":"convert_units()"},{"location":"api/finance/","text":"Finance Finance-specific data cleaning functions. convert_currency(df, api_key, column_name=None, from_currency=None, to_currency=None, historical_date=None, make_new_column=False) Deprecated function. Source code in janitor/finance.py @pf.register_dataframe_method @deprecated_alias(colname=\"column_name\") def convert_currency( df: pd.DataFrame, api_key: str, column_name: str = None, from_currency: str = None, to_currency: str = None, historical_date: date = None, make_new_column: bool = False, ) -> pd.DataFrame: \"\"\"Deprecated function.\"\"\" raise JanitorError( \"The `convert_currency` function has been temporarily disabled due to \" \"exchangeratesapi.io disallowing free pinging of its API. \" \"(Our tests started to fail due to this issue.) \" \"There is no easy way around this problem \" \"except to find a new API to call on.\" \"Please comment on issue #829 \" \"(https://github.com/pyjanitor-devs/pyjanitor/issues/829) \" \"if you know of an alternative API that we can call on, \" \"otherwise the function will be removed in pyjanitor's 1.0 release.\" ) convert_stock(stock_symbol) This function takes in a stock symbol as a parameter, queries an API for the companies full name and returns it Functional usage example: import janitor.finance janitor.finance.convert_stock(\"aapl\") Parameters: Name Type Description Default stock_symbol str Stock ticker Symbol required Returns: Type Description str Full company name Exceptions: Type Description ConnectionError Internet connection is not available Source code in janitor/finance.py def convert_stock(stock_symbol: str) -> str: \"\"\" This function takes in a stock symbol as a parameter, queries an API for the companies full name and returns it Functional usage example: ```python import janitor.finance janitor.finance.convert_stock(\"aapl\") ``` :param stock_symbol: Stock ticker Symbol :raises ConnectionError: Internet connection is not available :returns: Full company name \"\"\" if is_connected(\"www.google.com\"): stock_symbol = stock_symbol.upper() return get_symbol(stock_symbol) else: raise ConnectionError( \"Connection Error: Client Not Connected to Internet\" ) get_symbol(symbol) This is a helper function to get a companies full name based on the stock symbol. Functional usage example: import janitor.finance janitor.finance.get_symbol(\"aapl\") Parameters: Name Type Description Default symbol str This is our stock symbol that we use to query the api for the companies full name. required Returns: Type Description Company full name Source code in janitor/finance.py def get_symbol(symbol: str): \"\"\" This is a helper function to get a companies full name based on the stock symbol. Functional usage example: ```python import janitor.finance janitor.finance.get_symbol(\"aapl\") ``` :param symbol: This is our stock symbol that we use to query the api for the companies full name. :return: Company full name \"\"\" result = requests.get( \"http://d.yimg.com/autoc.\" + \"finance.yahoo.com/autoc?query={}&region=1&lang=en\".format(symbol) ).json() for x in result[\"ResultSet\"][\"Result\"]: if x[\"symbol\"] == symbol: return x[\"name\"] else: return None inflate_currency(df, column_name=None, country=None, currency_year=None, to_year=None, make_new_column=False) Inflates a column of monetary values from one year to another, based on the currency's country. The provided country can be any economy name or code from the World Bank [list of economies] (https://databank.worldbank.org/data/download/site-content/CLASS.xls). Note : This method mutates the original DataFrame. Functional usage example: import pandas as pd import janitor.finance df = pd.DataFrame(...) df = janitor.finance.inflate_currency( df=df, column_name='profit', country='USA', currency_year=2015, to_year=2018, make_new_column=True ) Method chaining usage example: import pandas as pd import janitor.finance df = pd.DataFrame(...) df = df.inflate_currency( column_name='profit', country='USA', currency_year=2015, to_year=2018, make_new_column=True ) Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name str Name of the column containing monetary values to inflate. None country str The country associated with the currency being inflated. May be any economy or code from the World Bank [List of economies] (https://databank.worldbank.org/data/download/site-content/CLASS.xls). None currency_year int The currency year to inflate from. The year should be 1960 or later. None to_year int The currency year to inflate to. The year should be 1960 or later. None make_new_column bool Generates new column for inflated currency if True, otherwise, inflates currency in place. False Returns: Type Description DataFrame The dataframe with inflated currency column. Source code in janitor/finance.py @pf.register_dataframe_method def inflate_currency( df: pd.DataFrame, column_name: str = None, country: str = None, currency_year: int = None, to_year: int = None, make_new_column: bool = False, ) -> pd.DataFrame: \"\"\" Inflates a column of monetary values from one year to another, based on the currency's country. The provided country can be any economy name or code from the World Bank [list of economies] (https://databank.worldbank.org/data/download/site-content/CLASS.xls). **Note**: This method mutates the original DataFrame. Functional usage example: ```python import pandas as pd import janitor.finance df = pd.DataFrame(...) df = janitor.finance.inflate_currency( df=df, column_name='profit', country='USA', currency_year=2015, to_year=2018, make_new_column=True ) ``` Method chaining usage example: ```python import pandas as pd import janitor.finance df = pd.DataFrame(...) df = df.inflate_currency( column_name='profit', country='USA', currency_year=2015, to_year=2018, make_new_column=True ) ``` :param df: A pandas DataFrame. :param column_name: Name of the column containing monetary values to inflate. :param country: The country associated with the currency being inflated. May be any economy or code from the World Bank [List of economies] (https://databank.worldbank.org/data/download/site-content/CLASS.xls). :param currency_year: The currency year to inflate from. The year should be 1960 or later. :param to_year: The currency year to inflate to. The year should be 1960 or later. :param make_new_column: Generates new column for inflated currency if True, otherwise, inflates currency in place. :returns: The dataframe with inflated currency column. \"\"\" inflator = _inflate_currency(country, currency_year, to_year) if make_new_column: new_column_name = column_name + \"_\" + str(to_year) df[new_column_name] = df[column_name] * inflator else: df[column_name] = df[column_name] * inflator return df","title":"Finance"},{"location":"api/finance/#finance","text":"Finance-specific data cleaning functions.","title":"Finance"},{"location":"api/finance/#janitor.finance.convert_currency","text":"Deprecated function. Source code in janitor/finance.py @pf.register_dataframe_method @deprecated_alias(colname=\"column_name\") def convert_currency( df: pd.DataFrame, api_key: str, column_name: str = None, from_currency: str = None, to_currency: str = None, historical_date: date = None, make_new_column: bool = False, ) -> pd.DataFrame: \"\"\"Deprecated function.\"\"\" raise JanitorError( \"The `convert_currency` function has been temporarily disabled due to \" \"exchangeratesapi.io disallowing free pinging of its API. \" \"(Our tests started to fail due to this issue.) \" \"There is no easy way around this problem \" \"except to find a new API to call on.\" \"Please comment on issue #829 \" \"(https://github.com/pyjanitor-devs/pyjanitor/issues/829) \" \"if you know of an alternative API that we can call on, \" \"otherwise the function will be removed in pyjanitor's 1.0 release.\" )","title":"convert_currency()"},{"location":"api/finance/#janitor.finance.convert_stock","text":"This function takes in a stock symbol as a parameter, queries an API for the companies full name and returns it Functional usage example: import janitor.finance janitor.finance.convert_stock(\"aapl\") Parameters: Name Type Description Default stock_symbol str Stock ticker Symbol required Returns: Type Description str Full company name Exceptions: Type Description ConnectionError Internet connection is not available Source code in janitor/finance.py def convert_stock(stock_symbol: str) -> str: \"\"\" This function takes in a stock symbol as a parameter, queries an API for the companies full name and returns it Functional usage example: ```python import janitor.finance janitor.finance.convert_stock(\"aapl\") ``` :param stock_symbol: Stock ticker Symbol :raises ConnectionError: Internet connection is not available :returns: Full company name \"\"\" if is_connected(\"www.google.com\"): stock_symbol = stock_symbol.upper() return get_symbol(stock_symbol) else: raise ConnectionError( \"Connection Error: Client Not Connected to Internet\" )","title":"convert_stock()"},{"location":"api/finance/#janitor.finance.get_symbol","text":"This is a helper function to get a companies full name based on the stock symbol. Functional usage example: import janitor.finance janitor.finance.get_symbol(\"aapl\") Parameters: Name Type Description Default symbol str This is our stock symbol that we use to query the api for the companies full name. required Returns: Type Description Company full name Source code in janitor/finance.py def get_symbol(symbol: str): \"\"\" This is a helper function to get a companies full name based on the stock symbol. Functional usage example: ```python import janitor.finance janitor.finance.get_symbol(\"aapl\") ``` :param symbol: This is our stock symbol that we use to query the api for the companies full name. :return: Company full name \"\"\" result = requests.get( \"http://d.yimg.com/autoc.\" + \"finance.yahoo.com/autoc?query={}&region=1&lang=en\".format(symbol) ).json() for x in result[\"ResultSet\"][\"Result\"]: if x[\"symbol\"] == symbol: return x[\"name\"] else: return None","title":"get_symbol()"},{"location":"api/finance/#janitor.finance.inflate_currency","text":"Inflates a column of monetary values from one year to another, based on the currency's country. The provided country can be any economy name or code from the World Bank [list of economies] (https://databank.worldbank.org/data/download/site-content/CLASS.xls). Note : This method mutates the original DataFrame. Functional usage example: import pandas as pd import janitor.finance df = pd.DataFrame(...) df = janitor.finance.inflate_currency( df=df, column_name='profit', country='USA', currency_year=2015, to_year=2018, make_new_column=True ) Method chaining usage example: import pandas as pd import janitor.finance df = pd.DataFrame(...) df = df.inflate_currency( column_name='profit', country='USA', currency_year=2015, to_year=2018, make_new_column=True ) Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name str Name of the column containing monetary values to inflate. None country str The country associated with the currency being inflated. May be any economy or code from the World Bank [List of economies] (https://databank.worldbank.org/data/download/site-content/CLASS.xls). None currency_year int The currency year to inflate from. The year should be 1960 or later. None to_year int The currency year to inflate to. The year should be 1960 or later. None make_new_column bool Generates new column for inflated currency if True, otherwise, inflates currency in place. False Returns: Type Description DataFrame The dataframe with inflated currency column. Source code in janitor/finance.py @pf.register_dataframe_method def inflate_currency( df: pd.DataFrame, column_name: str = None, country: str = None, currency_year: int = None, to_year: int = None, make_new_column: bool = False, ) -> pd.DataFrame: \"\"\" Inflates a column of monetary values from one year to another, based on the currency's country. The provided country can be any economy name or code from the World Bank [list of economies] (https://databank.worldbank.org/data/download/site-content/CLASS.xls). **Note**: This method mutates the original DataFrame. Functional usage example: ```python import pandas as pd import janitor.finance df = pd.DataFrame(...) df = janitor.finance.inflate_currency( df=df, column_name='profit', country='USA', currency_year=2015, to_year=2018, make_new_column=True ) ``` Method chaining usage example: ```python import pandas as pd import janitor.finance df = pd.DataFrame(...) df = df.inflate_currency( column_name='profit', country='USA', currency_year=2015, to_year=2018, make_new_column=True ) ``` :param df: A pandas DataFrame. :param column_name: Name of the column containing monetary values to inflate. :param country: The country associated with the currency being inflated. May be any economy or code from the World Bank [List of economies] (https://databank.worldbank.org/data/download/site-content/CLASS.xls). :param currency_year: The currency year to inflate from. The year should be 1960 or later. :param to_year: The currency year to inflate to. The year should be 1960 or later. :param make_new_column: Generates new column for inflated currency if True, otherwise, inflates currency in place. :returns: The dataframe with inflated currency column. \"\"\" inflator = _inflate_currency(country, currency_year, to_year) if make_new_column: new_column_name = column_name + \"_\" + str(to_year) df[new_column_name] = df[column_name] * inflator else: df[column_name] = df[column_name] * inflator return df","title":"inflate_currency()"},{"location":"api/functions/","text":"Functions General Functions pyjanitor's general-purpose data cleaning functions. NOTE: Instructions for future contributors: Place the source code of the functions in a file named after the function. Place utility functions in the same file. If you use a utility function from another source file, please refactor it out to janitor.functions.utils . Import the function into this file so that it shows up in the top-level API. Sort the imports in alphabetical order. Try to group related functions together (e.g. see convert_date.py ) Never import utils. add_columns add_column(df, column_name, value, fill_remaining=False) Add a column to the dataframe. This method does not mutate the original DataFrame. Intended to be the method-chaining alternative to:: df[column_name] = value Method chaining syntax adding a column with only a single value: # This will add a column with only one value. df = pd.DataFrame(...).add_column(column_name=\"new_column\", 2) Method chaining syntax adding a column with more than one value: # This will add a column with an iterable of values. vals = [1, 2, 5, ..., 3, 4] # of same length as the dataframe. df = pd.DataFrame(...).add_column(column_name=\"new_column\", vals) Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name str Name of the new column. Should be a string, in order for the column name to be compatible with the Feather binary format (this is a useful thing to have). required value Union[List[Any], Tuple[Any], Any] Either a single value, or a list/tuple of values. required fill_remaining bool If value is a tuple or list that is smaller than the number of rows in the DataFrame, repeat the list or tuple (R-style) to the end of the DataFrame. False Returns: Type Description DataFrame A pandas DataFrame with an added column. Exceptions: Type Description ValueError if attempting to add a column that already exists. ValueError if value has more elements that number of rows in the DataFrame. ValueError if attempting to add an iterable of values with a length not equal to the number of DataFrame rows. ValueError if value has length of `0``. Source code in janitor/functions/add_columns.py @pf.register_dataframe_method @deprecated_alias(col_name=\"column_name\") def add_column( df: pd.DataFrame, column_name: str, value: Union[List[Any], Tuple[Any], Any], fill_remaining: bool = False, ) -> pd.DataFrame: \"\"\"Add a column to the dataframe. This method does not mutate the original DataFrame. Intended to be the method-chaining alternative to:: df[column_name] = value Method chaining syntax adding a column with only a single value: # This will add a column with only one value. df = pd.DataFrame(...).add_column(column_name=\"new_column\", 2) Method chaining syntax adding a column with more than one value: # This will add a column with an iterable of values. vals = [1, 2, 5, ..., 3, 4] # of same length as the dataframe. df = pd.DataFrame(...).add_column(column_name=\"new_column\", vals) :param df: A pandas DataFrame. :param column_name: Name of the new column. Should be a string, in order for the column name to be compatible with the Feather binary format (this is a useful thing to have). :param value: Either a single value, or a list/tuple of values. :param fill_remaining: If value is a tuple or list that is smaller than the number of rows in the DataFrame, repeat the list or tuple (R-style) to the end of the DataFrame. :returns: A pandas DataFrame with an added column. :raises ValueError: if attempting to add a column that already exists. :raises ValueError: if `value` has more elements that number of rows in the DataFrame. :raises ValueError: if attempting to add an iterable of values with a length not equal to the number of DataFrame rows. :raises ValueError: if `value` has length of `0``. \"\"\" # TODO: Convert examples to notebook. # :Setup: # ```python # import pandas as pd # import janitor # data = { # \"a\": [1, 2, 3] * 3, # \"Bell__Chart\": [1, 2, 3] * 3, # \"decorated-elephant\": [1, 2, 3] * 3, # \"animals\": [\"rabbit\", \"leopard\", \"lion\"] * 3, # \"cities\": [\"Cambridge\", \"Shanghai\", \"Basel\"] * 3, # } # df = pd.DataFrame(data) # :Example 1: Create a new column with a single value: # ```python # df.add_column(\"city_pop\", 100000) # :Output: # ```python # a Bell__Chart decorated-elephant animals cities city_pop # 0 1 1 1 rabbit Cambridge 100000 # 1 2 2 2 leopard Shanghai 100000 # 2 3 3 3 lion Basel 100000 # 3 1 1 1 rabbit Cambridge 100000 # 4 2 2 2 leopard Shanghai 100000 # 5 3 3 3 lion Basel 100000 # 6 1 1 1 rabbit Cambridge 100000 # 7 2 2 2 leopard Shanghai 100000 # 8 3 3 3 lion Basel 100000 # :Example 2: Create a new column with an iterator which fills to the # column # size: # ```python # df.add_column(\"city_pop\", range(3), fill_remaining=True) # :Output: # ```python # a Bell__Chart decorated-elephant animals cities city_pop # 0 1 1 1 rabbit Cambridge 0 # 1 2 2 2 leopard Shanghai 1 # 2 3 3 3 lion Basel 2 # 3 1 1 1 rabbit Cambridge 0 # 4 2 2 2 leopard Shanghai 1 # 5 3 3 3 lion Basel 2 # 6 1 1 1 rabbit Cambridge 0 # 7 2 2 2 leopard Shanghai 1 # 8 3 3 3 lion Basel 2 # :Example 3: Add new column based on mutation of other columns: # ```python # df.add_column(\"city_pop\", df.Bell__Chart - 2 * df.a) # :Output: # ```python # a Bell__Chart decorated-elephant animals cities city_pop # 0 1 1 1 rabbit Cambridge -1 # 1 2 2 2 leopard Shanghai -2 # 2 3 3 3 lion Basel -3 # 3 1 1 1 rabbit Cambridge -1 # 4 2 2 2 leopard Shanghai -2 # 5 3 3 3 lion Basel -3 # 6 1 1 1 rabbit Cambridge -1 # 7 2 2 2 leopard Shanghai -2 # 8 3 3 3 lion Basel -3 df = df.copy() check(\"column_name\", column_name, [str]) if column_name in df.columns: raise ValueError( f\"Attempted to add column that already exists: \" f\"{column_name}.\" ) nrows = df.shape[0] if hasattr(value, \"__len__\") and not isinstance( value, (str, bytes, bytearray) ): # if `value` is a list, ndarray, etc. if len(value) > nrows: raise ValueError( \"`value` has more elements than number of rows \" f\"in your `DataFrame`. vals: {len(value)}, \" f\"df: {nrows}\" ) if len(value) != nrows and not fill_remaining: raise ValueError( \"Attempted to add iterable of values with length\" \" not equal to number of DataFrame rows\" ) if len(value) == 0: raise ValueError( \"`value` has to be an iterable of minimum length 1\" ) len_value = len(value) elif fill_remaining: # relevant if a scalar val was passed, yet fill_remaining == True len_value = 1 value = [value] nrows = df.shape[0] if fill_remaining: times_to_loop = int(np.ceil(nrows / len_value)) fill_values = list(value) * times_to_loop df[column_name] = fill_values[:nrows] else: df[column_name] = value return df add_columns(df, fill_remaining=False, **kwargs) Add multiple columns to the dataframe. This method does not mutate the original DataFrame. Method to augment add_column with ability to add multiple columns in one go. This replaces the need for multiple add_column calls. Usage is through supplying kwargs where the key is the col name and the values correspond to the values of the new DataFrame column. Values passed can be scalar or iterable (list, ndarray, etc.) Usage example: x = 3 y = np.arange(0, 10) df = pd.DataFrame(...).add_columns(x=x, y=y) Parameters: Name Type Description Default df DataFrame A pandas dataframe. required fill_remaining bool If value is a tuple or list that is smaller than the number of rows in the DataFrame, repeat the list or tuple (R-style) to the end of the DataFrame. (Passed to add_column ) False kwargs column, value pairs which are looped through in add_column calls. {} Returns: Type Description DataFrame A pandas DataFrame with added columns. Source code in janitor/functions/add_columns.py @pf.register_dataframe_method def add_columns( df: pd.DataFrame, fill_remaining: bool = False, **kwargs ) -> pd.DataFrame: \"\"\"Add multiple columns to the dataframe. This method does not mutate the original DataFrame. Method to augment `add_column` with ability to add multiple columns in one go. This replaces the need for multiple `add_column` calls. Usage is through supplying kwargs where the key is the col name and the values correspond to the values of the new DataFrame column. Values passed can be scalar or iterable (list, ndarray, etc.) Usage example: x = 3 y = np.arange(0, 10) df = pd.DataFrame(...).add_columns(x=x, y=y) :param df: A pandas dataframe. :param fill_remaining: If value is a tuple or list that is smaller than the number of rows in the DataFrame, repeat the list or tuple (R-style) to the end of the DataFrame. (Passed to `add_column`) :param kwargs: column, value pairs which are looped through in `add_column` calls. :returns: A pandas DataFrame with added columns. \"\"\" # Note: error checking can pretty much be handled in `add_column` for col_name, values in kwargs.items(): df = df.add_column(col_name, values, fill_remaining=fill_remaining) return df also Implementation source for chainable function also . also(df, func, *args, **kwargs) Run a function with side effects. THis function allows you to run an arbitrary function in the pyjanitor method chain. Doing so will let you do things like save the dataframe to disk midway while continuing to modify the dataframe afterwards. Example usage: df = ( pd.DataFrame(...) .query(...) .also(lambda df: print(f\"DataFrame shape is: {df.shape}\")) .transform_column(...) .also(lambda df: df.to_csv(\"midpoint.csv\")) .also( lambda df: print( f\"Column col_name has these values: {set(df['col_name'].unique())}\" ) ) .group_add(...) ) Parameters: Name Type Description Default df DataFrame A pandas dataframe. required func Callable A function you would like to run in the method chain. It should take one DataFrame object as a parameter and have no return. If there is a return, it will be ignored. required args Optional arguments for func . () kwargs Optional keyword arguments for func . {} Returns: Type Description DataFrame The input pandas DataFrame. Source code in janitor/functions/also.py @pf.register_dataframe_method def also(df: pd.DataFrame, func: Callable, *args, **kwargs) -> pd.DataFrame: \"\"\" Run a function with side effects. THis function allows you to run an arbitrary function in the `pyjanitor` method chain. Doing so will let you do things like save the dataframe to disk midway while continuing to modify the dataframe afterwards. Example usage: ```python df = ( pd.DataFrame(...) .query(...) .also(lambda df: print(f\"DataFrame shape is: {df.shape}\")) .transform_column(...) .also(lambda df: df.to_csv(\"midpoint.csv\")) .also( lambda df: print( f\"Column col_name has these values: {set(df['col_name'].unique())}\" ) ) .group_add(...) ) ``` :param df: A pandas dataframe. :param func: A function you would like to run in the method chain. It should take one DataFrame object as a parameter and have no return. If there is a return, it will be ignored. :param args: Optional arguments for `func`. :param kwargs: Optional keyword arguments for `func`. :returns: The input pandas DataFrame. \"\"\" # noqa: E501 func(df.copy(), *args, **kwargs) return df bin_numeric bin_numeric(df, from_column_name, to_column_name, num_bins=5, labels=None) Generate a new column that labels bins for a specified numeric column. This method mutates the original DataFrame. Makes use of pandas cut() function to bin data of one column, generating a new column with the results. import pandas as pd import janitor df = ( pd.DataFrame(...) .bin_numeric( from_column_name='col1', to_column_name='col1_binned', num_bins=3, labels=['1-2', '3-4', '5-6'] ) ) Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required from_column_name Hashable The column whose data you want binned. required to_column_name Hashable The new column to be created with the binned data. required num_bins int The number of bins to be utilized. 5 labels Optional[str] Optionally rename numeric bin ranges with labels. Number of label names must match number of bins specified. None Returns: Type Description DataFrame A pandas DataFrame. Exceptions: Type Description ValueError if number of labels do not match number of bins. Source code in janitor/functions/bin_numeric.py @pf.register_dataframe_method @deprecated_alias(from_column=\"from_column_name\", to_column=\"to_column_name\") def bin_numeric( df: pd.DataFrame, from_column_name: Hashable, to_column_name: Hashable, num_bins: int = 5, labels: Optional[str] = None, ) -> pd.DataFrame: \"\"\" Generate a new column that labels bins for a specified numeric column. This method mutates the original DataFrame. Makes use of pandas `cut()` function to bin data of one column, generating a new column with the results. ```python import pandas as pd import janitor df = ( pd.DataFrame(...) .bin_numeric( from_column_name='col1', to_column_name='col1_binned', num_bins=3, labels=['1-2', '3-4', '5-6'] ) ) ``` :param df: A pandas DataFrame. :param from_column_name: The column whose data you want binned. :param to_column_name: The new column to be created with the binned data. :param num_bins: The number of bins to be utilized. :param labels: Optionally rename numeric bin ranges with labels. Number of label names must match number of bins specified. :return: A pandas DataFrame. :raises ValueError: if number of labels do not match number of bins. \"\"\" if not labels: df[str(to_column_name)] = pd.cut( df[str(from_column_name)], bins=num_bins ) else: if not len(labels) == num_bins: raise ValueError(\"Number of labels must match number of bins.\") df[str(to_column_name)] = pd.cut( df[str(from_column_name)], bins=num_bins, labels=labels ) return df case_when case_when(df, *args, *, column_name) Convenience function for creating a column, based on a condition, or multiple conditions. It is similar to SQL and dplyr's case_when, with inspiration from pydatatable if_else function. If your scenario requires direct replacement of values, pandas' replace method or map method should be better suited and more efficient; if the conditions check if a value is within a range of values, pandas' cut or qcut should be more efficient; np.where/np.select are also performant options. This function relies on pd.Series.mask method. When multiple conditions are satisfied, the first one is used. The variable *args parameters takes arguments of the form : condition0 , value0 , condition1 , value1 , ..., default . If condition0 evaluates to True , then assign value0 to column_name , if condition1 evaluates to True , then assign value1 to column_name , and so on. If none of the conditions evaluate to True , assign default to column_name . This function can be likened to SQL's case_when : CASE WHEN condition0 THEN value0 WHEN condition1 THEN value1 --- more conditions ELSE default END AS column_name compared to python's if-elif-else : if condition0: value0 elif condition1: value1 # more elifs else: default Functional usage syntax: import pandas as pd import janitor as jn df = pd.DataFrame(...) right = pd.DataFrame(...) df = jn.case_when( df, condition0, result0, condition1, result1, ..., default, column_name = 'column', ) Method chaining syntax: .. code-block:: python df = df.case_when( condition0, result0, condition1, result1, ..., default, column_name = 'column', ) Parameters: Name Type Description Default df DataFrame A Pandas dataframe. required args Variable argument of conditions and expected values. Takes the form condition0 , value0 , condition1 , value1 , ..., default . condition can be a 1-D boolean array, a callable, or a string. If condition is a callable, it should evaluate to a 1-D boolean array. The array should have the same length as the DataFrame. If it is a string, it is computed on the dataframe, via df.eval , and should return a 1-D boolean array. result can be a scalar, a 1-D array, or a callable. If result is a callable, it should evaluate to a 1-D array. For a 1-D array, it should have the same length as the DataFrame. The default argument applies if none of condition0 , condition1 , ..., evaluates to True . Value can be a scalar, a callabe, or a 1-D array. if default is a callable, it should evaluate to a 1-D array. The 1-D array should be the same length as the DataFrame. () column_name str Name of column to assign results to. A new column is created, if it does not already exist in the DataFrame. required Returns: Type Description DataFrame A pandas DataFrame. Exceptions: Type Description ValueError if the condition fails to evaluate. Source code in janitor/functions/case_when.py @pf.register_dataframe_method def case_when(df: pd.DataFrame, *args, column_name: str) -> pd.DataFrame: \"\"\" Convenience function for creating a column, based on a condition, or multiple conditions. It is similar to SQL and dplyr's case_when, with inspiration from `pydatatable` if_else function. If your scenario requires direct replacement of values, pandas' `replace` method or `map` method should be better suited and more efficient; if the conditions check if a value is within a range of values, pandas' `cut` or `qcut` should be more efficient; `np.where/np.select` are also performant options. This function relies on `pd.Series.mask` method. When multiple conditions are satisfied, the first one is used. The variable `*args` parameters takes arguments of the form : `condition0`, `value0`, `condition1`, `value1`, ..., `default`. If `condition0` evaluates to `True`, then assign `value0` to `column_name`, if `condition1` evaluates to `True`, then assign `value1` to `column_name`, and so on. If none of the conditions evaluate to `True`, assign `default` to `column_name`. This function can be likened to SQL's `case_when`: ```sql CASE WHEN condition0 THEN value0 WHEN condition1 THEN value1 --- more conditions ELSE default END AS column_name ``` compared to python's `if-elif-else`: ```python if condition0: value0 elif condition1: value1 # more elifs else: default ``` Functional usage syntax: ```python import pandas as pd import janitor as jn df = pd.DataFrame(...) right = pd.DataFrame(...) df = jn.case_when( df, condition0, result0, condition1, result1, ..., default, column_name = 'column', ) ``` Method chaining syntax: .. code-block:: python ```python df = df.case_when( condition0, result0, condition1, result1, ..., default, column_name = 'column', ) ``` :param df: A Pandas dataframe. :param args: Variable argument of conditions and expected values. Takes the form `condition0`, `value0`, `condition1`, `value1`, ..., `default`. `condition` can be a 1-D boolean array, a callable, or a string. If `condition` is a callable, it should evaluate to a 1-D boolean array. The array should have the same length as the DataFrame. If it is a string, it is computed on the dataframe, via `df.eval`, and should return a 1-D boolean array. `result` can be a scalar, a 1-D array, or a callable. If `result` is a callable, it should evaluate to a 1-D array. For a 1-D array, it should have the same length as the DataFrame. The `default` argument applies if none of `condition0`, `condition1`, ..., evaluates to `True`. Value can be a scalar, a callabe, or a 1-D array. if `default` is a callable, it should evaluate to a 1-D array. The 1-D array should be the same length as the DataFrame. :param column_name: Name of column to assign results to. A new column is created, if it does not already exist in the DataFrame. :raises ValueError: if the condition fails to evaluate. :returns: A pandas DataFrame. \"\"\" conditions, targets, default = _case_when_checks(df, args, column_name) if len(conditions) == 1: default = default.mask(conditions[0], targets[0]) return df.assign(**{column_name: default}) # ensures value assignment is on a first come basis conditions = conditions[::-1] targets = targets[::-1] for condition, value, index in zip(conditions, targets, count()): try: default = default.mask(condition, value) # error `feedoff` idea from SO # https://stackoverflow.com/a/46091127/7175713 except Exception as e: raise ValueError( f\"\"\" condition{index} and value{index} failed to evaluate. Original error message: {e} \"\"\" ) from e return df.assign(**{column_name: default}) change_type change_type(df, column_name, dtype, ignore_exception=False) Change the type of a column. This method mutates the original DataFrame. Exceptions that are raised can be ignored. For example, if one has a mixed dtype column that has non-integer strings and integers, and you want to coerce everything to integers, you can optionally ignore the non-integer strings and replace them with NaN or keep the original value Intended to be the method-chaining alternative to: df[col] = df[col].astype(dtype) Method chaining syntax: df = pd.DataFrame(...).change_type('col1', str) Parameters: Name Type Description Default df DataFrame A pandas dataframe. required column_name Hashable A column in the dataframe. required dtype type The datatype to convert to. Should be one of the standard Python types, or a numpy datatype. required ignore_exception bool one of `{False, \"fillna\", \"keep_values\"}``. False Returns: Type Description DataFrame A pandas DataFrame with changed column types. Exceptions: Type Description ValueError if unknown option provided for `ignore_exception``. Source code in janitor/functions/change_type.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def change_type( df: pd.DataFrame, column_name: Hashable, dtype: type, ignore_exception: bool = False, ) -> pd.DataFrame: \"\"\"Change the type of a column. This method mutates the original DataFrame. Exceptions that are raised can be ignored. For example, if one has a mixed dtype column that has non-integer strings and integers, and you want to coerce everything to integers, you can optionally ignore the non-integer strings and replace them with `NaN` or keep the original value Intended to be the method-chaining alternative to: ```python df[col] = df[col].astype(dtype) ``` Method chaining syntax: ```python df = pd.DataFrame(...).change_type('col1', str) ``` :param df: A pandas dataframe. :param column_name: A column in the dataframe. :param dtype: The datatype to convert to. Should be one of the standard Python types, or a numpy datatype. :param ignore_exception: one of `{False, \"fillna\", \"keep_values\"}``. :returns: A pandas DataFrame with changed column types. :raises ValueError: if unknown option provided for `ignore_exception``. \"\"\" if not ignore_exception: df[column_name] = df[column_name].astype(dtype) elif ignore_exception == \"keep_values\": df[column_name] = df[column_name].astype(dtype, errors=\"ignore\") elif ignore_exception == \"fillna\": # returns None when conversion def convert(x, dtype): try: return dtype(x) except ValueError: return None df[column_name] = df[column_name].apply(lambda x: convert(x, dtype)) else: raise ValueError(\"unknown option for ignore_exception\") return df clean_names Functions for cleaning columns names. clean_names(df, strip_underscores=None, case_type='lower', remove_special=False, strip_accents=True, preserve_original_columns=True, enforce_string=True, truncate_limit=None) Clean column names. Takes all column names, converts them to lowercase, then replaces all spaces with underscores. By default, column names are converted to string types. This can be switched off by passing in enforce_string=False . This method does not mutate the original DataFrame. Functional usage syntax: df = clean_names(df) Method chaining syntax: ```python import pandas as pd import janitor df = pd.DataFrame(...).clean_names() Examples of transformation: Columns before: First Name, Last Name, Employee Status, Subject Columns after: first_name, last_name, employee_status, subject Parameters: Name Type Description Default df DataFrame The pandas DataFrame object. required strip_underscores Union[str, bool] (optional) Removes the outer underscores from all column names. Default None keeps outer underscores. Values can be either 'left', 'right' or 'both' or the respective shorthand 'l', 'r' and True. None case_type str (optional) Whether to make columns lower or uppercase. Current case may be preserved with 'preserve', while snake case conversion (from CamelCase or camelCase only) can be turned on using \"snake\". Default 'lower' makes all characters lowercase. 'lower' remove_special bool (optional) Remove special characters from columns. Only letters, numbers and underscores are preserved. False strip_accents bool Whether or not to remove accents from columns names. True preserve_original_columns bool (optional) Preserve original names. This is later retrievable using df.original_columns . True enforce_string bool Whether or not to convert all column names to string type. Defaults to True, but can be turned off. Columns with >1 levels will not be converted by default. True truncate_limit int (optional) Truncates formatted column names to the specified length. Default None does not truncate. None Returns: Type Description DataFrame A pandas DataFrame. Source code in janitor/functions/clean_names.py @pf.register_dataframe_method def clean_names( df: pd.DataFrame, strip_underscores: Optional[Union[str, bool]] = None, case_type: str = \"lower\", remove_special: bool = False, strip_accents: bool = True, preserve_original_columns: bool = True, enforce_string: bool = True, truncate_limit: int = None, ) -> pd.DataFrame: \"\"\" Clean column names. Takes all column names, converts them to lowercase, then replaces all spaces with underscores. By default, column names are converted to string types. This can be switched off by passing in `enforce_string=False`. This method does not mutate the original DataFrame. Functional usage syntax: ```python df = clean_names(df) ``` Method chaining syntax: ```python import pandas as pd import janitor df = pd.DataFrame(...).clean_names() Examples of transformation: - Columns before: First Name, Last Name, Employee Status, Subject - Columns after: first_name, last_name, employee_status, subject :param df: The pandas DataFrame object. :param strip_underscores: (optional) Removes the outer underscores from all column names. Default None keeps outer underscores. Values can be either 'left', 'right' or 'both' or the respective shorthand 'l', 'r' and True. :param case_type: (optional) Whether to make columns lower or uppercase. Current case may be preserved with 'preserve', while snake case conversion (from CamelCase or camelCase only) can be turned on using \"snake\". Default 'lower' makes all characters lowercase. :param remove_special: (optional) Remove special characters from columns. Only letters, numbers and underscores are preserved. :param strip_accents: Whether or not to remove accents from columns names. :param preserve_original_columns: (optional) Preserve original names. This is later retrievable using `df.original_columns`. :param enforce_string: Whether or not to convert all column names to string type. Defaults to True, but can be turned off. Columns with >1 levels will not be converted by default. :param truncate_limit: (optional) Truncates formatted column names to the specified length. Default None does not truncate. :returns: A pandas DataFrame. \"\"\" original_column_names = list(df.columns) if enforce_string: df = df.rename(columns=str) df = df.rename(columns=lambda x: _change_case(x, case_type)) df = df.rename(columns=_normalize_1) if remove_special: df = df.rename(columns=_remove_special) if strip_accents: df = df.rename(columns=_strip_accents) df = df.rename(columns=lambda x: re.sub(\"_+\", \"_\", x)) # noqa: PD005 df = _strip_underscores(df, strip_underscores) df = df.rename(columns=lambda x: x[:truncate_limit]) # Store the original column names, if enabled by user if preserve_original_columns: df.__dict__[\"original_columns\"] = original_column_names return df coalesce coalesce(df, *column_names, *, target_column_name=None, default_value=None) Coalesce two or more columns of data in order of column names provided. This finds the first non-missing value at each position. This method does not mutate the original DataFrame. TODO: Turn the example in this docstring into a Jupyter notebook. Example: import pandas as pd import janitor as jn df = pd.DataFrame({\"A\": [1, 2, np.nan], \"B\": [np.nan, 10, np.nan], \"C\": [5, 10, 7]}) A B C 0 1.0 NaN 5 1 2.0 10.0 10 2 NaN NaN 7 df.coalesce('A', 'B', 'C', target_column_name = 'D') A B C D 0 1.0 NaN 5 1.0 1 2.0 10.0 10 2.0 2 NaN NaN 7 7.0 If no target column is provided, then the first column is updated, with the null values removed: df.coalesce('A', 'B', 'C') A B C 0 1.0 NaN 5 1 2.0 10.0 10 2 7.0 NaN 7 If nulls remain, you can fill it with the default_value : df = pd.DataFrame({'s1':[np.nan,np.nan,6,9,9], 's2':[np.nan,8,7,9,9]}) s1 s2 0 NaN NaN 1 NaN 8.0 2 6.0 7.0 3 9.0 9.0 4 9.0 9.0 df.coalesce('s1', 's2', target_column_name = 's3', default_value = 0) s1 s2 s3 0 NaN NaN 0.0 1 NaN 8.0 8.0 2 6.0 7.0 6.0 3 9.0 9.0 9.0 4 9.0 9.0 9.0 Functional usage syntax: df = coalesce(df, 'col1', 'col2', target_column_name ='col3') Method chaining syntax: import pandas as pd import janitor df = pd.DataFrame(...).coalesce('col1', 'col2') The first example will create a new column called col3 with values from col2 inserted where values from col1 are NaN . The second example will update the values of col1 , since it is the first column in column_names . This is more syntactic diabetes! For R users, this should look familiar to dplyr 's coalesce function; for Python users, the interface should be more intuitive than the pandas.Series.combine_first method. Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_names A list of column names. () target_column_name Optional[str] The new column name after combining. If None , then the first column in column_names is updated, with the Null values replaced. None default_value Union[int, float, str] A scalar to replace any remaining nulls after coalescing. None Returns: Type Description DataFrame A pandas DataFrame with coalesced columns. Exceptions: Type Description ValueError if length of column_names is less than 2. Source code in janitor/functions/coalesce.py @pf.register_dataframe_method @deprecated_alias(columns=\"column_names\", new_column_name=\"target_column_name\") def coalesce( df: pd.DataFrame, *column_names, target_column_name: Optional[str] = None, default_value: Optional[Union[int, float, str]] = None, ) -> pd.DataFrame: \"\"\" Coalesce two or more columns of data in order of column names provided. This finds the first non-missing value at each position. This method does not mutate the original DataFrame. TODO: Turn the example in this docstring into a Jupyter notebook. Example: ```python import pandas as pd import janitor as jn df = pd.DataFrame({\"A\": [1, 2, np.nan], \"B\": [np.nan, 10, np.nan], \"C\": [5, 10, 7]}) A B C 0 1.0 NaN 5 1 2.0 10.0 10 2 NaN NaN 7 df.coalesce('A', 'B', 'C', target_column_name = 'D') A B C D 0 1.0 NaN 5 1.0 1 2.0 10.0 10 2.0 2 NaN NaN 7 7.0 ``` If no target column is provided, then the first column is updated, with the null values removed: ```python df.coalesce('A', 'B', 'C') A B C 0 1.0 NaN 5 1 2.0 10.0 10 2 7.0 NaN 7 ``` If nulls remain, you can fill it with the `default_value`: ```python df = pd.DataFrame({'s1':[np.nan,np.nan,6,9,9], 's2':[np.nan,8,7,9,9]}) s1 s2 0 NaN NaN 1 NaN 8.0 2 6.0 7.0 3 9.0 9.0 4 9.0 9.0 df.coalesce('s1', 's2', target_column_name = 's3', default_value = 0) s1 s2 s3 0 NaN NaN 0.0 1 NaN 8.0 8.0 2 6.0 7.0 6.0 3 9.0 9.0 9.0 4 9.0 9.0 9.0 ``` Functional usage syntax: ```python df = coalesce(df, 'col1', 'col2', target_column_name ='col3') ``` Method chaining syntax: ```python import pandas as pd import janitor df = pd.DataFrame(...).coalesce('col1', 'col2') ``` The first example will create a new column called `col3` with values from `col2` inserted where values from `col1` are `NaN`. The second example will update the values of `col1`, since it is the first column in `column_names`. This is more syntactic diabetes! For R users, this should look familiar to `dplyr`'s `coalesce` function; for Python users, the interface should be more intuitive than the `pandas.Series.combine_first` method. :param df: A pandas DataFrame. :param column_names: A list of column names. :param target_column_name: The new column name after combining. If `None`, then the first column in `column_names` is updated, with the Null values replaced. :param default_value: A scalar to replace any remaining nulls after coalescing. :returns: A pandas DataFrame with coalesced columns. :raises ValueError: if length of `column_names` is less than 2. \"\"\" if not column_names: return df if len(column_names) < 2: raise ValueError( \"\"\" The number of columns to coalesce should be a minimum of 2. \"\"\" ) column_names = [*column_names] column_names = _select_column_names(column_names, df) if target_column_name: check(\"target_column_name\", target_column_name, [str]) if default_value: check(\"default_value\", default_value, [int, float, str]) if target_column_name is None: target_column_name = column_names[0] # bfill/ffill combo is faster than combine_first outcome = ( df.filter(column_names) .bfill(axis=\"columns\") .ffill(axis=\"columns\") .iloc[:, 0] ) if outcome.hasnans and (default_value is not None): outcome = outcome.fillna(default_value) return df.assign(**{target_column_name: outcome}) collapse_levels collapse_levels(df, sep='_') Flatten multi-level column dataframe to a single level. This method mutates the original DataFrame. Given a DataFrame containing multi-level columns, flatten to single- level by string-joining the column labels in each level. After a groupby / aggregate operation where .agg() is passed a list of multiple aggregation functions, a multi-level DataFrame is returned with the name of the function applied in the second level. It is sometimes convenient for later indexing to flatten out this multi-level configuration back into a single level. This function does this through a simple string-joining of all the names across different levels in a single column. Method chaining syntax given two value columns [max_speed, type] : data = {\"class\": [\"bird\", \"bird\", \"bird\", \"mammal\", \"mammal\"], \"max_speed\": [389, 389, 24, 80, 21], \"type\": [\"falcon\", \"falcon\", \"parrot\", \"Lion\", \"Monkey\"]} df = ( pd.DataFrame(data) .groupby('class') .agg(['mean', 'median']) .collapse_levels(sep='_') ) Before applying .collapse_levels , the .agg operation returns a multi-level column DataFrame whose columns are (level 1, level 2) : [('class', ''), ('max_speed', 'mean'), ('max_speed', 'median'), ('type', 'mean'), ('type', 'median')] .collapse_levels then flattens the column names to: ['class', 'max_speed_mean', 'max_speed_median', 'type_mean', 'type_median'] Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required sep str String separator used to join the column level names '_' Returns: Type Description DataFrame A flattened pandas DataFrame. Source code in janitor/functions/collapse_levels.py @pf.register_dataframe_method def collapse_levels(df: pd.DataFrame, sep: str = \"_\") -> pd.DataFrame: \"\"\" Flatten multi-level column dataframe to a single level. This method mutates the original DataFrame. Given a `DataFrame` containing multi-level columns, flatten to single- level by string-joining the column labels in each level. After a `groupby` / `aggregate` operation where `.agg()` is passed a list of multiple aggregation functions, a multi-level DataFrame is returned with the name of the function applied in the second level. It is sometimes convenient for later indexing to flatten out this multi-level configuration back into a single level. This function does this through a simple string-joining of all the names across different levels in a single column. Method chaining syntax given two value columns `[max_speed, type]`: ```python data = {\"class\": [\"bird\", \"bird\", \"bird\", \"mammal\", \"mammal\"], \"max_speed\": [389, 389, 24, 80, 21], \"type\": [\"falcon\", \"falcon\", \"parrot\", \"Lion\", \"Monkey\"]} df = ( pd.DataFrame(data) .groupby('class') .agg(['mean', 'median']) .collapse_levels(sep='_') ) ``` Before applying `.collapse_levels`, the `.agg` operation returns a multi-level column DataFrame whose columns are `(level 1, level 2)`: [('class', ''), ('max_speed', 'mean'), ('max_speed', 'median'), ('type', 'mean'), ('type', 'median')] `.collapse_levels` then flattens the column names to: ['class', 'max_speed_mean', 'max_speed_median', 'type_mean', 'type_median'] :param df: A pandas DataFrame. :param sep: String separator used to join the column level names :returns: A flattened pandas DataFrame. \"\"\" check(\"sep\", sep, [str]) # if already single-level, just return the DataFrame if not isinstance(df.columns, pd.MultiIndex): return df df.columns = [ sep.join(str(el) for el in tup if str(el) != \"\") for tup in df # noqa: PD011 ] return df complete complete(df, *columns, *, sort=False, by=None) It is modeled after tidyr's complete function, and is a wrapper around expand_grid and pd.merge . Combinations of column names or a list/tuple of column names, or even a dictionary of column names and new values are possible. It can also handle duplicated data. MultiIndex columns are not supported. Functional usage syntax: import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.complete( df = df, column_label, (column1, column2, ...), {column1: new_values, ...}, by = label/list_of_labels ) Method chaining syntax: df = ( pd.DataFrame(...) .complete( column_label, (column1, column2, ...), {column1: new_values, ...}, by = label/list_of_labels ) Parameters: Name Type Description Default df DataFrame A pandas dataframe. required *columns This refers to the columns to be completed. It could be column labels (string type), a list/tuple of column labels, or a dictionary that pairs column labels with new values. () sort bool Sort DataFrame based on *columns. Default is False . False by Union[list, str] label or list of labels to group by. The explicit missing rows are returned per group. None Returns: Type Description DataFrame A pandas DataFrame with explicit missing rows, if any. Source code in janitor/functions/complete.py @pf.register_dataframe_method def complete( df: pd.DataFrame, *columns, sort: bool = False, by: Optional[Union[list, str]] = None, ) -> pd.DataFrame: \"\"\" It is modeled after tidyr's `complete` function, and is a wrapper around `expand_grid` and `pd.merge`. Combinations of column names or a list/tuple of column names, or even a dictionary of column names and new values are possible. It can also handle duplicated data. MultiIndex columns are not supported. Functional usage syntax: ```python import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.complete( df = df, column_label, (column1, column2, ...), {column1: new_values, ...}, by = label/list_of_labels ) ``` Method chaining syntax: ```python df = ( pd.DataFrame(...) .complete( column_label, (column1, column2, ...), {column1: new_values, ...}, by = label/list_of_labels ) ``` :param df: A pandas dataframe. :param *columns: This refers to the columns to be completed. It could be column labels (string type), a list/tuple of column labels, or a dictionary that pairs column labels with new values. :param sort: Sort DataFrame based on *columns. Default is `False`. :param by: label or list of labels to group by. The explicit missing rows are returned per group. :returns: A pandas DataFrame with explicit missing rows, if any. \"\"\" if not columns: return df df = df.copy() return _computations_complete(df, columns, sort, by) concatenate_columns concatenate_columns(df, column_names, new_column_name, sep='-', ignore_empty=True) Concatenates the set of columns into a single column. Used to quickly generate an index based on a group of columns. This method mutates the original DataFrame. Functional usage syntax: df = concatenate_columns(df, column_names=['col1', 'col2'], new_column_name='id', sep='-') Method chaining syntax: df = (pd.DataFrame(...). concatenate_columns(column_names=['col1', 'col2'], new_column_name='id', sep='-')) Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_names List[Hashable] A list of columns to concatenate together. required new_column_name The name of the new column. required sep str The separator between each column's data. '-' ignore_empty bool Ignore null values if exists. True Returns: Type Description DataFrame A pandas DataFrame with concatenated columns. Exceptions: Type Description JanitorError if at least two columns are not provided within `column_names``. Source code in janitor/functions/concatenate_columns.py @pf.register_dataframe_method @deprecated_alias(columns=\"column_names\") def concatenate_columns( df: pd.DataFrame, column_names: List[Hashable], new_column_name, sep: str = \"-\", ignore_empty: bool = True, ) -> pd.DataFrame: \"\"\"Concatenates the set of columns into a single column. Used to quickly generate an index based on a group of columns. This method mutates the original DataFrame. Functional usage syntax: df = concatenate_columns(df, column_names=['col1', 'col2'], new_column_name='id', sep='-') Method chaining syntax: df = (pd.DataFrame(...). concatenate_columns(column_names=['col1', 'col2'], new_column_name='id', sep='-')) :param df: A pandas DataFrame. :param column_names: A list of columns to concatenate together. :param new_column_name: The name of the new column. :param sep: The separator between each column's data. :param ignore_empty: Ignore null values if exists. :returns: A pandas DataFrame with concatenated columns. :raises JanitorError: if at least two columns are not provided within `column_names``. \"\"\" if len(column_names) < 2: raise JanitorError(\"At least two columns must be specified\") df[new_column_name] = ( df[column_names].fillna(\"\").astype(str).agg(sep.join, axis=1) ) if ignore_empty: def remove_empty_string(x): return sep.join(x for x in x.split(sep) if x) df[new_column_name] = df[new_column_name].transform( remove_empty_string ) return df conditional_join conditional_join(df, right, *conditions, *, how='inner', sort_by_appearance=False) This is a convenience function that operates similarly to pd.merge , but allows joins on inequality operators, or a combination of equi and non-equi joins. If the join is solely on equality, pd.merge function is more efficient and should be used instead. If you are interested in nearest joins, or rolling joins, pd.merge_asof covers that. There is also the IntervalIndex, which is usually more efficient for range joins, especially if the intervals do not overlap. This function returns rows, if any, where values from df meet the condition(s) for values from right . The conditions are passed in as a variable argument of tuples, where the tuple is of the form (left_on, right_on, op) ; left_on is the column label from df , right_on is the column label from right , while op is the operator. The operator can be any of == , != , <= , < , >= , > . A binary search is used to get the relevant rows for non-equi joins; this avoids a cartesian join, and makes the process less memory intensive. For equi-joins, Pandas internal merge function (a hash join) is used. The join is done only on the columns. MultiIndex columns are not supported. For non-equi joins, only numeric and date columns are supported. Only inner , left , and right joins are supported. If the columns from df and right have nothing in common, a single index column is returned; else, a MultiIndex column is returned. Functional usage syntax: import pandas as pd import janitor as jn df = pd.DataFrame(...) right = pd.DataFrame(...) df = jn.conditional_join( df, right, (col_from_df, col_from_right, join_operator), (col_from_df, col_from_right, join_operator), ..., how = 'inner' # or left/right sort_by_appearance = True # or False ) Method chaining syntax: .. code-block:: python df.conditional_join( right, (col_from_df, col_from_right, join_operator), (col_from_df, col_from_right, join_operator), ..., how = 'inner' # or left/right sort_by_appearance = True # or False ) Parameters: Name Type Description Default df DataFrame A Pandas DataFrame. required right Union[pandas.core.frame.DataFrame, pandas.core.series.Series] Named Series or DataFrame to join to. required conditions Variable argument of tuple(s) of the form (left_on, right_on, op) , where left_on is the column label from df , right_on is the column label from right , while op is the operator. The operator can be any of == , != , <= , < , >= , > . () how str Indicates the type of join to be performed. It can be one of inner , left , right . Full join is not supported. Defaults to inner . 'inner' sort_by_appearance bool Default is False . If True, values from right that meet the join condition will be returned in the final dataframe in the same order that they were before the join. False Returns: Type Description DataFrame A pandas DataFrame of the two merged Pandas objects. Source code in janitor/functions/conditional_join.py @pf.register_dataframe_method def conditional_join( df: pd.DataFrame, right: Union[pd.DataFrame, pd.Series], *conditions, how: str = \"inner\", sort_by_appearance: bool = False, ) -> pd.DataFrame: \"\"\" This is a convenience function that operates similarly to ``pd.merge``, but allows joins on inequality operators, or a combination of equi and non-equi joins. If the join is solely on equality, `pd.merge` function is more efficient and should be used instead. If you are interested in nearest joins, or rolling joins, `pd.merge_asof` covers that. There is also the IntervalIndex, which is usually more efficient for range joins, especially if the intervals do not overlap. This function returns rows, if any, where values from `df` meet the condition(s) for values from `right`. The conditions are passed in as a variable argument of tuples, where the tuple is of the form `(left_on, right_on, op)`; `left_on` is the column label from `df`, `right_on` is the column label from `right`, while `op` is the operator. The operator can be any of `==`, `!=`, `<=`, `<`, `>=`, `>`. A binary search is used to get the relevant rows for non-equi joins; this avoids a cartesian join, and makes the process less memory intensive. For equi-joins, Pandas internal merge function (a hash join) is used. The join is done only on the columns. MultiIndex columns are not supported. For non-equi joins, only numeric and date columns are supported. Only `inner`, `left`, and `right` joins are supported. If the columns from `df` and `right` have nothing in common, a single index column is returned; else, a MultiIndex column is returned. Functional usage syntax: ```python import pandas as pd import janitor as jn df = pd.DataFrame(...) right = pd.DataFrame(...) df = jn.conditional_join( df, right, (col_from_df, col_from_right, join_operator), (col_from_df, col_from_right, join_operator), ..., how = 'inner' # or left/right sort_by_appearance = True # or False ) ``` Method chaining syntax: .. code-block:: python df.conditional_join( right, (col_from_df, col_from_right, join_operator), (col_from_df, col_from_right, join_operator), ..., how = 'inner' # or left/right sort_by_appearance = True # or False ) :param df: A Pandas DataFrame. :param right: Named Series or DataFrame to join to. :param conditions: Variable argument of tuple(s) of the form `(left_on, right_on, op)`, where `left_on` is the column label from `df`, `right_on` is the column label from `right`, while `op` is the operator. The operator can be any of `==`, `!=`, `<=`, `<`, `>=`, `>`. :param how: Indicates the type of join to be performed. It can be one of `inner`, `left`, `right`. Full join is not supported. Defaults to `inner`. :param sort_by_appearance: Default is `False`. If True, values from `right` that meet the join condition will be returned in the final dataframe in the same order that they were before the join. :returns: A pandas DataFrame of the two merged Pandas objects. \"\"\" return _conditional_join_compute( df, right, conditions, how, sort_by_appearance ) convert_date convert_excel_date(df, column_name) Convert Excel's serial date format into Python datetime format. This method mutates the original DataFrame. Implementation is also from Stack Overflow Functional usage syntax: df = convert_excel_date(df, column_name='date') Method chaining syntax: import pandas as pd import janitor df = pd.DataFrame(...).convert_excel_date('date') Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable A column name. required Returns: Type Description DataFrame A pandas DataFrame with corrected dates. Exceptions: Type Description ValueError if there are non numeric values in the column. Source code in janitor/functions/convert_date.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def convert_excel_date( df: pd.DataFrame, column_name: Hashable ) -> pd.DataFrame: \"\"\" Convert Excel's serial date format into Python datetime format. This method mutates the original DataFrame. Implementation is also from [Stack Overflow](https://stackoverflow.com/questions/38454403/convert-excel-style-date-with-pandas) Functional usage syntax: ```python df = convert_excel_date(df, column_name='date') ``` Method chaining syntax: ```python import pandas as pd import janitor df = pd.DataFrame(...).convert_excel_date('date') ``` :param df: A pandas DataFrame. :param column_name: A column name. :returns: A pandas DataFrame with corrected dates. :raises ValueError: if there are non numeric values in the column. \"\"\" # noqa: E501 if not is_numeric_dtype(df[column_name]): raise ValueError( \"There are non-numeric values in the column. \\ All values must be numeric\" ) df[column_name] = pd.TimedeltaIndex( df[column_name], unit=\"d\" ) + dt.datetime( 1899, 12, 30 ) # noqa: W503 return df convert_matlab_date(df, column_name) Convert Matlab's serial date number into Python datetime format. Implementation is also from Stack Overflow This method mutates the original DataFrame. Functional usage syntax: df = convert_matlab_date(df, column_name='date') Method chaining syntax: import pandas as pd import janitor df = pd.DataFrame(...).convert_matlab_date('date') Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable A column name. required Returns: Type Description DataFrame A pandas DataFrame with corrected dates. Source code in janitor/functions/convert_date.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def convert_matlab_date( df: pd.DataFrame, column_name: Hashable ) -> pd.DataFrame: \"\"\" Convert Matlab's serial date number into Python datetime format. Implementation is also from [Stack Overflow](https://stackoverflow.com/questions/13965740/converting-matlabs-datenum-format-to-python) This method mutates the original DataFrame. Functional usage syntax: ```python df = convert_matlab_date(df, column_name='date') ``` Method chaining syntax: ```python import pandas as pd import janitor df = pd.DataFrame(...).convert_matlab_date('date') ``` :param df: A pandas DataFrame. :param column_name: A column name. :returns: A pandas DataFrame with corrected dates. \"\"\" # noqa: E501 days = pd.Series([dt.timedelta(v % 1) for v in df[column_name]]) df[column_name] = ( df[column_name].astype(int).apply(dt.datetime.fromordinal) + days - dt.timedelta(days=366) ) return df convert_unix_date(df, column_name) Convert unix epoch time into Python datetime format. Note that this ignores local tz and convert all timestamps to naive datetime based on UTC! This method mutates the original DataFrame. Functional usage syntax: df = convert_unix_date(df, column_name='date') Method chaining syntax: import pandas as pd import janitor df = pd.DataFrame(...).convert_unix_date('date') Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable A column name. required Returns: Type Description DataFrame A pandas DataFrame with corrected dates. Source code in janitor/functions/convert_date.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def convert_unix_date(df: pd.DataFrame, column_name: Hashable) -> pd.DataFrame: \"\"\" Convert unix epoch time into Python datetime format. Note that this ignores local tz and convert all timestamps to naive datetime based on UTC! This method mutates the original DataFrame. Functional usage syntax: ```python df = convert_unix_date(df, column_name='date') ``` Method chaining syntax: ```python import pandas as pd import janitor df = pd.DataFrame(...).convert_unix_date('date') ``` :param df: A pandas DataFrame. :param column_name: A column name. :returns: A pandas DataFrame with corrected dates. \"\"\" try: df[column_name] = pd.to_datetime(df[column_name], unit=\"s\") except OutOfBoundsDatetime: # Indicates time is in milliseconds. df[column_name] = pd.to_datetime(df[column_name], unit=\"ms\") return df count_cumulative_unique count_cumulative_unique(df, column_name, dest_column_name, case_sensitive=True) Generates a running total of cumulative unique values in a given column. Functional usage syntax: import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.functions.count_cumulative_unique( df=df, column_name='animals', dest_column_name='animals_unique_count', case_sensitive=True ) Method chaining usage example: import pandas as pd import janitor df = pd.DataFrame(...) df = df.count_cumulative_unique( column_name='animals', dest_column_name='animals_unique_count', case_sensitive=True ) A new column will be created containing a running count of unique values in the specified column. If case_sensitive is True , then the case of any letters will matter (i.e., a != A ); otherwise, the case of any letters will not matter. This method mutates the original DataFrame. Parameters: Name Type Description Default df DataFrame A pandas dataframe. required column_name Hashable Name of the column containing values from which a running count of unique values will be created. required dest_column_name str The name of the new column containing the cumulative count of unique values that will be created. required case_sensitive bool Whether or not uppercase and lowercase letters will be considered equal (e.g., 'A' != 'a' if True ). True Returns: Type Description DataFrame A pandas DataFrame with a new column containing a cumulative count of unique values from another column. Source code in janitor/functions/count_cumulative_unique.py @pf.register_dataframe_method def count_cumulative_unique( df: pd.DataFrame, column_name: Hashable, dest_column_name: str, case_sensitive: bool = True, ) -> pd.DataFrame: \"\"\"Generates a running total of cumulative unique values in a given column. Functional usage syntax: ```python import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.functions.count_cumulative_unique( df=df, column_name='animals', dest_column_name='animals_unique_count', case_sensitive=True ) ``` Method chaining usage example: ```python import pandas as pd import janitor df = pd.DataFrame(...) df = df.count_cumulative_unique( column_name='animals', dest_column_name='animals_unique_count', case_sensitive=True ) ``` A new column will be created containing a running count of unique values in the specified column. If `case_sensitive` is `True`, then the case of any letters will matter (i.e., `a != A`); otherwise, the case of any letters will not matter. This method mutates the original DataFrame. :param df: A pandas dataframe. :param column_name: Name of the column containing values from which a running count of unique values will be created. :param dest_column_name: The name of the new column containing the cumulative count of unique values that will be created. :param case_sensitive: Whether or not uppercase and lowercase letters will be considered equal (e.g., 'A' != 'a' if `True`). :returns: A pandas DataFrame with a new column containing a cumulative count of unique values from another column. \"\"\" if not case_sensitive: # Make it so that the the same uppercase and lowercase # letter are treated as one unique value df[column_name] = df[column_name].astype(str).map(str.lower) df[dest_column_name] = ( ( df[[column_name]] .drop_duplicates() .assign(dummyabcxyz=1) .dummyabcxyz.cumsum() ) .reindex(df.index) .ffill() .astype(int) ) return df currency_column_to_numeric currency_column_to_numeric(df, column_name, cleaning_style=None, cast_non_numeric=None, fill_all_non_numeric=None, remove_non_numeric=False) Convert currency column to numeric. This method does not mutate the original DataFrame. This method allows one to take a column containing currency values, inadvertently imported as a string, and cast it as a float. This is usually the case when reading CSV files that were modified in Excel. Empty strings (i.e. '' ) are retained as NaN values. Parameters: Name Type Description Default df DataFrame The DataFrame required column_name The column to modify required cleaning_style Optional[str] What style of cleaning to perform. If None, standard cleaning is applied. Options are: * 'accounting': Replaces numbers in parentheses with negatives, removes commas. None cast_non_numeric Optional[dict] A dict of how to coerce certain strings. For example, if there are values of 'REORDER' in the DataFrame, {'REORDER': 0} will cast all instances of 'REORDER' to 0. None fill_all_non_numeric Union[float, int] Similar to cast_non_numeric , but fills all strings to the same value. For example, fill_all_non_numeric=1, will make everything that doesn't coerce to a currency 1. None remove_non_numeric bool Will remove rows of a DataFrame that contain non-numeric values in the column_name column. Defaults to False . False Returns: Type Description DataFrame A pandas DataFrame. Source code in janitor/functions/currency_column_to_numeric.py @pf.register_dataframe_method @deprecated_alias(col_name=\"column_name\", type=\"cleaning_style\") def currency_column_to_numeric( df: pd.DataFrame, column_name, cleaning_style: Optional[str] = None, cast_non_numeric: Optional[dict] = None, fill_all_non_numeric: Optional[Union[float, int]] = None, remove_non_numeric: bool = False, ) -> pd.DataFrame: \"\"\"Convert currency column to numeric. This method does not mutate the original DataFrame. This method allows one to take a column containing currency values, inadvertently imported as a string, and cast it as a float. This is usually the case when reading CSV files that were modified in Excel. Empty strings (i.e. `''`) are retained as `NaN` values. :param df: The DataFrame :param column_name: The column to modify :param cleaning_style: What style of cleaning to perform. If None, standard cleaning is applied. Options are: * 'accounting': Replaces numbers in parentheses with negatives, removes commas. :param cast_non_numeric: A dict of how to coerce certain strings. For example, if there are values of 'REORDER' in the DataFrame, {'REORDER': 0} will cast all instances of 'REORDER' to 0. :param fill_all_non_numeric: Similar to `cast_non_numeric`, but fills all strings to the same value. For example, fill_all_non_numeric=1, will make everything that doesn't coerce to a currency 1. :param remove_non_numeric: Will remove rows of a DataFrame that contain non-numeric values in the `column_name` column. Defaults to `False`. :returns: A pandas DataFrame. \"\"\" check(\"column_name\", column_name, [str]) column_series = df[column_name] if cleaning_style == \"accounting\": df.loc[:, column_name] = df[column_name].apply( _clean_accounting_column ) return df if cast_non_numeric: check(\"cast_non_numeric\", cast_non_numeric, [dict]) _make_cc_patrial = partial( _currency_column_to_numeric, cast_non_numeric=cast_non_numeric ) column_series = column_series.apply(_make_cc_patrial) if remove_non_numeric: df = df.loc[column_series != \"\", :] # _replace_empty_string_with_none is applied here after the check on # remove_non_numeric since \"\" is our indicator that a string was coerced # in the original column column_series = _replace_empty_string_with_none(column_series) if fill_all_non_numeric is not None: check(\"fill_all_non_numeric\", fill_all_non_numeric, [int, float]) column_series = column_series.fillna(fill_all_non_numeric) column_series = _replace_original_empty_string_with_none(column_series) df = df.assign(**{column_name: pd.to_numeric(column_series)}) return df deconcatenate_column deconcatenate_column(df, column_name, sep=None, new_column_names=None, autoname=None, preserve_position=False) De-concatenates a single column into multiple columns. The column to de-concatenate can be either a collection (list, tuple, ...) which can be separated out with pd.Series.tolist()``, or a string to slice based on sep``. To determine this behaviour automatically, the first element in the column specified is inspected. If it is a string, then sep must be specified. Else, the function assumes that it is an iterable type (e.g. list or `tuple``), and will attempt to deconcatenate by splitting the list. Given a column with string values, this is the inverse of the concatenate_columns function. Used to quickly split columns out of a single column. The keyword argument preserve_position`` takes True or False boolean that controls whether the new_column_names will take the original position of the to-be-deconcatenated `column_name : When preserve_position=False (default), df.columns change from [..., column_name, ...] to [..., column_name, ..., new_column_names] . In other words, the deconcatenated new columns are appended to the right of the original dataframe and the original column_name is NOT dropped. When preserve_position=True , df.column change from [..., column_name, ...] to [..., new_column_names, ...] . In other words, the deconcatenated new column will REPLACE the original column_name at its original position, and column_name itself is dropped. The keyword argument autoname accepts a base string and then automatically creates numbered column names based off the base string. For example, if col is passed in as the argument to autoname``, and 4 columns are created, then the resulting columns will be named col1, col2, col3, col4``. Numbering is always 1-indexed, not 0-indexed, in order to make the column names human-friendly. This method does not mutate the original DataFrame. Functional usage syntax: df = deconcatenate_column( df, column_name='id', new_column_names=['col1', 'col2'], sep='-', preserve_position=True ) Method chaining syntax: df = (pd.DataFrame(...). deconcatenate_column( column_name='id', new_column_names=['col1', 'col2'], sep='-', preserve_position=True )) Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable The column to split. required sep Optional[str] The separator delimiting the column's data. None new_column_names Union[List[str], Tuple[str]] A list of new column names post-splitting. None autoname str A base name for automatically naming the new columns. Takes precedence over new_column_names if both are provided. None preserve_position bool Boolean for whether or not to preserve original position of the column upon de-concatenation, default to False False Returns: Type Description DataFrame A pandas DataFrame with a deconcatenated column. Exceptions: Type Description ValueError if column_name is not present in the DataFrame. ValueError if sep is not provided and the column values are of type `str``. ValueError if either new_column_names or `autoname`` is not supplied. JanitorError if incorrect number of names is provided within `new_column_names``. Source code in janitor/functions/deconcatenate_column.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def deconcatenate_column( df: pd.DataFrame, column_name: Hashable, sep: Optional[str] = None, new_column_names: Optional[Union[List[str], Tuple[str]]] = None, autoname: str = None, preserve_position: bool = False, ) -> pd.DataFrame: \"\"\"De-concatenates a single column into multiple columns. The column to de-concatenate can be either a collection (list, tuple, ...) which can be separated out with `pd.Series.tolist()``, or a string to slice based on `sep``. To determine this behaviour automatically, the first element in the column specified is inspected. If it is a string, then `sep` must be specified. Else, the function assumes that it is an iterable type (e.g. `list` or `tuple``), and will attempt to deconcatenate by splitting the list. Given a column with string values, this is the inverse of the `concatenate_columns` function. Used to quickly split columns out of a single column. The keyword argument `preserve_position`` takes `True` or `False` boolean that controls whether the `new_column_names`` will take the original position of the to-be-deconcatenated `column_name``: - When `preserve_position=False` (default), `df.columns` change from `[..., column_name, ...]` to `[..., column_name, ..., new_column_names]`. In other words, the deconcatenated new columns are appended to the right of the original dataframe and the original `column_name` is NOT dropped. - When `preserve_position=True`, `df.column` change from `[..., column_name, ...]` to `[..., new_column_names, ...]`. In other words, the deconcatenated new column will REPLACE the original `column_name` at its original position, and `column_name` itself is dropped. The keyword argument `autoname` accepts a base string and then automatically creates numbered column names based off the base string. For example, if `col` is passed in as the argument to `autoname``, and 4 columns are created, then the resulting columns will be named `col1, col2, col3, col4``. Numbering is always 1-indexed, not 0-indexed, in order to make the column names human-friendly. This method does not mutate the original DataFrame. Functional usage syntax: df = deconcatenate_column( df, column_name='id', new_column_names=['col1', 'col2'], sep='-', preserve_position=True ) Method chaining syntax: df = (pd.DataFrame(...). deconcatenate_column( column_name='id', new_column_names=['col1', 'col2'], sep='-', preserve_position=True )) :param df: A pandas DataFrame. :param column_name: The column to split. :param sep: The separator delimiting the column's data. :param new_column_names: A list of new column names post-splitting. :param autoname: A base name for automatically naming the new columns. Takes precedence over `new_column_names` if both are provided. :param preserve_position: Boolean for whether or not to preserve original position of the column upon de-concatenation, default to False :returns: A pandas DataFrame with a deconcatenated column. :raises ValueError: if `column_name` is not present in the DataFrame. :raises ValueError: if `sep` is not provided and the column values are of type `str``. :raises ValueError: if either `new_column_names` or `autoname`` is not supplied. :raises JanitorError: if incorrect number of names is provided within `new_column_names``. \"\"\" if column_name not in df.columns: raise ValueError(f\"column name {column_name} not present in DataFrame\") if isinstance(df[column_name].iloc[0], str): if sep is None: raise ValueError( \"`sep` must be specified if the column values \" \"are of type `str`.\" ) df_deconcat = df[column_name].str.split(sep, expand=True) else: df_deconcat = pd.DataFrame( df[column_name].to_list(), columns=new_column_names, index=df.index ) if new_column_names is None and autoname is None: raise ValueError( \"One of `new_column_names` or `autoname` must be supplied.\" ) if autoname: new_column_names = [ f\"{autoname}{i}\" for i in range(1, df_deconcat.shape[1] + 1) ] if not len(new_column_names) == df_deconcat.shape[1]: raise JanitorError( f\"you need to provide {len(df_deconcat.shape[1])} names \" \"to `new_column_names`\" ) df_deconcat.columns = new_column_names df_new = pd.concat([df, df_deconcat], axis=1) if preserve_position: df_original = df.copy() cols = list(df_original.columns) index_original = cols.index(column_name) for i, col_new in enumerate(new_column_names): cols.insert(index_original + i, col_new) df_new = df_new.select_columns(cols).drop(columns=column_name) return df_new drop_constant_columns drop_constant_columns(df) Finds and drops the constant columns from a Pandas DataFrame. This method does not mutate the original DataFrame. Functional usage syntax: import pandas as pd import janitor as jn data_dict = { \"a\": [1, 1, 1] * 3, \"Bell__Chart\": [1, 2, 3] * 3, \"decorated-elephant\": [1, 1, 1] * 3, \"animals\": [\"rabbit\", \"leopard\", \"lion\"] * 3, \"cities\": [\"Cambridge\", \"Shanghai\", \"Basel\"] * 3 } df = pd.DataFrame(data_dict) df = jn.functions.drop_constant_columns(df) Method chaining usage example: import pandas as pd import janitor df = pd.DataFrame(...) df = df.drop_constant_columns() Parameters: Name Type Description Default df DataFrame Input Pandas DataFrame required Returns: Type Description DataFrame The Pandas DataFrame with the constant columns dropped. Source code in janitor/functions/drop_constant_columns.py @pf.register_dataframe_method def drop_constant_columns( df: pd.DataFrame, ) -> pd.DataFrame: \"\"\" Finds and drops the constant columns from a Pandas DataFrame. This method does not mutate the original DataFrame. Functional usage syntax: ```python import pandas as pd import janitor as jn data_dict = { \"a\": [1, 1, 1] * 3, \"Bell__Chart\": [1, 2, 3] * 3, \"decorated-elephant\": [1, 1, 1] * 3, \"animals\": [\"rabbit\", \"leopard\", \"lion\"] * 3, \"cities\": [\"Cambridge\", \"Shanghai\", \"Basel\"] * 3 } df = pd.DataFrame(data_dict) df = jn.functions.drop_constant_columns(df) ``` Method chaining usage example: ```python import pandas as pd import janitor df = pd.DataFrame(...) df = df.drop_constant_columns() ``` :param df: Input Pandas DataFrame :returns: The Pandas DataFrame with the constant columns dropped. \"\"\" # Find the constant columns constant_columns = [] for col in df.columns: if len(df[col].unique()) == 1: constant_columns.append(col) # Drop constant columns from df and return it return df.drop(labels=constant_columns, axis=1) drop_duplicate_columns drop_duplicate_columns(df, column_name, nth_index=0) Remove a duplicated column specified by column_name, its index. This method does not mutate the original DataFrame. Column order 0 is to remove the first column, order 1 is to remove the second column, and etc The corresponding tidyverse R's library is: select(-<column_name>_<nth_index + 1>) Method chaining syntax: df = pd.DataFrame({ \"a\": range(10), \"b\": range(10), \"A\": range(10, 20), \"a*\": range(20, 30), }).clean_names(remove_special=True) # remove a duplicated second 'a' column df.drop_duplicate_columns(column_name=\"a\", nth_index=1) Parameters: Name Type Description Default df DataFrame A pandas DataFrame required column_name Hashable Column to be removed required nth_index int Among the duplicated columns, select the nth column to drop. 0 Returns: Type Description DataFrame A pandas DataFrame Source code in janitor/functions/drop_duplicate_columns.py @pf.register_dataframe_method def drop_duplicate_columns( df: pd.DataFrame, column_name: Hashable, nth_index: int = 0 ) -> pd.DataFrame: \"\"\"Remove a duplicated column specified by column_name, its index. This method does not mutate the original DataFrame. Column order 0 is to remove the first column, order 1 is to remove the second column, and etc The corresponding tidyverse R's library is: `select(-<column_name>_<nth_index + 1>)` Method chaining syntax: df = pd.DataFrame({ \"a\": range(10), \"b\": range(10), \"A\": range(10, 20), \"a*\": range(20, 30), }).clean_names(remove_special=True) # remove a duplicated second 'a' column df.drop_duplicate_columns(column_name=\"a\", nth_index=1) :param df: A pandas DataFrame :param column_name: Column to be removed :param nth_index: Among the duplicated columns, select the nth column to drop. :return: A pandas DataFrame \"\"\" cols = df.columns.to_list() col_indexes = [ col_idx for col_idx, col_name in enumerate(cols) if col_name == column_name ] # given that a column could be duplicated, # user could opt based on its order removed_col_idx = col_indexes[nth_index] # get the column indexes without column that is being removed filtered_cols = [ c_i for c_i, c_v in enumerate(cols) if c_i != removed_col_idx ] return df.iloc[:, filtered_cols] dropnotnull dropnotnull(df, column_name) Drop rows that do not have null values in the given column. This method does not mutate the original DataFrame. Example usage: df = pd.DataFrame(...).dropnotnull('column3') Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable The column name to drop rows from. required Returns: Type Description DataFrame A pandas DataFrame with dropped rows. Source code in janitor/functions/dropnotnull.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def dropnotnull(df: pd.DataFrame, column_name: Hashable) -> pd.DataFrame: \"\"\" Drop rows that do not have null values in the given column. This method does not mutate the original DataFrame. Example usage: ```python df = pd.DataFrame(...).dropnotnull('column3') ``` :param df: A pandas DataFrame. :param column_name: The column name to drop rows from. :returns: A pandas DataFrame with dropped rows. \"\"\" return df[pd.isna(df[column_name])] encode_categorical encode_categorical(df, column_names=None, **kwargs) Encode the specified columns with Pandas' category dtype . It is syntactic sugar around pd.Categorical . This method does not mutate the original DataFrame. Note: In versions < 0.20.11, this method mutates the original DataFrame. TODO: The big chunk of examples below should be moved into a Jupyter notebook. This will keep the docstring consistent and to-the-point. Examples: col1 col2 col3 0 2.0 a 2020-01-01 1 1.0 b 2020-01-02 2 3.0 c 2020-01-03 3 1.0 d 2020-01-04 4 NaN a 2020-01-05 df.dtypes col1 float64 col2 object col3 datetime64[ns] dtype: object Specific columns can be converted to category type: df = ( pd.DataFrame(...) .encode_categorical( column_names=['col1', 'col2', 'col3'] ) ) df.dtypes col1 category col2 category col3 category dtype: object Note that for the code above, the categories were inferred from the columns, and is unordered: df['col3'] 0 2020-01-01 1 2020-01-02 2 2020-01-03 3 2020-01-04 4 2020-01-05 Name: col3, dtype: category Categories (5, datetime64[ns]): [2020-01-01, 2020-01-02, 2020-01-03, 2020-01-04, 2020-01-05] Explicit categories can be provided, and ordered via the `kwargs`` parameter: df = (pd.DataFrame(...) .encode_categorical( col1 = ([3, 2, 1, 4], \"appearance\"), col2 = (['a','d','c','b'], \"sort\") ) ) df['col1'] 0 2 1 1 2 3 3 1 4 NaN Name: col1, dtype: category Categories (4, int64): [3 < 2 < 1 < 4] df['col2'] 0 a 1 b 2 c 3 d 4 a Name: col2, dtype: category Categories (4, object): [a < b < c < d] When the order parameter is \"appearance\", the categories argument is used as-is; if the order is \"sort\", the categories argument is sorted in ascending order; if order is `None``, then the categories argument is applied unordered. A User Warning will be generated if some or all of the unique values in the column are not present in the provided categories argument. df = (pd.DataFrame(...) .encode_categorical( col1 = ( categories = [4, 5, 6], order = \"appearance\" ) ) UserWarning: None of the values in col1 are in [4, 5, 6]; this might create nulls for all your values in the new categorical column. df['col1'] 0 NaN 1 NaN 2 NaN 3 NaN 4 NaN Name: col1, dtype: category Categories (3, int64): [4 < 5 < 6] .. note:: if categories is None in the kwargs tuple, then the values for categories are inferred from the column; if order is None, then the values for categories are applied unordered. .. note:: column_names and kwargs parameters cannot be used at the same time. Functional usage syntax: ```python import pandas as pd import janitor as jn With `column_names``:: categorical_cols = ['col1', 'col2', 'col4'] df = jn.encode_categorical( df, columns = categorical_cols) # one way With `kwargs``:: df = jn.encode_categorical( df, col1 = (categories, order), col2 = (categories = [values], order=\"sort\" # or \"appearance\" or None ) Method chaining syntax: With `column_names``:: categorical_cols = ['col1', 'col2', 'col4'] df = (pd.DataFrame(...) .encode_categorical(columns=categorical_cols) ) With `kwargs``:: df = ( pd.DataFrame(...) .encode_categorical( col1 = (categories, order), col2 = (categories = [values]/None, order=\"sort\" # or \"appearance\" or None ) ) Parameters: Name Type Description Default df DataFrame The pandas DataFrame object. required column_names Union[str, Iterable[str], Hashable] A column name or an iterable (list or tuple) of column names. None kwargs A pairing of column name to a tuple of ( categories , order ). This is useful in creating categorical columns that are ordered, or if the user needs to explicitly specify the categories. {} Returns: Type Description DataFrame A pandas DataFrame. Exceptions: Type Description ValueError if both column_names and kwargs are provided. Source code in janitor/functions/encode_categorical.py @pf.register_dataframe_method @deprecated_alias(columns=\"column_names\") def encode_categorical( df: pd.DataFrame, column_names: Union[str, Iterable[str], Hashable] = None, **kwargs, ) -> pd.DataFrame: \"\"\"Encode the specified columns with Pandas' [category dtype][cat]. [cat]: http://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html It is syntactic sugar around `pd.Categorical`. This method does not mutate the original DataFrame. Note: In versions < 0.20.11, this method mutates the original DataFrame. TODO: The big chunk of examples below should be moved into a Jupyter notebook. This will keep the docstring consistent and to-the-point. Examples: ```python col1 col2 col3 0 2.0 a 2020-01-01 1 1.0 b 2020-01-02 2 3.0 c 2020-01-03 3 1.0 d 2020-01-04 4 NaN a 2020-01-05 df.dtypes col1 float64 col2 object col3 datetime64[ns] dtype: object ``` Specific columns can be converted to category type: ```python df = ( pd.DataFrame(...) .encode_categorical( column_names=['col1', 'col2', 'col3'] ) ) df.dtypes col1 category col2 category col3 category dtype: object ``` Note that for the code above, the categories were inferred from the columns, and is unordered: df['col3'] 0 2020-01-01 1 2020-01-02 2 2020-01-03 3 2020-01-04 4 2020-01-05 Name: col3, dtype: category Categories (5, datetime64[ns]): [2020-01-01, 2020-01-02, 2020-01-03, 2020-01-04, 2020-01-05] Explicit categories can be provided, and ordered via the `kwargs`` parameter: df = (pd.DataFrame(...) .encode_categorical( col1 = ([3, 2, 1, 4], \"appearance\"), col2 = (['a','d','c','b'], \"sort\") ) ) df['col1'] 0 2 1 1 2 3 3 1 4 NaN Name: col1, dtype: category Categories (4, int64): [3 < 2 < 1 < 4] df['col2'] 0 a 1 b 2 c 3 d 4 a Name: col2, dtype: category Categories (4, object): [a < b < c < d] When the `order` parameter is \"appearance\", the categories argument is used as-is; if the `order` is \"sort\", the categories argument is sorted in ascending order; if `order` is `None``, then the categories argument is applied unordered. A User Warning will be generated if some or all of the unique values in the column are not present in the provided `categories` argument. ```python df = (pd.DataFrame(...) .encode_categorical( col1 = ( categories = [4, 5, 6], order = \"appearance\" ) ) UserWarning: None of the values in col1 are in [4, 5, 6]; this might create nulls for all your values in the new categorical column. df['col1'] 0 NaN 1 NaN 2 NaN 3 NaN 4 NaN Name: col1, dtype: category Categories (3, int64): [4 < 5 < 6] ``` .. note:: if `categories` is None in the `kwargs` tuple, then the values for `categories` are inferred from the column; if `order` is None, then the values for categories are applied unordered. .. note:: `column_names` and `kwargs` parameters cannot be used at the same time. Functional usage syntax: ```python import pandas as pd import janitor as jn - With `column_names``:: categorical_cols = ['col1', 'col2', 'col4'] df = jn.encode_categorical( df, columns = categorical_cols) # one way - With `kwargs``:: df = jn.encode_categorical( df, col1 = (categories, order), col2 = (categories = [values], order=\"sort\" # or \"appearance\" or None ) Method chaining syntax: - With `column_names``:: categorical_cols = ['col1', 'col2', 'col4'] df = (pd.DataFrame(...) .encode_categorical(columns=categorical_cols) ) - With `kwargs``:: df = ( pd.DataFrame(...) .encode_categorical( col1 = (categories, order), col2 = (categories = [values]/None, order=\"sort\" # or \"appearance\" or None ) ) :param df: The pandas DataFrame object. :param column_names: A column name or an iterable (list or tuple) of column names. :param kwargs: A pairing of column name to a tuple of (`categories`, `order`). This is useful in creating categorical columns that are ordered, or if the user needs to explicitly specify the categories. :returns: A pandas DataFrame. :raises ValueError: if both ``column_names`` and ``kwargs`` are provided. \"\"\" # noqa: E501 if all((column_names, kwargs)): raise ValueError( \"\"\" Only one of `column_names` or `kwargs` can be provided. \"\"\" ) # column_names deal with only category dtype (unordered) # kwargs takes care of scenarios where user wants an ordered category # or user supplies specific categories to create the categorical if column_names is not None: check(\"column_names\", column_names, [list, tuple, Hashable]) if isinstance(column_names, (list, tuple)): check_column(df, column_names) dtypes = {col: \"category\" for col in column_names} return df.astype(dtypes) if isinstance(column_names, Hashable): check_column(df, [column_names]) return df.astype({column_names: \"category\"}) return _computations_as_categorical(df, **kwargs) expand_column expand_column(df, column_name, sep='|', concat=True) Expand a categorical column with multiple labels into dummy-coded columns. Super sugary syntax that wraps :py:meth: pandas.Series.str.get_dummies . This method does not mutate the original DataFrame. Functional usage syntax: df = expand_column( df, column_name='col_name', sep=', ' # note space in sep ) Method chaining syntax: import pandas as pd import janitor df = ( pd.DataFrame(...) .expand_column( column_name='col_name', sep=', ' ) ) Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable Which column to expand. required sep str The delimiter, same to :py:meth: ~pandas.Series.str.get_dummies 's sep , default as | . '|' concat bool Whether to return the expanded column concatenated to the original dataframe ( concat=True ), or to return it standalone ( concat=False ). True Returns: Type Description DataFrame A pandas DataFrame with an expanded column. Source code in janitor/functions/expand_column.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def expand_column( df: pd.DataFrame, column_name: Hashable, sep: str = \"|\", concat: bool = True, ) -> pd.DataFrame: \"\"\"Expand a categorical column with multiple labels into dummy-coded columns. Super sugary syntax that wraps :py:meth:`pandas.Series.str.get_dummies`. This method does not mutate the original DataFrame. Functional usage syntax: df = expand_column( df, column_name='col_name', sep=', ' # note space in sep ) Method chaining syntax: import pandas as pd import janitor df = ( pd.DataFrame(...) .expand_column( column_name='col_name', sep=', ' ) ) :param df: A pandas DataFrame. :param column_name: Which column to expand. :param sep: The delimiter, same to :py:meth:`~pandas.Series.str.get_dummies`'s `sep`, default as `|`. :param concat: Whether to return the expanded column concatenated to the original dataframe (`concat=True`), or to return it standalone (`concat=False`). :returns: A pandas DataFrame with an expanded column. \"\"\" expanded_df = df[column_name].str.get_dummies(sep=sep) if concat: df = df.join(expanded_df) return df return expanded_df expand_grid expand_grid(df=None, df_key=None, *, others=None) Creates a DataFrame from a cartesian combination of all inputs. It is not restricted to DataFrame; it can work with any list-like structure that is 1 or 2 dimensional. If method-chaining to a DataFrame, a string argument to df_key parameter must be provided. Data types are preserved in this function, including Pandas' extension array dtypes. The output will always be a DataFrame, usually a MultiIndex, with the keys of the others dictionary serving as the top level columns. If a DataFrame with MultiIndex columns is part of the arguments in others , the columns are flattened, before the final cartesian DataFrame is generated. If a Pandas Series/DataFrame is passed, and has a labeled index, or a MultiIndex index, the index is discarded; the final DataFrame will have a RangeIndex. The MultiIndexed DataFrame can be flattened using pyjanitor's collapse_levels method; the user can also decide to drop any of the levels, via Pandas' droplevel method. Functional usage syntax: import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.expand_grid(df=df, df_key=\"...\", others={...}) Method-chaining usage syntax: import pandas as pd import janitor as jn df = pd.DataFrame(...).expand_grid(df_key=\"bla\",others={...}) Usage independent of a DataFrame .. code-block:: python ```python import pandas as pd from janitor import expand_grid df = expand_grid(others = {\"x\":range(1,4), \"y\":[1,2]}) Parameters: Name Type Description Default df Optional[pandas.core.frame.DataFrame] A pandas DataFrame. None df_key Optional[str] name of key for the dataframe. It becomes part of the column names of the dataframe. None others Optional[Dict] A dictionary that contains the data to be combined with the dataframe. If no dataframe exists, all inputs in others will be combined to create a DataFrame. None Returns: Type Description DataFrame A pandas DataFrame of the cartesian product. Exceptions: Type Description KeyError if there is a DataFrame and df_key is not provided. Source code in janitor/functions/expand_grid.py @pf.register_dataframe_method def expand_grid( df: Optional[pd.DataFrame] = None, df_key: Optional[str] = None, *, others: Optional[Dict] = None, ) -> pd.DataFrame: \"\"\" Creates a DataFrame from a cartesian combination of all inputs. It is not restricted to DataFrame; it can work with any list-like structure that is 1 or 2 dimensional. If method-chaining to a DataFrame, a string argument to `df_key` parameter must be provided. Data types are preserved in this function, including Pandas' extension array dtypes. The output will always be a DataFrame, usually a MultiIndex, with the keys of the `others` dictionary serving as the top level columns. If a DataFrame with MultiIndex columns is part of the arguments in `others`, the columns are flattened, before the final cartesian DataFrame is generated. If a Pandas Series/DataFrame is passed, and has a labeled index, or a MultiIndex index, the index is discarded; the final DataFrame will have a RangeIndex. The MultiIndexed DataFrame can be flattened using pyjanitor's `collapse_levels` method; the user can also decide to drop any of the levels, via Pandas' `droplevel` method. Functional usage syntax: ```python import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.expand_grid(df=df, df_key=\"...\", others={...}) ``` Method-chaining usage syntax: ```python import pandas as pd import janitor as jn df = pd.DataFrame(...).expand_grid(df_key=\"bla\",others={...}) ``` Usage independent of a DataFrame .. code-block:: python ```python import pandas as pd from janitor import expand_grid df = expand_grid(others = {\"x\":range(1,4), \"y\":[1,2]}) :param df: A pandas DataFrame. :param df_key: name of key for the dataframe. It becomes part of the column names of the dataframe. :param others: A dictionary that contains the data to be combined with the dataframe. If no dataframe exists, all inputs in `others` will be combined to create a DataFrame. :returns: A pandas DataFrame of the cartesian product. :raises KeyError: if there is a DataFrame and `df_key` is not provided. \"\"\" if not others: if df is not None: return df return check(\"others\", others, [dict]) # if there is a DataFrame, for the method chaining, # it must have a key, to create a name value pair if df is not None: df = df.copy() if not df_key: raise KeyError( \"\"\" Using `expand_grid` as part of a DataFrame method chain requires that a string argument be provided for the `df_key` parameter. \"\"\" ) check(\"df_key\", df_key, [str]) others = {**{df_key: df}, **others} return _computations_expand_grid(others) factorize_columns factorize_columns(df, column_names, suffix='_enc', **kwargs) Converts labels into numerical data This method will create a new column with the string _enc appended after the original column's name. This can be overriden with the suffix parameter. Internally this method uses pandas factorize method. It takes in an optional suffix and keyword arguments also. An empty string as suffix will override the existing column. This method mutates the original DataFrame. Functional usage syntax: df = factorize_columns( df, column_names=\"my_categorical_column\", suffix=\"_enc\" ) # one way Method chaining syntax: import pandas as pd import janitor categorical_cols = ['col1', 'col2', 'col4'] df = ( pd.DataFrame(...) .factorize_columns( column_names=categorical_cols, suffix=\"_enc\" ) ) Parameters: Name Type Description Default df DataFrame The pandas DataFrame object. required column_names Union[str, Iterable[str], Hashable] A column name or an iterable (list or tuple) of column names. required suffix str Suffix to be used for the new column. Default value is _enc. An empty string suffix means, it will override the existing column '_enc' **kwargs Keyword arguments. It takes any of the keyword arguments, which the pandas factorize method takes like sort,na_sentinel,size_hint {} Returns: Type Description DataFrame A pandas DataFrame. Source code in janitor/functions/factorize_columns.py @pf.register_dataframe_method def factorize_columns( df: pd.DataFrame, column_names: Union[str, Iterable[str], Hashable], suffix: str = \"_enc\", **kwargs, ) -> pd.DataFrame: \"\"\" Converts labels into numerical data This method will create a new column with the string `_enc` appended after the original column's name. This can be overriden with the suffix parameter. Internally this method uses pandas `factorize` method. It takes in an optional suffix and keyword arguments also. An empty string as suffix will override the existing column. This method mutates the original DataFrame. Functional usage syntax: ```python df = factorize_columns( df, column_names=\"my_categorical_column\", suffix=\"_enc\" ) # one way ``` Method chaining syntax: ```python import pandas as pd import janitor categorical_cols = ['col1', 'col2', 'col4'] df = ( pd.DataFrame(...) .factorize_columns( column_names=categorical_cols, suffix=\"_enc\" ) ) ``` :param df: The pandas DataFrame object. :param column_names: A column name or an iterable (list or tuple) of column names. :param suffix: Suffix to be used for the new column. Default value is _enc. An empty string suffix means, it will override the existing column :param **kwargs: Keyword arguments. It takes any of the keyword arguments, which the pandas factorize method takes like sort,na_sentinel,size_hint :returns: A pandas DataFrame. \"\"\" df = _factorize(df, column_names, suffix, **kwargs) return df fill fill_direction(df, **kwargs) Provide a method-chainable function for filling missing values in selected columns. It is a wrapper for pd.Series.ffill and pd.Series.bfill , and pairs the column name with one of up , down , updown , and downup . import pandas as pd import janitor as jn df text code 0 ragnar NaN 1 NaN 2.0 2 sammywemmy 3.0 3 NaN NaN 4 ginger 5.0 Fill on a single column: df.fill_direction(code = 'up') text code 0 ragnar 2.0 1 NaN 2.0 2 sammywemmy 3.0 3 NaN 5.0 4 ginger 5.0 Fill on multiple columns: df.fill_direction(text = 'down', code = 'down') text code 0 ragnar NaN 1 ragnar 2.0 2 sammywemmy 3.0 3 sammywemmy 3.0 4 ginger 5.0 Fill multiple columns in different directions: df.fill_direction(text = 'up', code = 'down') text code 0 ragnar NaN 1 sammywemmy 2.0 2 sammywemmy 3.0 3 ginger 3.0 4 ginger 5.0 Functional usage syntax: import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.fill_direction( df = df, column_1 = direction_1, column_2 = direction_2, ) Method-chaining usage syntax: import pandas as pd import janitor as jn df = pd.DataFrame(...) .fill_direction( column_1 = direction_1, column_2 = direction_2, ) Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required kwargs Key - value pairs of columns and directions. Directions can be either down , up , updown (fill up then down) and downup (fill down then up). {} Returns: Type Description DataFrame A pandas DataFrame with modified column(s). Exceptions: Type Description ValueError if column supplied is not in the DataFrame. ValueError if direction supplied is not one of down , up , updown , or downup . Source code in janitor/functions/fill.py @pf.register_dataframe_method def fill_direction(df: pd.DataFrame, **kwargs) -> pd.DataFrame: \"\"\" Provide a method-chainable function for filling missing values in selected columns. It is a wrapper for `pd.Series.ffill` and `pd.Series.bfill`, and pairs the column name with one of `up`, `down`, `updown`, and `downup`. ```python import pandas as pd import janitor as jn df text code 0 ragnar NaN 1 NaN 2.0 2 sammywemmy 3.0 3 NaN NaN 4 ginger 5.0 ``` Fill on a single column: ```python df.fill_direction(code = 'up') text code 0 ragnar 2.0 1 NaN 2.0 2 sammywemmy 3.0 3 NaN 5.0 4 ginger 5.0 ``` Fill on multiple columns: ```python df.fill_direction(text = 'down', code = 'down') text code 0 ragnar NaN 1 ragnar 2.0 2 sammywemmy 3.0 3 sammywemmy 3.0 4 ginger 5.0 ``` Fill multiple columns in different directions: ```python df.fill_direction(text = 'up', code = 'down') text code 0 ragnar NaN 1 sammywemmy 2.0 2 sammywemmy 3.0 3 ginger 3.0 4 ginger 5.0 ``` Functional usage syntax: ```python import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.fill_direction( df = df, column_1 = direction_1, column_2 = direction_2, ) ``` Method-chaining usage syntax: ```python import pandas as pd import janitor as jn df = pd.DataFrame(...) .fill_direction( column_1 = direction_1, column_2 = direction_2, ) ``` :param df: A pandas DataFrame. :param kwargs: Key - value pairs of columns and directions. Directions can be either `down`, `up`, `updown` (fill up then down) and `downup` (fill down then up). :returns: A pandas DataFrame with modified column(s). :raises ValueError: if column supplied is not in the DataFrame. :raises ValueError: if direction supplied is not one of `down`, `up`, `updown`, or `downup`. \"\"\" if not kwargs: return df fill_types = {fill.name for fill in _FILLTYPE} for column_name, fill_type in kwargs.items(): check(\"column_name\", column_name, [str]) check(\"fill_type\", fill_type, [str]) if fill_type.upper() not in fill_types: raise ValueError( \"\"\" fill_type should be one of up, down, updown, or downup. \"\"\" ) check_column(df, kwargs) new_values = {} for column_name, fill_type in kwargs.items(): direction = _FILLTYPE[f\"{fill_type.upper()}\"].value if len(direction) == 1: direction = methodcaller(direction[0]) output = direction(df[column_name]) else: direction = [methodcaller(entry) for entry in direction] output = _chain_func(df[column_name], *direction) new_values[column_name] = output return df.assign(**new_values) fill_empty(df, column_names, value) Fill NaN values in specified columns with a given value. Super sugary syntax that wraps pandas.DataFrame.fillna . This method mutates the original DataFrame. Functional usage syntax: df = fill_empty(df, column_names=[col1, col2], value=0) Method chaining syntax: import pandas as pd import janitor df = pd.DataFrame(...).fill_empty(column_names=col1, value=0) Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_names Union[str, Iterable[str], Hashable] column_names: A column name or an iterable (list or tuple) of column names. If a single column name is passed in, then only that column will be filled; if a list or tuple is passed in, then those columns will all be filled with the same value. required value The value that replaces the NaN values. required Returns: Type Description DataFrame A pandas DataFrame with NaN values filled. Source code in janitor/functions/fill.py @pf.register_dataframe_method @deprecated_alias(columns=\"column_names\") def fill_empty( df: pd.DataFrame, column_names: Union[str, Iterable[str], Hashable], value ) -> pd.DataFrame: \"\"\" Fill `NaN` values in specified columns with a given value. Super sugary syntax that wraps `pandas.DataFrame.fillna`. This method mutates the original DataFrame. Functional usage syntax: ```python df = fill_empty(df, column_names=[col1, col2], value=0) ``` Method chaining syntax: ```python import pandas as pd import janitor df = pd.DataFrame(...).fill_empty(column_names=col1, value=0) ``` :param df: A pandas DataFrame. :param column_names: column_names: A column name or an iterable (list or tuple) of column names. If a single column name is passed in, then only that column will be filled; if a list or tuple is passed in, then those columns will all be filled with the same value. :param value: The value that replaces the `NaN` values. :returns: A pandas DataFrame with `NaN` values filled. \"\"\" check_column(df, column_names) return _fill_empty(df, column_names, value=value) filter filter_column_isin(df, column_name, iterable, complement=False) Filter a dataframe for values in a column that exist in another iterable. This method does not mutate the original DataFrame. Assumes exact matching; fuzzy matching not implemented. The below example syntax will filter the DataFrame such that we only get rows for which the names are exactly James and John . df = ( pd.DataFrame(...) .clean_names() .filter_column_isin(column_name=\"names\", iterable=[\"James\", \"John\"] ) ) This is the method chaining alternative to: df = df[df['names'].isin(['James', 'John'])] If complement is True , then we will only get rows for which the names are not James or John . Parameters: Name Type Description Default df DataFrame A pandas DataFrame required column_name Hashable The column on which to filter. required iterable Iterable An iterable. Could be a list, tuple, another pandas Series. required complement bool Whether to return the complement of the selection or not. False Returns: Type Description DataFrame A filtered pandas DataFrame. Exceptions: Type Description ValueError if iterable does not have a length of 1 or greater. Source code in janitor/functions/filter.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def filter_column_isin( df: pd.DataFrame, column_name: Hashable, iterable: Iterable, complement: bool = False, ) -> pd.DataFrame: \"\"\" Filter a dataframe for values in a column that exist in another iterable. This method does not mutate the original DataFrame. Assumes exact matching; fuzzy matching not implemented. The below example syntax will filter the DataFrame such that we only get rows for which the `names` are exactly `James` and `John`. ```python df = ( pd.DataFrame(...) .clean_names() .filter_column_isin(column_name=\"names\", iterable=[\"James\", \"John\"] ) ) ``` This is the method chaining alternative to: ```python df = df[df['names'].isin(['James', 'John'])] ``` If `complement` is `True`, then we will only get rows for which the names are not `James` or `John`. :param df: A pandas DataFrame :param column_name: The column on which to filter. :param iterable: An iterable. Could be a list, tuple, another pandas Series. :param complement: Whether to return the complement of the selection or not. :returns: A filtered pandas DataFrame. :raises ValueError: if `iterable` does not have a length of `1` or greater. \"\"\" if len(iterable) == 0: raise ValueError( \"`iterable` kwarg must be given an iterable of length 1 or greater\" ) criteria = df[column_name].isin(iterable) if complement: return df[~criteria] return df[criteria] filter_date(df, column_name, start_date=None, end_date=None, years=None, months=None, days=None, column_date_options=None, format=None) Filter a date-based column based on certain criteria. This method does not mutate the original DataFrame. Dates may be finicky and this function builds on top of the magic from the pandas to_datetime function that is able to parse dates well. Additional options to parse the date type of your column may be found at the official pandas documentation Note This method will cast your column to a Timestamp! Note This only affects the format of the start_date and end_date parameters. If there's an issue with the format of the DataFrame being parsed, you would pass {'format': your_format} to column_date_options . Parameters: Name Type Description Default df DataFrame The dataframe to filter on. required column_name Hashable The column which to apply the fraction transformation. required start_date Optional[datetime.date] The beginning date to use to filter the DataFrame. None end_date Optional[datetime.date] The end date to use to filter the DataFrame. None years Optional[List] The years to use to filter the DataFrame. None months Optional[List] The months to use to filter the DataFrame. None days Optional[List] The days to use to filter the DataFrame. None column_date_options Optional[Dict] 'Special options to use when parsing the date column in the original DataFrame. The options may be found at the official Pandas documentation.' None format Optional[str] 'If you're using a format for start_date or end_date that is not recognized natively by pandas' to_datetime function, you may supply the format yourself. Python date and time formats may be found at link . None Returns: Type Description DataFrame A filtered pandas DataFrame. Source code in janitor/functions/filter.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\", start=\"start_date\", end=\"end_date\") def filter_date( df: pd.DataFrame, column_name: Hashable, start_date: Optional[dt.date] = None, end_date: Optional[dt.date] = None, years: Optional[List] = None, months: Optional[List] = None, days: Optional[List] = None, column_date_options: Optional[Dict] = None, format: Optional[str] = None, # skipcq: PYL-W0622 ) -> pd.DataFrame: \"\"\" Filter a date-based column based on certain criteria. This method does not mutate the original DataFrame. Dates may be finicky and this function builds on top of the *magic* from the pandas `to_datetime` function that is able to parse dates well. Additional options to parse the date type of your column may be found at the official pandas [documentation][datetime] [datetime]: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html !!!note This method will cast your column to a Timestamp! !!!note This only affects the format of the `start_date` and `end_date` parameters. If there's an issue with the format of the DataFrame being parsed, you would pass `{'format': your_format}` to `column_date_options`. :param df: The dataframe to filter on. :param column_name: The column which to apply the fraction transformation. :param start_date: The beginning date to use to filter the DataFrame. :param end_date: The end date to use to filter the DataFrame. :param years: The years to use to filter the DataFrame. :param months: The months to use to filter the DataFrame. :param days: The days to use to filter the DataFrame. :param column_date_options: 'Special options to use when parsing the date column in the original DataFrame. The options may be found at the official Pandas documentation.' :param format: 'If you're using a format for `start_date` or `end_date` that is not recognized natively by pandas' `to_datetime` function, you may supply the format yourself. Python date and time formats may be found at [link](http://strftime.org/). :returns: A filtered pandas DataFrame. \"\"\" # noqa: E501 def _date_filter_conditions(conditions): \"\"\"Taken from: https://stackoverflow.com/a/13616382.\"\"\" return reduce(np.logical_and, conditions) if column_date_options: df.loc[:, column_name] = pd.to_datetime( df.loc[:, column_name], **column_date_options ) else: df.loc[:, column_name] = pd.to_datetime(df.loc[:, column_name]) _filter_list = [] if start_date: start_date = pd.to_datetime(start_date, format=format) _filter_list.append(df.loc[:, column_name] >= start_date) if end_date: end_date = pd.to_datetime(end_date, format=format) _filter_list.append(df.loc[:, column_name] <= end_date) if years: _filter_list.append(df.loc[:, column_name].dt.year.isin(years)) if months: _filter_list.append(df.loc[:, column_name].dt.month.isin(months)) if days: _filter_list.append(df.loc[:, column_name].dt.day.isin(days)) if start_date and end_date and start_date > end_date: warnings.warn( f\"Your start date of {start_date} is after your end date of \" f\"{end_date}. Is this intended?\" ) return df.loc[_date_filter_conditions(_filter_list), :] filter_on(df, criteria, complement=False) Return a dataframe filtered on a particular criteria. This method does not mutate the original DataFrame. This is super-sugary syntax that wraps the pandas .query() API, enabling users to use strings to quickly specify filters for filtering their dataframe. The intent is that filter_on as a verb better matches the intent of a pandas user than the verb query . Let's say we wanted to filter students based on whether they failed an exam or not, which is defined as their score (in the \"score\" column) being less than 50. df = (pd.DataFrame(...) .filter_on('score < 50', complement=False) ...) # chain on more data preprocessing. This stands in contrast to the in-place syntax that is usually used: df = pd.DataFrame(...) df = df[df['score'] < 3] As with the filter_string function, a more seamless flow can be expressed in the code. Functional usage syntax: df = filter_on(df, 'score < 50', complement=False) Method chaining syntax: .filter_on('score < 50', complement=False)) ``` Credit to Brant Peterson for the name. Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required criteria str A filtering criteria that returns an array or Series of booleans, on which pandas can filter on. required complement bool Whether to return the complement of the filter or not. False Returns: Type Description DataFrame A filtered pandas DataFrame. Source code in janitor/functions/filter.py @pf.register_dataframe_method def filter_on( df: pd.DataFrame, criteria: str, complement: bool = False ) -> pd.DataFrame: \"\"\" Return a dataframe filtered on a particular criteria. This method does not mutate the original DataFrame. This is super-sugary syntax that wraps the pandas `.query()` API, enabling users to use strings to quickly specify filters for filtering their dataframe. The intent is that `filter_on` as a verb better matches the intent of a pandas user than the verb `query`. Let's say we wanted to filter students based on whether they failed an exam or not, which is defined as their score (in the \"score\" column) being less than 50. ```python df = (pd.DataFrame(...) .filter_on('score < 50', complement=False) ...) # chain on more data preprocessing. ``` This stands in contrast to the in-place syntax that is usually used: ```python df = pd.DataFrame(...) df = df[df['score'] < 3] ``` As with the `filter_string` function, a more seamless flow can be expressed in the code. Functional usage syntax: ```python df = filter_on(df, 'score < 50', complement=False) ``` Method chaining syntax: .filter_on('score < 50', complement=False)) ``` Credit to Brant Peterson for the name. :param df: A pandas DataFrame. :param criteria: A filtering criteria that returns an array or Series of booleans, on which pandas can filter on. :param complement: Whether to return the complement of the filter or not. :returns: A filtered pandas DataFrame. \"\"\" if complement: return df.query(\"not \" + criteria) return df.query(criteria) filter_string(df, column_name, search_string, complement=False) Filter a string-based column according to whether it contains a substring. This is super sugary syntax that builds on top of pandas.Series.str.contains . Because this uses internally pandas.Series.str.contains , which allows a regex string to be passed into it, thus search_string can also be a regex pattern. This method does not mutate the original DataFrame. This function allows us to method chain filtering operations: df = (pd.DataFrame(...) .filter_string('column', search_string='pattern', complement=False) ...) # chain on more data preprocessing. This stands in contrast to the in-place syntax that is usually used: df = pd.DataFrame(...) df = df[df['column'].str.contains('pattern')]] As can be seen here, the API design allows for a more seamless flow in expressing the filtering operations. Functional usage syntax: df = filter_string(df, column_name='column', search_string='pattern', complement=False) Method chaining syntax: df = (pd.DataFrame(...) .filter_string(column_name='column', search_string='pattern', complement=False) ...) Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable The column to filter. The column should contain strings. required search_string str A regex pattern or a (sub-)string to search. required complement bool Whether to return the complement of the filter or not. False Returns: Type Description DataFrame A filtered pandas DataFrame. Source code in janitor/functions/filter.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def filter_string( df: pd.DataFrame, column_name: Hashable, search_string: str, complement: bool = False, ) -> pd.DataFrame: \"\"\" Filter a string-based column according to whether it contains a substring. This is super sugary syntax that builds on top of `pandas.Series.str.contains`. Because this uses internally `pandas.Series.str.contains`, which allows a regex string to be passed into it, thus `search_string` can also be a regex pattern. This method does not mutate the original DataFrame. This function allows us to method chain filtering operations: ```python df = (pd.DataFrame(...) .filter_string('column', search_string='pattern', complement=False) ...) # chain on more data preprocessing. ``` This stands in contrast to the in-place syntax that is usually used: ```python df = pd.DataFrame(...) df = df[df['column'].str.contains('pattern')]] ``` As can be seen here, the API design allows for a more seamless flow in expressing the filtering operations. Functional usage syntax: ```python df = filter_string(df, column_name='column', search_string='pattern', complement=False) ``` Method chaining syntax: ```python df = (pd.DataFrame(...) .filter_string(column_name='column', search_string='pattern', complement=False) ...) ``` :param df: A pandas DataFrame. :param column_name: The column to filter. The column should contain strings. :param search_string: A regex pattern or a (sub-)string to search. :param complement: Whether to return the complement of the filter or not. :returns: A filtered pandas DataFrame. \"\"\" # noqa: E501 criteria = df[column_name].str.contains(search_string) if complement: return df[~criteria] return df[criteria] find_replace find_replace(df, match='exact', **mappings) Perform a find-and-replace action on provided columns. Depending on use case, users can choose either exact, full-value matching, or regular-expression-based fuzzy matching (hence allowing substring matching in the latter case). For strings, the matching is always case sensitive. For instance, given a DataFrame containing orders at a coffee shop: df = pd.DataFrame({ 'customer': ['Mary', 'Tom', 'Lila'], 'order': ['ice coffee', 'lemonade', 'regular coffee'] }) df customer order 0 Mary ice coffee 1 Tom lemonade 2 Lila regular coffee Our task is to replace values ice coffee and regular coffee of the order column into latte . Example 1 for exact matching #Functional usage df = find_replace( df, match='exact', order={'ice coffee': 'latte', 'regular coffee': 'latte'}, ) # Method chaining usage df = df.find_replace( match='exact' order={'ice coffee': 'latte', 'regular coffee': 'latte'}, ) Example 2: Regular-expression-based matching # Functional usage df = find_replace( df, match='regex', order={'coffee$': 'latte'}, ) # Method chaining usage df = df.find_replace( match='regex', order={'coffee$': 'latte'}, ) To perform a find and replace on the entire DataFrame, pandas' df.replace() function provides the appropriate functionality. You can find more detail on the replace docs. This function only works with column names that have no spaces or punctuation in them. For example, a column name item_name would work with find_replace , because it is a contiguous string that can be parsed correctly, but item name would not be parsed correctly by the Python interpreter. If you have column names that might not be compatible, we recommend calling on clean_names() as the first method call. If, for whatever reason, that is not possible, then _find_replace is available as a function that you can do a pandas pipe call on. Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required match str Whether or not to perform an exact match or not. Valid values are \"exact\" or \"regex\". 'exact' mappings keyword arguments corresponding to column names that have dictionaries passed in indicating what to find (keys) and what to replace with (values). {} Returns: Type Description DataFrame A pandas DataFrame with replaced values. Source code in janitor/functions/find_replace.py @pf.register_dataframe_method def find_replace( df: pd.DataFrame, match: str = \"exact\", **mappings ) -> pd.DataFrame: \"\"\" Perform a find-and-replace action on provided columns. Depending on use case, users can choose either exact, full-value matching, or regular-expression-based fuzzy matching (hence allowing substring matching in the latter case). For strings, the matching is always case sensitive. For instance, given a DataFrame containing orders at a coffee shop: ```python df = pd.DataFrame({ 'customer': ['Mary', 'Tom', 'Lila'], 'order': ['ice coffee', 'lemonade', 'regular coffee'] }) df customer order 0 Mary ice coffee 1 Tom lemonade 2 Lila regular coffee ``` Our task is to replace values `ice coffee` and `regular coffee` of the `order` column into `latte`. Example 1 for exact matching ```python #Functional usage df = find_replace( df, match='exact', order={'ice coffee': 'latte', 'regular coffee': 'latte'}, ) ``` ```python # Method chaining usage df = df.find_replace( match='exact' order={'ice coffee': 'latte', 'regular coffee': 'latte'}, ) ``` Example 2: Regular-expression-based matching ```python # Functional usage df = find_replace( df, match='regex', order={'coffee$': 'latte'}, ) ``` ```python # Method chaining usage df = df.find_replace( match='regex', order={'coffee$': 'latte'}, ) ``` To perform a find and replace on the entire DataFrame, pandas' `df.replace()` function provides the appropriate functionality. You can find more detail on the [replace] docs. [replace]: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html This function only works with column names that have no spaces or punctuation in them. For example, a column name `item_name` would work with `find_replace`, because it is a contiguous string that can be parsed correctly, but `item name` would not be parsed correctly by the Python interpreter. If you have column names that might not be compatible, we recommend calling on `clean_names()` as the first method call. If, for whatever reason, that is not possible, then `_find_replace` is available as a function that you can do a pandas [pipe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pipe.html) call on. :param df: A pandas DataFrame. :param match: Whether or not to perform an exact match or not. Valid values are \"exact\" or \"regex\". :param mappings: keyword arguments corresponding to column names that have dictionaries passed in indicating what to find (keys) and what to replace with (values). :returns: A pandas DataFrame with replaced values. \"\"\" # noqa: E501 for column_name, mapper in mappings.items(): df = _find_replace(df, column_name, mapper, match=match) return df flag_nulls flag_nulls(df, column_name='null_flag', columns=None) Creates a new column to indicate whether you have null values in a given row. If the columns parameter is not set, looks across the entire DataFrame, otherwise will look only in the columns you set. import pandas as pd import janitor as jn df = pd.DataFrame( {'a': [1, 2, None, 4], 'b': [5.0, None, 7.0, 8.0]}) df.flag_nulls() jn.functions.flag_nulls(df) df.flag_nulls(columns=['b']) Parameters: Name Type Description Default df DataFrame Input Pandas dataframe. required column_name Optional[Hashable] Name for the output column. Defaults to 'null_flag'. 'null_flag' columns Union[str, Iterable[str], Hashable] List of columns to look at for finding null values. If you only want to look at one column, you can simply give its name. If set to None (default), all DataFrame columns are used. None Returns: Type Description DataFrame Input dataframe with the null flag column. Exceptions: Type Description ValueError if column_name is already present in the DataFrame. ValueError if a column within columns is no present in the DataFrame. .. # noqa: DAR402 Source code in janitor/functions/flag_nulls.py @pf.register_dataframe_method def flag_nulls( df: pd.DataFrame, column_name: Optional[Hashable] = \"null_flag\", columns: Optional[Union[str, Iterable[str], Hashable]] = None, ) -> pd.DataFrame: \"\"\"Creates a new column to indicate whether you have null values in a given row. If the columns parameter is not set, looks across the entire DataFrame, otherwise will look only in the columns you set. ```python import pandas as pd import janitor as jn df = pd.DataFrame( {'a': [1, 2, None, 4], 'b': [5.0, None, 7.0, 8.0]}) df.flag_nulls() jn.functions.flag_nulls(df) df.flag_nulls(columns=['b']) ``` :param df: Input Pandas dataframe. :param column_name: Name for the output column. Defaults to 'null_flag'. :param columns: List of columns to look at for finding null values. If you only want to look at one column, you can simply give its name. If set to None (default), all DataFrame columns are used. :returns: Input dataframe with the null flag column. :raises ValueError: if `column_name` is already present in the DataFrame. :raises ValueError: if a column within `columns` is no present in the DataFrame. .. # noqa: DAR402 \"\"\" # Sort out columns input if isinstance(columns, str): columns = [columns] elif columns is None: columns = df.columns elif not isinstance(columns, Iterable): # catches other hashable types columns = [columns] # Input sanitation checks check_column(df, columns) check_column(df, [column_name], present=False) # This algorithm works best for n_rows >> n_cols. See issue #501 null_array = np.zeros(len(df)) for col in columns: null_array = np.logical_or(null_array, pd.isna(df[col])) df = df.copy() df[column_name] = null_array.astype(int) return df get_dupes get_dupes(df, column_names=None) Return all duplicate rows. This method does not mutate the original DataFrame. Functional usage syntax: df = pd.DataFrame(...) df = get_dupes(df) Method chaining syntax: import pandas as pd import janitor df = pd.DataFrame(...).get_dupes() Parameters: Name Type Description Default df DataFrame The pandas DataFrame object. required column_names Union[str, Iterable[str], Hashable] (optional) A column name or an iterable (list or tuple) of column names. Following pandas API, this only considers certain columns for identifying duplicates. Defaults to using all columns. None Returns: Type Description DataFrame The duplicate rows, as a pandas DataFrame. Source code in janitor/functions/get_dupes.py @pf.register_dataframe_method @deprecated_alias(columns=\"column_names\") def get_dupes( df: pd.DataFrame, column_names: Optional[Union[str, Iterable[str], Hashable]] = None, ) -> pd.DataFrame: \"\"\" Return all duplicate rows. This method does not mutate the original DataFrame. Functional usage syntax: ```python df = pd.DataFrame(...) df = get_dupes(df) ``` Method chaining syntax: ```python import pandas as pd import janitor df = pd.DataFrame(...).get_dupes() ``` :param df: The pandas DataFrame object. :param column_names: (optional) A column name or an iterable (list or tuple) of column names. Following pandas API, this only considers certain columns for identifying duplicates. Defaults to using all columns. :returns: The duplicate rows, as a pandas DataFrame. \"\"\" dupes = df.duplicated(subset=column_names, keep=False) return df[dupes == True] # noqa: E712 groupby_agg groupby_agg(df, by, new_column_name, agg_column_name, agg, dropna=True) Shortcut for assigning a groupby-transform to a new column. This method does not mutate the original DataFrame. Without this function, we would have to write a verbose line: df = df.assign(...=df.groupby(...)[...].transform(...)) Now, this function can be method-chained: import pandas as pd import janitor df = pd.DataFrame(...).groupby_agg(by='group', agg='mean', agg_column_name=\"col1\" new_column_name='col1_mean_by_group', dropna = True/False) Examples:: import pandas as pd import janitor as jn group var1 0 1 1 1 1 1 2 1 1 3 1 1 4 1 2 5 2 1 6 2 2 7 2 2 8 2 2 9 2 3 Let's get the count per group and var1 :: df.groupby_agg( by = ['group', 'var1'], agg = 'size', agg_column_name = 'var1', new_column_name = 'count' ) group var1 size 0 1 1 4 1 1 1 4 2 1 1 4 3 1 1 4 4 1 2 1 5 2 1 1 6 2 2 3 7 2 2 3 8 2 2 3 9 2 3 1 If the data has null values, you can include the null values by passing False to dropna ; this feature was introduced in Pandas 1.1:: name type num nulls 0 black chair 4 1.0 1 black chair 5 1.0 2 black sofa 12 NaN 3 red sofa 4 NaN 4 red plate 3 3.0 Let's get the count, including the null values, grouping on nulls column:: df.groupby_agg( by=\"nulls\", new_column_name=\"num_count\", agg_column_name=\"num\", agg=\"size\", dropna=False, ) name type num nulls num_count 0 black chair 4 1.0 2 1 black chair 5 1.0 2 2 black sofa 12 NaN 2 3 red sofa 4 NaN 2 4 red plate 3 3.0 1 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required by Union[List, str] Column(s) to groupby on, either a str or a list of str required new_column_name str Name of the aggregation output column. required agg_column_name str Name of the column to aggregate over. required agg Union[Callable, str] How to aggregate. required dropna bool Whether or not to include null values, if present in the by column(s). Default is True. True Returns: Type Description DataFrame A pandas DataFrame. Source code in janitor/functions/groupby_agg.py @pf.register_dataframe_method @deprecated_alias(new_column=\"new_column_name\", agg_column=\"agg_column_name\") def groupby_agg( df: pd.DataFrame, by: Union[List, str], new_column_name: str, agg_column_name: str, agg: Union[Callable, str], dropna: bool = True, ) -> pd.DataFrame: \"\"\" Shortcut for assigning a groupby-transform to a new column. This method does not mutate the original DataFrame. Without this function, we would have to write a verbose line: df = df.assign(...=df.groupby(...)[...].transform(...)) Now, this function can be method-chained: import pandas as pd import janitor df = pd.DataFrame(...).groupby_agg(by='group', agg='mean', agg_column_name=\"col1\" new_column_name='col1_mean_by_group', dropna = True/False) Examples:: import pandas as pd import janitor as jn group var1 0 1 1 1 1 1 2 1 1 3 1 1 4 1 2 5 2 1 6 2 2 7 2 2 8 2 2 9 2 3 Let's get the count per `group` and `var1`:: df.groupby_agg( by = ['group', 'var1'], agg = 'size', agg_column_name = 'var1', new_column_name = 'count' ) group var1 size 0 1 1 4 1 1 1 4 2 1 1 4 3 1 1 4 4 1 2 1 5 2 1 1 6 2 2 3 7 2 2 3 8 2 2 3 9 2 3 1 If the data has null values, you can include the null values by passing `False` to `dropna`; this feature was introduced in Pandas 1.1:: name type num nulls 0 black chair 4 1.0 1 black chair 5 1.0 2 black sofa 12 NaN 3 red sofa 4 NaN 4 red plate 3 3.0 Let's get the count, including the null values, grouping on `nulls` column:: df.groupby_agg( by=\"nulls\", new_column_name=\"num_count\", agg_column_name=\"num\", agg=\"size\", dropna=False, ) name type num nulls num_count 0 black chair 4 1.0 2 1 black chair 5 1.0 2 2 black sofa 12 NaN 2 3 red sofa 4 NaN 2 4 red plate 3 3.0 1 :param df: A pandas DataFrame. :param by: Column(s) to groupby on, either a `str` or a `list` of `str` :param new_column_name: Name of the aggregation output column. :param agg_column_name: Name of the column to aggregate over. :param agg: How to aggregate. :param dropna: Whether or not to include null values, if present in the `by` column(s). Default is True. :returns: A pandas DataFrame. \"\"\" df = df.copy() df[new_column_name] = df.groupby(by, dropna=dropna)[ agg_column_name ].transform(agg) return df groupby_topk groupby_topk(df, groupby_column_name, sort_column_name, k, sort_values_kwargs=None) Return top k rows from a groupby of a set of columns. Returns a DataFrame that has the top k values grouped by groupby_column_name and sorted by sort_column_name . Additional parameters to the sorting (such as ascending=True ) can be passed using sort_values_kwargs . List of all sort_values() parameters can be found here . import pandas as pd import janitor as jn age ID result 0 20 1 pass 1 22 2 fail 2 24 3 pass 3 23 4 pass 4 21 5 fail 5 22 6 pass Ascending top 3: df.groupby_topk('result', 'age', 3) age ID result result fail 4 21 5 fail 1 22 2 fail pass 0 20 1 pass 5 22 6 pass 3 23 4 pass Descending top 2: df.groupby_topk('result', 'age', 2, {'ascending':False}) age ID result result fail 1 22 2 fail 4 21 5 fail pass 2 24 3 pass 3 23 4 pass Functional usage syntax: import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.groupby_topk( df = df, groupby_column_name = 'groupby_column', sort_column_name = 'sort_column', k = 5 ) Method-chaining usage syntax: import pandas as pd import janitor as jn df = ( pd.DataFrame(...) .groupby_topk( df = df, groupby_column_name = 'groupby_column', sort_column_name = 'sort_column', k = 5 ) ) Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required groupby_column_name Hashable Column name to group input DataFrame df by. required sort_column_name Hashable Name of the column to sort along the input DataFrame df . required k int Number of top rows to return from each group after sorting. required sort_values_kwargs Dict Arguments to be passed to sort_values function. None Returns: Type Description DataFrame A pandas DataFrame with top k rows that are grouped by groupby_column_name column with each group sorted along the column sort_column_name . Exceptions: Type Description ValueError if k is less than 1. ValueError if groupby_column_name not in DataFrame df . ValueError if sort_column_name not in DataFrame df . KeyError if inplace:True is present in sort_values_kwargs . Source code in janitor/functions/groupby_topk.py @pf.register_dataframe_method def groupby_topk( df: pd.DataFrame, groupby_column_name: Hashable, sort_column_name: Hashable, k: int, sort_values_kwargs: Dict = None, ) -> pd.DataFrame: \"\"\" Return top `k` rows from a groupby of a set of columns. Returns a DataFrame that has the top `k` values grouped by `groupby_column_name` and sorted by `sort_column_name`. Additional parameters to the sorting (such as `ascending=True`) can be passed using `sort_values_kwargs`. List of all sort_values() parameters can be found [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html). ```python import pandas as pd import janitor as jn age ID result 0 20 1 pass 1 22 2 fail 2 24 3 pass 3 23 4 pass 4 21 5 fail 5 22 6 pass ``` Ascending top 3: ```python df.groupby_topk('result', 'age', 3) age ID result result fail 4 21 5 fail 1 22 2 fail pass 0 20 1 pass 5 22 6 pass 3 23 4 pass ``` Descending top 2: ```python df.groupby_topk('result', 'age', 2, {'ascending':False}) age ID result result fail 1 22 2 fail 4 21 5 fail pass 2 24 3 pass 3 23 4 pass ``` Functional usage syntax: ```python import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.groupby_topk( df = df, groupby_column_name = 'groupby_column', sort_column_name = 'sort_column', k = 5 ) ``` Method-chaining usage syntax: ```python import pandas as pd import janitor as jn df = ( pd.DataFrame(...) .groupby_topk( df = df, groupby_column_name = 'groupby_column', sort_column_name = 'sort_column', k = 5 ) ) ``` :param df: A pandas DataFrame. :param groupby_column_name: Column name to group input DataFrame `df` by. :param sort_column_name: Name of the column to sort along the input DataFrame `df`. :param k: Number of top rows to return from each group after sorting. :param sort_values_kwargs: Arguments to be passed to sort_values function. :returns: A pandas DataFrame with top `k` rows that are grouped by `groupby_column_name` column with each group sorted along the column `sort_column_name`. :raises ValueError: if `k` is less than 1. :raises ValueError: if `groupby_column_name` not in DataFrame `df`. :raises ValueError: if `sort_column_name` not in DataFrame `df`. :raises KeyError: if `inplace:True` is present in `sort_values_kwargs`. \"\"\" # noqa: E501 # Convert the default sort_values_kwargs from None to empty Dict sort_values_kwargs = sort_values_kwargs or {} # Check if groupby_column_name and sort_column_name exists in the DataFrame check_column(df, [groupby_column_name, sort_column_name]) # Check if k is greater than 0. if k < 1: raise ValueError( \"Numbers of rows per group to be returned must be greater than 0.\" ) # Check if inplace:True in sort values kwargs because it returns None if ( \"inplace\" in sort_values_kwargs.keys() and sort_values_kwargs[\"inplace\"] ): raise KeyError(\"Cannot use `inplace=True` in `sort_values_kwargs`.\") return df.groupby(groupby_column_name).apply( lambda d: d.sort_values(sort_column_name, **sort_values_kwargs).head(k) ) impute impute(df, column_name, value=None, statistic_column_name=None) Method-chainable imputation of values in a column. This method mutates the original DataFrame. Underneath the hood, this function calls the .fillna() method available to every pandas.Series object. Method-chaining example: import numpy as np import pandas as pd import janitor data = { \"a\": [1, 2, 3], \"sales\": np.nan, \"score\": [np.nan, 3, 2]} df = ( pd.DataFrame(data) # Impute null values with 0 .impute(column_name='sales', value=0.0) # Impute null values with median .impute(column_name='score', statistic_column_name='median') ) Either one of value or statistic_column_name should be provided. If value is provided, then all null values in the selected column will take on the value provided. If statistic_column_name is provided, then all null values in the selected column will take on the summary statistic value of other non-null values. Currently supported statistics include: mean (also aliased by average ) median mode minimum (also aliased by min ) maximum (also aliased by max ) Parameters: Name Type Description Default df DataFrame A pandas DataFrame required column_name Hashable The name of the column on which to impute values. required value Optional[Any] (optional) The value to impute. None statistic_column_name Optional[str] (optional) The column statistic to impute. None Returns: Type Description DataFrame An imputed pandas DataFrame. Exceptions: Type Description ValueError if both value and statistic are provided. KeyError if statistic is not one of mean , average median , mode , minimum , min , maximum , or max . Source code in janitor/functions/impute.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") @deprecated_alias(statistic=\"statistic_column_name\") def impute( df: pd.DataFrame, column_name: Hashable, value: Optional[Any] = None, statistic_column_name: Optional[str] = None, ) -> pd.DataFrame: \"\"\" Method-chainable imputation of values in a column. This method mutates the original DataFrame. Underneath the hood, this function calls the `.fillna()` method available to every `pandas.Series` object. Method-chaining example: ```python import numpy as np import pandas as pd import janitor data = { \"a\": [1, 2, 3], \"sales\": np.nan, \"score\": [np.nan, 3, 2]} df = ( pd.DataFrame(data) # Impute null values with 0 .impute(column_name='sales', value=0.0) # Impute null values with median .impute(column_name='score', statistic_column_name='median') ) ``` Either one of `value` or `statistic_column_name` should be provided. If `value` is provided, then all null values in the selected column will take on the value provided. If `statistic_column_name` is provided, then all null values in the selected column will take on the summary statistic value of other non-null values. Currently supported statistics include: - `mean` (also aliased by `average`) - `median` - `mode` - `minimum` (also aliased by `min`) - `maximum` (also aliased by `max`) :param df: A pandas DataFrame :param column_name: The name of the column on which to impute values. :param value: (optional) The value to impute. :param statistic_column_name: (optional) The column statistic to impute. :returns: An imputed pandas DataFrame. :raises ValueError: if both `value` and `statistic` are provided. :raises KeyError: if `statistic` is not one of `mean`, `average` `median`, `mode`, `minimum`, `min`, `maximum`, or `max`. \"\"\" # Firstly, we check that only one of `value` or `statistic` are provided. if value is not None and statistic_column_name is not None: raise ValueError( \"Only one of `value` or `statistic` should be provided\" ) # If statistic is provided, then we compute the relevant summary statistic # from the other data. funcs = { \"mean\": np.mean, \"average\": np.mean, # aliased \"median\": np.median, \"mode\": mode, \"minimum\": np.min, \"min\": np.min, # aliased \"maximum\": np.max, \"max\": np.max, # aliased } if statistic_column_name is not None: # Check that the statistic keyword argument is one of the approved. if statistic_column_name not in funcs.keys(): raise KeyError(f\"`statistic` must be one of {funcs.keys()}\") value = funcs[statistic_column_name]( df[column_name].dropna().to_numpy() ) # special treatment for mode, because scipy stats mode returns a # moderesult object. if statistic_column_name == \"mode\": value = value.mode[0] # The code is architected this way - if `value` is not provided but # statistic is, we then overwrite the None value taken on by `value`, and # use it to set the imputation column. if value is not None: df[column_name] = df[column_name].fillna(value) return df jitter jitter(df, column_name, dest_column_name, scale, clip=None, random_state=None) Adds Gaussian noise (jitter) to the values of a column. Functional usage syntax: import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.functions.jitter( df=df, column_name='values', dest_column_name='values_jitter', scale=1.0, clip=None, random_state=None, ) Method chaining usage example: import pandas as pd import janitor df = pd.DataFrame(...) df = df.jitter( column_name='values', dest_column_name='values_jitter', scale=1.0, clip=None, random_state=None, ) A new column will be created containing the values of the original column with Gaussian noise added. For each value in the column, a Gaussian distribution is created having a location (mean) equal to the value and a scale (standard deviation) equal to scale . A random value is then sampled from this distribution, which is the jittered value. If a tuple is supplied for clip , then any values of the new column less than clip[0] will be set to clip[0] , and any values greater than clip[1] will be set to clip[1] . Additionally, if a numeric value is supplied for random_state , this value will be used to set the random seed used for sampling. NaN values are ignored in this method. This method mutates the original DataFrame. Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable Name of the column containing values to add Gaussian jitter to. required dest_column_name str The name of the new column containing the jittered values that will be created. required scale number A positive value multiplied by the original column value to determine the scale (standard deviation) of the Gaussian distribution to sample from. (A value of zero results in no jittering.) required clip Optional[Iterable[numpy.number]] An iterable of two values (minimum and maximum) to clip the jittered values to, default to None. None random_state Optional[numpy.number] An integer or 1-d array value used to set the random seed, default to None. None Returns: Type Description DataFrame A pandas DataFrame with a new column containing Gaussian-jittered values from another column. Exceptions: Type Description TypeError if column_name is not numeric. ValueError if scale is not a numerical value greater than 0 . ValueError if clip is not an iterable of length 2 . ValueError if clip[0] is not less than clip[1] . Source code in janitor/functions/jitter.py @pf.register_dataframe_method def jitter( df: pd.DataFrame, column_name: Hashable, dest_column_name: str, scale: np.number, clip: Optional[Iterable[np.number]] = None, random_state: Optional[np.number] = None, ) -> pd.DataFrame: \"\"\" Adds Gaussian noise (jitter) to the values of a column. Functional usage syntax: ```python import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.functions.jitter( df=df, column_name='values', dest_column_name='values_jitter', scale=1.0, clip=None, random_state=None, ) ``` Method chaining usage example: ``` import pandas as pd import janitor df = pd.DataFrame(...) df = df.jitter( column_name='values', dest_column_name='values_jitter', scale=1.0, clip=None, random_state=None, ) ``` A new column will be created containing the values of the original column with Gaussian noise added. For each value in the column, a Gaussian distribution is created having a location (mean) equal to the value and a scale (standard deviation) equal to `scale`. A random value is then sampled from this distribution, which is the jittered value. If a tuple is supplied for `clip`, then any values of the new column less than `clip[0]` will be set to `clip[0]`, and any values greater than `clip[1]` will be set to `clip[1]`. Additionally, if a numeric value is supplied for `random_state`, this value will be used to set the random seed used for sampling. NaN values are ignored in this method. This method mutates the original DataFrame. :param df: A pandas DataFrame. :param column_name: Name of the column containing values to add Gaussian jitter to. :param dest_column_name: The name of the new column containing the jittered values that will be created. :param scale: A positive value multiplied by the original column value to determine the scale (standard deviation) of the Gaussian distribution to sample from. (A value of zero results in no jittering.) :param clip: An iterable of two values (minimum and maximum) to clip the jittered values to, default to None. :param random_state: An integer or 1-d array value used to set the random seed, default to None. :returns: A pandas DataFrame with a new column containing Gaussian-jittered values from another column. :raises TypeError: if `column_name` is not numeric. :raises ValueError: if `scale` is not a numerical value greater than `0`. :raises ValueError: if `clip` is not an iterable of length `2`. :raises ValueError: if `clip[0]` is not less than `clip[1]`. \"\"\" # Check types check(\"scale\", scale, [int, float]) # Check that `column_name` is a numeric column if not np.issubdtype(df[column_name].dtype, np.number): raise TypeError(f\"{column_name} must be a numeric column.\") if scale <= 0: raise ValueError(\"`scale` must be a numeric value greater than 0.\") values = df[column_name] if random_state is not None: np.random.seed(random_state) result = np.random.normal(loc=values, scale=scale) if clip: # Ensure `clip` has length 2 if len(clip) != 2: raise ValueError(\"`clip` must be an iterable of length 2.\") # Ensure the values in `clip` are ordered as min, max if clip[1] < clip[0]: raise ValueError(\"`clip[0]` must be less than `clip[1]`.\") result = np.clip(result, *clip) df[dest_column_name] = result return df join_apply join_apply(df, func, new_column_name) Join the result of applying a function across dataframe rows. This method does not mutate the original DataFrame. This is a convenience function that allows us to apply arbitrary functions that take any combination of information from any of the columns. The only requirement is that the function signature takes in a row from the DataFrame. The example below shows us how to sum the result of two columns into a new column. df = ( pd.DataFrame({'a':[1, 2, 3], 'b': [2, 3, 4]}) .join_apply(lambda x: 2 * x['a'] + x['b'], new_column_name=\"2a+b\") ) This following example shows us how to use conditionals in the same function. def take_a_if_even(x): if x['a'] % 2: return x['a'] else: return x['b'] df = ( pd.DataFrame({'a': [1, 2, 3], 'b': [2, 3, 4]}) .join_apply(take_a_if_even, 'a_if_even') ) Parameters: Name Type Description Default df DataFrame A pandas DataFrame required func Callable A function that is applied elementwise across all rows of the DataFrame. required new_column_name str New column name. required Returns: Type Description DataFrame A pandas DataFrame with new column appended. Source code in janitor/functions/join_apply.py @pf.register_dataframe_method def join_apply( df: pd.DataFrame, func: Callable, new_column_name: str ) -> pd.DataFrame: \"\"\" Join the result of applying a function across dataframe rows. This method does not mutate the original DataFrame. This is a convenience function that allows us to apply arbitrary functions that take any combination of information from any of the columns. The only requirement is that the function signature takes in a row from the DataFrame. The example below shows us how to sum the result of two columns into a new column. ```python df = ( pd.DataFrame({'a':[1, 2, 3], 'b': [2, 3, 4]}) .join_apply(lambda x: 2 * x['a'] + x['b'], new_column_name=\"2a+b\") ) ``` This following example shows us how to use conditionals in the same function. ```python def take_a_if_even(x): if x['a'] % 2: return x['a'] else: return x['b'] df = ( pd.DataFrame({'a': [1, 2, 3], 'b': [2, 3, 4]}) .join_apply(take_a_if_even, 'a_if_even') ) ``` :param df: A pandas DataFrame :param func: A function that is applied elementwise across all rows of the DataFrame. :param new_column_name: New column name. :returns: A pandas DataFrame with new column appended. \"\"\" df = df.copy().join(df.apply(func, axis=1).rename(new_column_name)) return df label_encode label_encode(df, column_names) Convert labels into numerical data. This method will create a new column with the string _enc appended after the original column's name. Consider this to be syntactic sugar. This method behaves differently from encode_categorical . This method creates a new column of numeric data. encode_categorical replaces the dtype of the original column with a categorical dtype. This method mutates the original DataFrame. Functional usage syntax: df = label_encode(df, column_names=\"my_categorical_column\") # one way Method chaining syntax: import pandas as pd import janitor categorical_cols = ['col1', 'col2', 'col4'] df = pd.DataFrame(...).label_encode(column_names=categorical_cols) Parameters: Name Type Description Default df DataFrame The pandas DataFrame object. required column_names Union[str, Iterable[str], Hashable] A column name or an iterable (list or tuple) of column names. required Returns: Type Description DataFrame A pandas DataFrame. Source code in janitor/functions/label_encode.py @pf.register_dataframe_method @deprecated_alias(columns=\"column_names\") def label_encode( df: pd.DataFrame, column_names: Union[str, Iterable[str], Hashable] ) -> pd.DataFrame: \"\"\" Convert labels into numerical data. This method will create a new column with the string `_enc` appended after the original column's name. Consider this to be syntactic sugar. This method behaves differently from `encode_categorical`. This method creates a new column of numeric data. `encode_categorical` replaces the dtype of the original column with a *categorical* dtype. This method mutates the original DataFrame. Functional usage syntax: ```python df = label_encode(df, column_names=\"my_categorical_column\") # one way ``` Method chaining syntax: ```python import pandas as pd import janitor categorical_cols = ['col1', 'col2', 'col4'] df = pd.DataFrame(...).label_encode(column_names=categorical_cols) ``` :param df: The pandas DataFrame object. :param column_names: A column name or an iterable (list or tuple) of column names. :returns: A pandas DataFrame. \"\"\" warnings.warn( \"label_encode will be deprecated in a 1.x release. \\ Please use factorize_columns instead\" ) df = _factorize(df, column_names, \"_enc\") return df limit_column_characters limit_column_characters(df, column_length, col_separator='_') Truncate column sizes to a specific length. This method mutates the original DataFrame. Method chaining will truncate all columns to a given length and append a given separator character with the index of duplicate columns, except for the first distinct column name. Parameters: Name Type Description Default df DataFrame A pandas dataframe. required column_length int Character length for which to truncate all columns. The column separator value and number for duplicate column name does not contribute. Therefore, if all columns are truncated to 10 characters, the first distinct column will be 10 characters and the remaining will be 12 characters (assuming a column separator of one character). required col_separator str The separator to use for counting distinct column values. I think an underscore looks nicest, however a period is a common option as well. Supply an empty string (i.e. '') to remove the separator. '_' Returns: Type Description DataFrame A pandas DataFrame with truncated column lengths. Source code in janitor/functions/limit_column_characters.py @pf.register_dataframe_method def limit_column_characters( df: pd.DataFrame, column_length: int, col_separator: str = \"_\" ) -> pd.DataFrame: \"\"\"Truncate column sizes to a specific length. This method mutates the original DataFrame. Method chaining will truncate all columns to a given length and append a given separator character with the index of duplicate columns, except for the first distinct column name. :param df: A pandas dataframe. :param column_length: Character length for which to truncate all columns. The column separator value and number for duplicate column name does not contribute. Therefore, if all columns are truncated to 10 characters, the first distinct column will be 10 characters and the remaining will be 12 characters (assuming a column separator of one character). :param col_separator: The separator to use for counting distinct column values. I think an underscore looks nicest, however a period is a common option as well. Supply an empty string (i.e. '') to remove the separator. :returns: A pandas DataFrame with truncated column lengths. \"\"\" # :Example Setup: # ```python # import pandas as pd # import janitor # data_dict = { # \"really_long_name_for_a_column\": range(10), # \"another_really_long_name_for_a_column\": \\ # [2 * item for item in range(10)], # \"another_really_longer_name_for_a_column\": list(\"lllongname\"), # \"this_is_getting_out_of_hand\": list(\"longername\"), # } # :Example: Standard truncation: # ```python # example_dataframe = pd.DataFrame(data_dict) # example_dataframe.limit_column_characters(7) # :Output: # ```python # really_ another another_1 this_is # 0 0 0 l l # 1 1 2 l o # 2 2 4 l n # 3 3 6 o g # 4 4 8 n e # 5 5 10 g r # 6 6 12 n n # 7 7 14 a a # 8 8 16 m m # 9 9 18 e e # :Example: Standard truncation with different separator character: # ```python # example_dataframe2 = pd.DataFrame(data_dict) # example_dataframe2.limit_column_characters(7, \".\") # ```python # really_ another another.1 this_is # 0 0 0 l l # 1 1 2 l o # 2 2 4 l n # 3 3 6 o g # 4 4 8 n e # 5 5 10 g r # 6 6 12 n n # 7 7 14 a a # 8 8 16 m m # 9 9 18 e e check(\"column_length\", column_length, [int]) check(\"col_separator\", col_separator, [str]) col_names = df.columns col_names = [col_name[:column_length] for col_name in col_names] col_name_set = set(col_names) col_name_count = {} # If no columns are duplicates, we can skip the loops below. if len(col_name_set) == len(col_names): df.columns = col_names return df for col_name_to_check in col_name_set: count = 0 for idx, col_name in enumerate(col_names): if col_name_to_check == col_name: col_name_count[idx] = count count += 1 final_col_names = [] for idx, col_name in enumerate(col_names): if col_name_count[idx] > 0: col_name_to_append = ( col_name + col_separator + str(col_name_count[idx]) ) final_col_names.append(col_name_to_append) else: final_col_names.append(col_name) df.columns = final_col_names return df min_max_scale min_max_scale(df, old_min=None, old_max=None, column_name=None, new_min=0, new_max=1) Scales data to between a minimum and maximum value. This method mutates the original DataFrame. If minimum and maximum are provided, the true min/max of the DataFrame or column is ignored in the scaling process and replaced with these values, instead. One can optionally set a new target minimum and maximum value using the new_min and new_max keyword arguments. This will result in the transformed data being bounded between new_min and new_max . If a particular column name is specified, then only that column of data are scaled. Otherwise, the entire dataframe is scaled. Method chaining syntax: df = pd.DataFrame(...).min_max_scale(column_name=\"a\") Setting custom minimum and maximum: df = ( pd.DataFrame(...) .min_max_scale( column_name=\"a\", new_min=2, new_max=10 ) ) Setting a min and max that is not based on the data, while applying to entire dataframe: df = ( pd.DataFrame(...) .min_max_scale( old_min=0, old_max=14, new_min=0, new_max=1, ) ) The aforementioned example might be applied to something like scaling the isoelectric points of amino acids. While technically they range from approx 3-10, we can also think of them on the pH scale which ranges from 1 to 14. Hence, 3 gets scaled not to 0 but approx. 0.15 instead, while 10 gets scaled to approx. 0.69 instead. Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required old_min (optional) Overrides for the current minimum value of the data to be transformed. None old_max (optional) Overrides for the current maximum value of the data to be transformed. None new_min (optional) The minimum value of the data after it has been scaled. 0 new_max (optional) The maximum value of the data after it has been scaled. 1 column_name (optional) The column on which to perform scaling. None Returns: Type Description DataFrame A pandas DataFrame with scaled data. Exceptions: Type Description ValueError if old_max is not greater than `old_min``. ValueError if new_max is not greater than `new_min``. Source code in janitor/functions/min_max_scale.py @pf.register_dataframe_method @deprecated_alias(col_name=\"column_name\") def min_max_scale( df: pd.DataFrame, old_min=None, old_max=None, column_name=None, new_min=0, new_max=1, ) -> pd.DataFrame: \"\"\" Scales data to between a minimum and maximum value. This method mutates the original DataFrame. If `minimum` and `maximum` are provided, the true min/max of the `DataFrame` or column is ignored in the scaling process and replaced with these values, instead. One can optionally set a new target minimum and maximum value using the `new_min` and `new_max` keyword arguments. This will result in the transformed data being bounded between `new_min` and `new_max`. If a particular column name is specified, then only that column of data are scaled. Otherwise, the entire dataframe is scaled. Method chaining syntax: ```python df = pd.DataFrame(...).min_max_scale(column_name=\"a\") ``` Setting custom minimum and maximum: ```python df = ( pd.DataFrame(...) .min_max_scale( column_name=\"a\", new_min=2, new_max=10 ) ) ``` Setting a min and max that is not based on the data, while applying to entire dataframe: ```python df = ( pd.DataFrame(...) .min_max_scale( old_min=0, old_max=14, new_min=0, new_max=1, ) ) ``` The aforementioned example might be applied to something like scaling the isoelectric points of amino acids. While technically they range from approx 3-10, we can also think of them on the pH scale which ranges from 1 to 14. Hence, 3 gets scaled not to 0 but approx. 0.15 instead, while 10 gets scaled to approx. 0.69 instead. :param df: A pandas DataFrame. :param old_min: (optional) Overrides for the current minimum value of the data to be transformed. :param old_max: (optional) Overrides for the current maximum value of the data to be transformed. :param new_min: (optional) The minimum value of the data after it has been scaled. :param new_max: (optional) The maximum value of the data after it has been scaled. :param column_name: (optional) The column on which to perform scaling. :returns: A pandas DataFrame with scaled data. :raises ValueError: if `old_max` is not greater than `old_min``. :raises ValueError: if `new_max` is not greater than `new_min``. \"\"\" if ( (old_min is not None) and (old_max is not None) and (old_max <= old_min) ): raise ValueError(\"`old_max` should be greater than `old_min`\") if new_max <= new_min: raise ValueError(\"`new_max` should be greater than `new_min`\") new_range = new_max - new_min if column_name: if old_min is None: old_min = df[column_name].min() if old_max is None: old_max = df[column_name].max() old_range = old_max - old_min df[column_name] = ( df[column_name] - old_min ) * new_range / old_range + new_min else: if old_min is None: old_min = df.min().min() if old_max is None: old_max = df.max().max() old_range = old_max - old_min df = (df - old_min) * new_range / old_range + new_min return df move move(df, source, target, position='before', axis=0) Move column or row to a position adjacent to another column or row in dataframe. Must have unique column names or indices. This operation does not reset the index of the dataframe. User must explicitly do so. Does not apply to multilevel dataframes. Functional usage syntax: df = move(df, source=3, target=15, position='after', axis=0) Method chaining syntax: import pandas as pd import janitor df = ( pd.DataFrame(...) .move(source=3, target=15, position='after', axis=0) ) Parameters: Name Type Description Default df DataFrame The pandas Dataframe object. required source Union[int, str] column or row to move required target Union[int, str] column or row to move adjacent to required position str Specifies whether the Series is moved to before or after the adjacent Series. Values can be either before or after ; defaults to before . 'before' axis int Axis along which the function is applied. 0 to move a row, 1 to move a column. 0 Returns: Type Description DataFrame The dataframe with the Series moved. Exceptions: Type Description ValueError if axis is not 0 or `1``. ValueError if position is not before or `after``. ValueError if source row or column is not in dataframe. ValueError if target row or column is not in dataframe. Source code in janitor/functions/move.py @pf.register_dataframe_method def move( df: pd.DataFrame, source: Union[int, str], target: Union[int, str], position: str = \"before\", axis: int = 0, ) -> pd.DataFrame: \"\"\" Move column or row to a position adjacent to another column or row in dataframe. Must have unique column names or indices. This operation does not reset the index of the dataframe. User must explicitly do so. Does not apply to multilevel dataframes. Functional usage syntax: ```python df = move(df, source=3, target=15, position='after', axis=0) ``` Method chaining syntax: ```python import pandas as pd import janitor df = ( pd.DataFrame(...) .move(source=3, target=15, position='after', axis=0) ) ``` :param df: The pandas Dataframe object. :param source: column or row to move :param target: column or row to move adjacent to :param position: Specifies whether the Series is moved to before or after the adjacent Series. Values can be either `before` or `after`; defaults to `before`. :param axis: Axis along which the function is applied. 0 to move a row, 1 to move a column. :returns: The dataframe with the Series moved. :raises ValueError: if `axis` is not `0` or `1``. :raises ValueError: if `position` is not `before` or `after``. :raises ValueError: if `source` row or column is not in dataframe. :raises ValueError: if `target` row or column is not in dataframe. \"\"\" df = df.copy() if axis not in [0, 1]: raise ValueError(f\"Invalid axis '{axis}'. Can only be 0 or 1.\") if position not in [\"before\", \"after\"]: raise ValueError( f\"Invalid position '{position}'. Can only be 'before' or 'after'.\" ) if axis == 0: names = list(df.index) if source not in names: raise ValueError(f\"Source row '{source}' not in dataframe.\") if target not in names: raise ValueError(f\"Target row '{target}' not in dataframe.\") names.remove(source) pos = names.index(target) if position == \"after\": pos += 1 names.insert(pos, source) df = df.loc[names, :] else: names = list(df.columns) if source not in names: raise ValueError(f\"Source column '{source}' not in dataframe.\") if target not in names: raise ValueError(f\"Target column '{target}' not in dataframe.\") names.remove(source) pos = names.index(target) if position == \"after\": pos += 1 names.insert(pos, source) df = df.loc[:, names] return df pivot pivot_longer(df, index=None, column_names=None, names_to=None, values_to='value', column_level=None, names_sep=None, names_pattern=None, sort_by_appearance=False, ignore_index=True) Unpivots a DataFrame from wide to long format. This method does not mutate the original DataFrame. It is a wrapper around pd.melt and is meant to serve as a single point for transformations that require pd.melt or pd.wide_to_long . It is modeled after the pivot_longer function in R's tidyr package, and offers more functionality and flexibility than pd.wide_to_long . This function is useful to massage a DataFrame into a format where one or more columns are considered measured variables, and all other columns are considered as identifier variables. All measured variables are unpivoted (and typically duplicated) along the row axis. Example 1: The following DataFrame contains heartrate data for patients treated with two different drugs, a and b . name a b 0 Wilbur 67 56 1 Petunia 80 90 2 Gregory 64 50 The column names a and b are actually the names of a measured variable (i.e. the name of a drug), but the values are a different measured variable (heartrate). We would like to unpivot these a and b columns into a drug column and a heartrate column. df.pivot_longer( column_names = ['a', 'b'], names_to = 'drug', values_to = 'heartrate', sort_by_appearance = True ) name drug heartrate 0 Wilbur a 67 1 Wilbur b 56 2 Petunia a 80 3 Petunia b 90 4 Gregory a 64 5 Gregory b 50 Note how the data is stacked in order of first appearance. If, however, you do not care for order of appearance, and want to wring out some more performance, you can set sort_by_appearance to False (the default is False ). df.pivot_longer( column_names = ['a', 'b'], names_to = 'drug', values_to = 'heartrate', sort_by_appearance = False ) name drug heartrate 0 Wilbur a 67 1 Petunia a 80 2 Gregory a 64 3 Wilbur b 56 4 Petunia b 90 5 Gregory b 50 You can set ignore_index to False , if you wish to reuse the index from the source DataFrame (the index will be repeated as many times as necessary): df.pivot_longer( column_names = ['a', 'b'], names_to = 'drug', values_to = 'heartrate', sort_by_appearance = False, ignore_index = False ) name drug heartrate 0 Wilbur a 67 1 Petunia a 80 2 Gregory a 64 0 Wilbur b 56 1 Petunia b 90 2 Gregory b 50 MultiIndex DataFrames are unpivoted in the same form that you would expect from pandas' melt : A B C D E F 0 a 1 2 1 b 3 4 2 c 5 6 df.pivot_longer( index = [(\"A\", \"D\")], names_to = [\"first\", \"second\"] ) (A, D) first second value 0 a B E 1 1 b B E 3 2 c B E 5 3 a C F 2 4 b C F 4 5 c C F 6 You can also unpivot on a specific level: df.pivot_longer( index = \"A\", names_to = \"first\", column_level = 0 ) A first value 0 a B 1 1 b B 3 2 c B 5 Example 2: The DataFrame below has year and month variables embedded within the column names. col1 2019-12 2020-01 2020-02 0 a -1.085631 -1.506295 -2.426679 1 b 0.997345 -0.578600 -0.428913 2 c 0.282978 1.651437 1.265936 pivot_longer can conveniently reshape the DataFrame into long format, with new columns for the year and month. You simply pass in the new column names to names_to , and pass the hyphen - to the names_sep argument. df.pivot_longer( index = 'col1', names_to = ('year', 'month'), names_sep = '-', sort_by_appearance = True ) col1 year month value 0 a 2019 12 -1.085631 1 a 2020 01 -1.506295 2 a 2020 02 -2.426679 3 b 2019 12 0.997345 4 b 2020 01 -0.578600 5 b 2020 02 -0.428913 6 c 2019 12 0.282978 7 c 2020 01 1.651437 8 c 2020 02 1.265936 Example 3: The DataFrame below has names embedded in it (measure1, measure2) that we would love to reuse as column names. treat1-measure1 treat1-measure2 treat2-measure1 treat2-measure2 0 1 4 2 5 1 2 5 3 4 For this, we use the .value variable, which signals to pivot_longer to treat the part of the column names corresponding to .value as new column names. The .value variable is similar to stubnames in pandas' wide_to_long function, but with more flexibility. df.pivot_longer( names_to = (\"group\", '.value'), names_sep = '-', sort_by_appearance = True ) group measure1 measure2 0 treat1 1 4 1 treat2 2 5 2 treat1 2 5 3 treat2 3 4 Let's break down the .value idea. When .value is used, pivot_longer creates a pairing. In the example above, we get a pairing {\"group\":[\"treat1\", \"treat2\"], \".value\":[\"measure1\", \"measure2\"]} . All the values associated with .value become new column names, while those not associated with .value ( treat1 and treat2 ) become values in a new column group . values_to is overridden during this process. Note The values not associated with .value (in the example above, this is the group column) are returned as object dtypes. You can change it to your preferred dtype using pandas' astype method. Example 4: You can also unpivot from wide to long using regular expressions n_1 n_2 n_3 pct_1 pct_2 pct_3 0 10 20 30 0.1 0.2 0.3 df.pivot_longer( names_to = (\".value\", \"name\"), names_pattern = \"(.*)_(.)\" ) name n pct 0 1 10.0 0.1 1 2 20.0 0.2 2 3 30.0 0.3 The same idea of .value works here as well. Based on the capturing groups in the regex in names_pattern , we have two pairings --> {\".value\":[\"n\", \"pct\"], \"name\":[1,2,3]} . Just like in the previous example, the values associated with .value become new column names, while those not associated with .value become values in the new column name . Note There are no limits to the pairing; however, you can only have one .value in names_to . Example 5: You can also pass a list/tuple of regular expressions that match specific patterns to names_pattern , along with a list/tuple of new names to names_to ; this can come in handy if .value falls short: GameID Date Visitor Score_V Home Score_H 0 1 9/10/2020 Houston Texans 20 Kansas City Chiefs 34 1 2 9/13/2020 Seattle Seahawks 38 Atlanta Falcons 25 df.pivot_longer( index = ['GameID','Date'], names_to = (\"Team\",\"Score\"), names_pattern = (\"^Visitor|Home\", \"^Score\") ) GameID Date Team Score 0 1 9/10/2020 Houston Texans 20 1 2 9/13/2020 Seattle Seahawks 38 2 1 9/10/2020 Kansas City Chiefs 34 3 2 9/13/2020 Atlanta Falcons 25 Note that in the code above, the number of entries in both names_to and names_pattern must match. Essentially, what the code does is look for columns that start with Visitor or Home (using the regex supplied) and puts all the values associated with these columns under a new column name Team . It then looks for columns that start with Score and collate all the values associated with these columns to a single column named Score . You can also take advantage of janitor.patterns function, or the select_columns syntax, which allows selection of columns via a regular expression; this can come in handy if you have a lot of column names to pass to the index or column_names parameters, and you do not wish to manually type them all. name wk1 wk2 wk3 wk4 0 Alice 5 9 20 22 1 Bob 7 11 17 33 2 Carla 6 13 39 40 df.pivot_longer(index = janitor.patterns(\"^(?!wk)\")) name variable value 0 Alice wk1 5 1 Bob wk1 7 2 Carla wk1 6 3 Alice wk2 9 4 Bob wk2 11 5 Carla wk2 13 6 Alice wk3 20 7 Bob wk3 17 8 Carla wk3 39 9 Alice wk4 22 10 Bob wk4 33 11 Carla wk4 40 Note Unpivoting a DataFrame with MultiIndex columns, when either names_sep or names_pattern is provided is not supported. Functional usage syntax: import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.pivot_longer( df = df, index = [column1, column2, ...], column_names = [column3, column4, ...], names_to = new_column_name, names_sep = string/regular expression, names_pattern = string/regular expression, values_to= new_column_name, column_level = None/int/str, sort_by_appearance = True/False, ignore_index = True/False, ) Method chaining syntax: df = ( pd.DataFrame(...) .pivot_longer( index = [column1, column2, ...], column_names = [column3, column4, ...], names_to = new_column_name, names_sep = string/regular expression, names_pattern = string/regular expression, values_to = new_column_name, column_level = None/int/str, sort_by_appearance = True/False, ignore_index = True/False, ) ) Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required index Union[List, Tuple, str, Pattern] Name(s) of columns to use as identifier variables. Should be either a single column name, or a list/tuple of column names. The janitor.select_columns syntax is supported here, allowing for flexible and dynamic column selection. Index should be a list of tuples if the columns are a MultiIndex. None column_names Union[List, Tuple, str, Pattern] Name(s) of columns to unpivot. Should be either a single column name or a list/tuple of column names. The janitor.select_columns syntax is supported here, allowing for flexible and dynamic column selection. Column_names should be a list of tuples if the columns are a MultiIndex. None names_to Union[List, Tuple, str] Name of new column as a string that will contain what were previously the column names in column_names . The default is variable if no value is provided. It can also be a list/tuple of strings that will serve as new column names, if name_sep or names_pattern is provided. If .value is in names_to , new column names will be extracted from part of the existing column names and overrides values_to . None names_sep Union[str, Pattern] Determines how the column name is broken up, if names_to contains multiple values. It takes the same specification as pandas' str.split method, and can be a string or regular expression. names_sep does not work with MultiIndex columns. None names_pattern Union[List, Tuple, str, Pattern] Determines how the column name is broken up. It can be a regular expression containing matching groups (it takes the same specification as pandas' str.extract method), or a list/tuple of regular expressions. If it is a single regex, the number of groups must match the length of names_to ( if the length of names_to is 3, then the number of groups must be 3. If names_to is a string, then there should be only one group in names_pattern``). For a list/tuple of regular expressions, names_to must also be a list/tuple and the lengths of both arguments must match(if the length of names_to is 4, then the length of names_pattern must also be 4). The entries in both arguments must also match positionally, i.e if names_to = (\"name1\", \"name2\", \"name3\") , then `names_pattern should be (\"regex1\", \"regex2\", \"regex3\"), with \"name1\" pairing \"regex1\", \"name2\" pairing \"regex2\", and \"name3\" pairing \"regex3\". names_pattern does not work with MultiIndex columns. None values_to Optional[str] Name of new column as a string that will contain what were previously the values of the columns in column_names . 'value' column_level Union[int, str] If columns are a MultiIndex, then use this level to unpivot the DataFrame. Provided for compatibility with pandas' melt, and applies only if neither names_sep nor names_pattern is provided. None sort_by_appearance Optional[bool] Default False . Boolean value that determines the final look of the DataFrame. If True , the unpivoted DataFrame will be stacked in order of first appearance. See examples for more details. pivot_longer is usually more performant if sort_by_appearance is `False``. False ignore_index Optional[bool] Default True . If True, original index is ignored. If False, the original index is retained and the Index labels will be repeated as necessary. True Returns: Type Description DataFrame A pandas DataFrame that has been unpivoted from wide to long format. Source code in janitor/functions/pivot.py @pf.register_dataframe_method def pivot_longer( df: pd.DataFrame, index: Optional[Union[List, Tuple, str, Pattern]] = None, column_names: Optional[Union[List, Tuple, str, Pattern]] = None, names_to: Optional[Union[List, Tuple, str]] = None, values_to: Optional[str] = \"value\", column_level: Optional[Union[int, str]] = None, names_sep: Optional[Union[str, Pattern]] = None, names_pattern: Optional[Union[List, Tuple, str, Pattern]] = None, sort_by_appearance: Optional[bool] = False, ignore_index: Optional[bool] = True, ) -> pd.DataFrame: \"\"\" Unpivots a DataFrame from *wide* to *long* format. This method does not mutate the original DataFrame. It is a wrapper around `pd.melt` and is meant to serve as a single point for transformations that require `pd.melt` or `pd.wide_to_long`. It is modeled after the `pivot_longer` function in R's tidyr package, and offers more functionality and flexibility than `pd.wide_to_long`. This function is useful to massage a DataFrame into a format where one or more columns are considered measured variables, and all other columns are considered as identifier variables. All measured variables are *unpivoted* (and typically duplicated) along the row axis. Example 1: The following DataFrame contains heartrate data for patients treated with two different drugs, `a` and `b`. ```python name a b 0 Wilbur 67 56 1 Petunia 80 90 2 Gregory 64 50 ``` The column names `a` and `b` are actually the names of a measured variable (i.e. the name of a drug), but the values are a different measured variable (heartrate). We would like to unpivot these `a` and `b` columns into a `drug` column and a `heartrate` column. ```python df.pivot_longer( column_names = ['a', 'b'], names_to = 'drug', values_to = 'heartrate', sort_by_appearance = True ) name drug heartrate 0 Wilbur a 67 1 Wilbur b 56 2 Petunia a 80 3 Petunia b 90 4 Gregory a 64 5 Gregory b 50 ``` Note how the data is stacked in order of first appearance. If, however, you do not care for order of appearance, and want to wring out some more performance, you can set `sort_by_appearance` to `False` (the default is `False`). ```python df.pivot_longer( column_names = ['a', 'b'], names_to = 'drug', values_to = 'heartrate', sort_by_appearance = False ) name drug heartrate 0 Wilbur a 67 1 Petunia a 80 2 Gregory a 64 3 Wilbur b 56 4 Petunia b 90 5 Gregory b 50 ``` You can set `ignore_index` to `False`, if you wish to reuse the index from the source DataFrame (the index will be repeated as many times as necessary): ```python df.pivot_longer( column_names = ['a', 'b'], names_to = 'drug', values_to = 'heartrate', sort_by_appearance = False, ignore_index = False ) name drug heartrate 0 Wilbur a 67 1 Petunia a 80 2 Gregory a 64 0 Wilbur b 56 1 Petunia b 90 2 Gregory b 50 ``` MultiIndex DataFrames are unpivoted in the same form that you would expect from pandas' `melt`: ```python A B C D E F 0 a 1 2 1 b 3 4 2 c 5 6 df.pivot_longer( index = [(\"A\", \"D\")], names_to = [\"first\", \"second\"] ) (A, D) first second value 0 a B E 1 1 b B E 3 2 c B E 5 3 a C F 2 4 b C F 4 5 c C F 6 ``` You can also unpivot on a specific level: ```python df.pivot_longer( index = \"A\", names_to = \"first\", column_level = 0 ) A first value 0 a B 1 1 b B 3 2 c B 5 ``` Example 2: The DataFrame below has year and month variables embedded within the column names. ```python col1 2019-12 2020-01 2020-02 0 a -1.085631 -1.506295 -2.426679 1 b 0.997345 -0.578600 -0.428913 2 c 0.282978 1.651437 1.265936 ``` `pivot_longer` can conveniently reshape the DataFrame into long format, with new columns for the year and month. You simply pass in the new column names to `names_to`, and pass the hyphen `-` to the `names_sep` argument. ```python df.pivot_longer( index = 'col1', names_to = ('year', 'month'), names_sep = '-', sort_by_appearance = True ) col1 year month value 0 a 2019 12 -1.085631 1 a 2020 01 -1.506295 2 a 2020 02 -2.426679 3 b 2019 12 0.997345 4 b 2020 01 -0.578600 5 b 2020 02 -0.428913 6 c 2019 12 0.282978 7 c 2020 01 1.651437 8 c 2020 02 1.265936 ``` Example 3: The DataFrame below has names embedded in it `(measure1, measure2)` that we would love to reuse as column names. ```python treat1-measure1 treat1-measure2 treat2-measure1 treat2-measure2 0 1 4 2 5 1 2 5 3 4 ``` For this, we use the `.value` variable, which signals to `pivot_longer` to treat the part of the column names corresponding to `.value` as new column names. The `.value` variable is similar to `stubnames` in pandas' `wide_to_long` function, but with more flexibility. ```python df.pivot_longer( names_to = (\"group\", '.value'), names_sep = '-', sort_by_appearance = True ) group measure1 measure2 0 treat1 1 4 1 treat2 2 5 2 treat1 2 5 3 treat2 3 4 ``` Let's break down the `.value` idea. When `.value` is used, `pivot_longer` creates a pairing. In the example above, we get a pairing `{\"group\":[\"treat1\", \"treat2\"], \".value\":[\"measure1\", \"measure2\"]}`. All the values associated with `.value` become new column names, while those not associated with `.value`(`treat1` and `treat2`) become values in a new column `group`. `values_to` is overridden during this process. !!! note The values not associated with `.value` (in the example above, this is the `group` column) are returned as object dtypes. You can change it to your preferred dtype using pandas' `astype` method. Example 4: You can also unpivot from wide to long using regular expressions ```python n_1 n_2 n_3 pct_1 pct_2 pct_3 0 10 20 30 0.1 0.2 0.3 df.pivot_longer( names_to = (\".value\", \"name\"), names_pattern = \"(.*)_(.)\" ) name n pct 0 1 10.0 0.1 1 2 20.0 0.2 2 3 30.0 0.3 ``` The same idea of `.value` works here as well. Based on the capturing groups in the regex in `names_pattern`, we have two pairings --> `{\".value\":[\"n\", \"pct\"], \"name\":[1,2,3]}`. Just like in the previous example, the values associated with `.value` become new column names, while those not associated with `.value` become values in the new column `name`. !!!note There are no limits to the pairing; however, you can only have one `.value` in `names_to`. Example 5: You can also pass a list/tuple of regular expressions that match specific patterns to `names_pattern`, along with a list/tuple of new names to `names_to`; this can come in handy if `.value` falls short: ```python GameID Date Visitor Score_V Home Score_H 0 1 9/10/2020 Houston Texans 20 Kansas City Chiefs 34 1 2 9/13/2020 Seattle Seahawks 38 Atlanta Falcons 25 df.pivot_longer( index = ['GameID','Date'], names_to = (\"Team\",\"Score\"), names_pattern = (\"^Visitor|Home\", \"^Score\") ) GameID Date Team Score 0 1 9/10/2020 Houston Texans 20 1 2 9/13/2020 Seattle Seahawks 38 2 1 9/10/2020 Kansas City Chiefs 34 3 2 9/13/2020 Atlanta Falcons 25 ``` Note that in the code above, the number of entries in both `names_to` and `names_pattern` must match. Essentially, what the code does is look for columns that start with `Visitor` or `Home` (using the regex supplied) and puts all the values associated with these columns under a new column name `Team`. It then looks for columns that start with `Score` and collate all the values associated with these columns to a single column named `Score`. You can also take advantage of `janitor.patterns` function, or the `select_columns` syntax, which allows selection of columns via a regular expression; this can come in handy if you have a lot of column names to pass to the `index` or `column_names` parameters, and you do not wish to manually type them all. ```python name wk1 wk2 wk3 wk4 0 Alice 5 9 20 22 1 Bob 7 11 17 33 2 Carla 6 13 39 40 df.pivot_longer(index = janitor.patterns(\"^(?!wk)\")) name variable value 0 Alice wk1 5 1 Bob wk1 7 2 Carla wk1 6 3 Alice wk2 9 4 Bob wk2 11 5 Carla wk2 13 6 Alice wk3 20 7 Bob wk3 17 8 Carla wk3 39 9 Alice wk4 22 10 Bob wk4 33 11 Carla wk4 40 ``` !!!note Unpivoting a DataFrame with MultiIndex columns, when either `names_sep` or `names_pattern` is provided is not supported. Functional usage syntax: ```python import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.pivot_longer( df = df, index = [column1, column2, ...], column_names = [column3, column4, ...], names_to = new_column_name, names_sep = string/regular expression, names_pattern = string/regular expression, values_to= new_column_name, column_level = None/int/str, sort_by_appearance = True/False, ignore_index = True/False, ) ``` Method chaining syntax: ```python df = ( pd.DataFrame(...) .pivot_longer( index = [column1, column2, ...], column_names = [column3, column4, ...], names_to = new_column_name, names_sep = string/regular expression, names_pattern = string/regular expression, values_to = new_column_name, column_level = None/int/str, sort_by_appearance = True/False, ignore_index = True/False, ) ) ``` :param df: A pandas DataFrame. :param index: Name(s) of columns to use as identifier variables. Should be either a single column name, or a list/tuple of column names. The `janitor.select_columns` syntax is supported here, allowing for flexible and dynamic column selection. Index should be a list of tuples if the columns are a MultiIndex. :param column_names: Name(s) of columns to unpivot. Should be either a single column name or a list/tuple of column names. The `janitor.select_columns` syntax is supported here, allowing for flexible and dynamic column selection. Column_names should be a list of tuples if the columns are a MultiIndex. :param names_to: Name of new column as a string that will contain what were previously the column names in `column_names`. The default is `variable` if no value is provided. It can also be a list/tuple of strings that will serve as new column names, if `name_sep` or `names_pattern` is provided. If `.value` is in `names_to`, new column names will be extracted from part of the existing column names and overrides`values_to`. :param names_sep: Determines how the column name is broken up, if `names_to` contains multiple values. It takes the same specification as pandas' `str.split` method, and can be a string or regular expression. `names_sep` does not work with MultiIndex columns. :param names_pattern: Determines how the column name is broken up. It can be a regular expression containing matching groups (it takes the same specification as pandas' `str.extract` method), or a list/tuple of regular expressions. If it is a single regex, the number of groups must match the length of `names_to` ( if the length of `names_to` is 3, then the number of groups must be 3. If `names_to` is a string, then there should be only one group in `names_pattern``). For a list/tuple of regular expressions, `names_to` must also be a list/tuple and the lengths of both arguments must match(if the length of `names_to` is 4, then the length of `names_pattern` must also be 4). The entries in both arguments must also match positionally, i.e if `names_to = (\"name1\", \"name2\", \"name3\")``, then `names_pattern`` should be (\"regex1\", \"regex2\", \"regex3\"), with \"name1\" pairing \"regex1\", \"name2\" pairing \"regex2\", and \"name3\" pairing \"regex3\". `names_pattern` does not work with MultiIndex columns. :param values_to: Name of new column as a string that will contain what were previously the values of the columns in `column_names`. :param column_level: If columns are a MultiIndex, then use this level to unpivot the DataFrame. Provided for compatibility with pandas' melt, and applies only if neither `names_sep` nor `names_pattern` is provided. :param sort_by_appearance: Default `False`. Boolean value that determines the final look of the DataFrame. If `True`, the unpivoted DataFrame will be stacked in order of first appearance. See examples for more details. `pivot_longer` is usually more performant if `sort_by_appearance` is `False``. :param ignore_index: Default `True`. If True, original index is ignored. If False, the original index is retained and the Index labels will be repeated as necessary. :returns: A pandas DataFrame that has been unpivoted from wide to long format. \"\"\" # this code builds on the wonderful work of @benjaminjack\u2019s PR # https://github.com/benjaminjack/pyjanitor/commit/e3df817903c20dd21634461c8a92aec137963ed0 df = df.copy() ( df, index, column_names, names_to, values_to, column_level, names_sep, names_pattern, sort_by_appearance, ignore_index, ) = _data_checks_pivot_longer( df, index, column_names, names_to, values_to, column_level, names_sep, names_pattern, sort_by_appearance, ignore_index, ) return _computations_pivot_longer( df, index, column_names, names_to, values_to, column_level, names_sep, names_pattern, sort_by_appearance, ignore_index, ) pivot_wider(df, index=None, names_from=None, values_from=None, levels_order=None, flatten_levels=True, names_sep='_', names_glue=None) Reshapes data from 'long' to 'wide' form. The number of columns are increased, while decreasing the number of rows. It is the inverse of the pivot_longer method, and is a wrapper around pd.DataFrame.pivot method. This method does not mutate the original DataFrame. .. note:: Column selection in index , names_from and values_from is possible using the janitor.select_columns syntax. .. note:: A ValueError is raised if the combination of the index and names_from is not unique. .. note:: By default, values from values_from are always at the top level if the columns are not flattened. If flattened, the values from values_from are usually at the start of each label in the columns. Functional usage syntax: import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.pivot_wider( df = df, index = [column1, column2, ...], names_from = [column3, column4, ...], value_from = [column5, column6, ...], levels_order = None/list, flatten_levels = True/False, names_sep='_', names_glue= Callable ) Method chaining syntax: df = ( pd.DataFrame(...) .pivot_wider( index = [column1, column2, ...], names_from = [column3, column4, ...], value_from = [column5, column6, ...], levels_order = None/list, flatten_levels = True/False, names_sep='_', names_glue= Callable ) ) Parameters: Name Type Description Default df DataFrame A pandas dataframe. required index Union[List, str] Name(s) of columns to use as identifier variables. Should be either a single column name, or a list of column names. The janitor.select_columns syntax is supported here, allowing for flexible and dynamic column selection. If index is not provided, the dataframe's index is used. None names_from Union[List, str] Name(s) of column(s) to use to make the new dataframe's columns. Should be either a single column name, or a list of column names. The janitor.select_columns syntax is supported here, allowing for flexible and dynamic column selection. A label or labels must be provided for names_from . None values_from Union[List, str] Name(s) of column(s) that will be used for populating the new dataframe's values. The janitor.select_columns syntax is supported here, allowing for flexible and dynamic column selection. If values_from is not specified, all remaining columns will be used. Note that values from values_from are usually at the top level, the dataframe's columns is not flattened, or the start of each label in the columns, if flattened. None levels_order Optional[list] Applicable if there are multiple names_from and/or values_from . Reorders the levels. Accepts a list of strings. If there are multiple values_from , pass a None to represent that level. None flatten_levels Optional[bool] Default is True . If False , the dataframe stays as a MultiIndex. True names_sep If names_from or values_from contain multiple variables, this will be used to join their values into a single string to use as a column name. Default is _ . Applicable only if flatten_levels is True . '_' names_glue Callable A callable to control the output of the flattened columns. Applicable only if flatten_levels is True. Function should be acceptable to pandas\u2019 map function. None Returns: Type Description DataFrame A pandas DataFrame that has been unpivoted from long to wide form. Source code in janitor/functions/pivot.py @pf.register_dataframe_method def pivot_wider( df: pd.DataFrame, index: Optional[Union[List, str]] = None, names_from: Optional[Union[List, str]] = None, values_from: Optional[Union[List, str]] = None, levels_order: Optional[list] = None, flatten_levels: Optional[bool] = True, names_sep=\"_\", names_glue: Callable = None, ) -> pd.DataFrame: \"\"\" Reshapes data from 'long' to 'wide' form. The number of columns are increased, while decreasing the number of rows. It is the inverse of the `pivot_longer` method, and is a wrapper around `pd.DataFrame.pivot` method. This method does not mutate the original DataFrame. .. note:: Column selection in `index`, `names_from` and `values_from` is possible using the `janitor.select_columns` syntax. .. note:: A ValueError is raised if the combination of the `index` and `names_from` is not unique. .. note:: By default, values from `values_from` are always at the top level if the columns are not flattened. If flattened, the values from `values_from` are usually at the start of each label in the columns. Functional usage syntax: ```python import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.pivot_wider( df = df, index = [column1, column2, ...], names_from = [column3, column4, ...], value_from = [column5, column6, ...], levels_order = None/list, flatten_levels = True/False, names_sep='_', names_glue= Callable ) ``` Method chaining syntax: ```python df = ( pd.DataFrame(...) .pivot_wider( index = [column1, column2, ...], names_from = [column3, column4, ...], value_from = [column5, column6, ...], levels_order = None/list, flatten_levels = True/False, names_sep='_', names_glue= Callable ) ) ``` :param df: A pandas dataframe. :param index: Name(s) of columns to use as identifier variables. Should be either a single column name, or a list of column names. The `janitor.select_columns` syntax is supported here, allowing for flexible and dynamic column selection. If `index` is not provided, the dataframe's index is used. :param names_from: Name(s) of column(s) to use to make the new dataframe's columns. Should be either a single column name, or a list of column names. The `janitor.select_columns` syntax is supported here, allowing for flexible and dynamic column selection. A label or labels must be provided for `names_from`. :param values_from: Name(s) of column(s) that will be used for populating the new dataframe's values. The `janitor.select_columns` syntax is supported here, allowing for flexible and dynamic column selection. If ``values_from`` is not specified, all remaining columns will be used. Note that values from `values_from` are usually at the top level, the dataframe's columns is not flattened, or the start of each label in the columns, if flattened. :param levels_order: Applicable if there are multiple `names_from` and/or `values_from`. Reorders the levels. Accepts a list of strings. If there are multiple `values_from`, pass a None to represent that level. :param flatten_levels: Default is `True`. If `False`, the dataframe stays as a MultiIndex. :param names_sep: If `names_from` or `values_from` contain multiple variables, this will be used to join their values into a single string to use as a column name. Default is `_`. Applicable only if `flatten_levels` is `True`. :param names_glue: A callable to control the output of the flattened columns. Applicable only if `flatten_levels` is True. Function should be acceptable to pandas\u2019 `map` function. :returns: A pandas DataFrame that has been unpivoted from long to wide form. \"\"\" df = df.copy() return _computations_pivot_wider( df, index, names_from, values_from, levels_order, flatten_levels, names_sep, names_glue, ) process_text process_text(df, column_name, string_function, **kwargs) Apply a Pandas string method to an existing column and return a dataframe. This function aims to make string cleaning easy, while chaining, by simply passing the string method name to the process_text function. This modifies an existing column; it does not create a new column. New columns can be created via pyjanitor's transform_columns . A list of all the string methods in Pandas can be accessed here <https://pandas.pydata.org/docs/user_guide/text.html#method-summary> __. Example: import pandas as pd import janitor as jn text code 0 Ragnar 1 1 sammywemmy 2 2 ginger 3 df.process_text(column_name = \"text\", string_function = \"lower\") text code 0 ragnar 1 1 sammywemmy 2 2 ginger 3 For string methods with parameters, simply pass the keyword arguments:: df.process_text( column_name = \"text\", string_function = \"extract\", pat = r\"(ag)\", expand = False, flags = re.IGNORECASE ) text code 0 ag 1 1 NaN 2 2 NaN 3 Functional usage syntax: import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.process_text( df = df, column_name, string_function = \"string_func_name_here\", kwargs ) Method-chaining usage syntax: import pandas as pd import janitor as jn df = ( pd.DataFrame(...) .process_text( column_name, string_function = \"string_func_name_here\", kwargs ) ) Parameters: Name Type Description Default df DataFrame A pandas dataframe. required column_name str String column to be operated on. required string_function str Pandas string method to be applied. required kwargs str Keyword arguments for parameters of the string_function . {} Returns: Type Description DataFrame A pandas dataframe with modified column(s). Exceptions: Type Description KeyError if string_function is not a Pandas string method. TypeError if the wrong kwarg is supplied. ValueError if column_name not found in dataframe. .. # noqa: DAR402 Source code in janitor/functions/process_text.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def process_text( df: pd.DataFrame, column_name: str, string_function: str, **kwargs: str, ) -> pd.DataFrame: \"\"\" Apply a Pandas string method to an existing column and return a dataframe. This function aims to make string cleaning easy, while chaining, by simply passing the string method name to the ``process_text`` function. This modifies an existing column; it does not create a new column. New columns can be created via pyjanitor's `transform_columns`. A list of all the string methods in Pandas can be accessed `here <https://pandas.pydata.org/docs/user_guide/text.html#method-summary>`__. Example: import pandas as pd import janitor as jn text code 0 Ragnar 1 1 sammywemmy 2 2 ginger 3 df.process_text(column_name = \"text\", string_function = \"lower\") text code 0 ragnar 1 1 sammywemmy 2 2 ginger 3 For string methods with parameters, simply pass the keyword arguments:: df.process_text( column_name = \"text\", string_function = \"extract\", pat = r\"(ag)\", expand = False, flags = re.IGNORECASE ) text code 0 ag 1 1 NaN 2 2 NaN 3 Functional usage syntax: import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.process_text( df = df, column_name, string_function = \"string_func_name_here\", kwargs ) Method-chaining usage syntax: import pandas as pd import janitor as jn df = ( pd.DataFrame(...) .process_text( column_name, string_function = \"string_func_name_here\", kwargs ) ) :param df: A pandas dataframe. :param column_name: String column to be operated on. :param string_function: Pandas string method to be applied. :param kwargs: Keyword arguments for parameters of the `string_function`. :returns: A pandas dataframe with modified column(s). :raises KeyError: if ``string_function`` is not a Pandas string method. :raises TypeError: if the wrong ``kwarg`` is supplied. :raises ValueError: if `column_name` not found in dataframe. .. # noqa: DAR402 \"\"\" check(\"column_name\", column_name, [str]) check(\"string_function\", string_function, [str]) check_column(df, [column_name]) pandas_string_methods = [ func.__name__ for _, func in inspect.getmembers(pd.Series.str, inspect.isfunction) if not func.__name__.startswith(\"_\") ] if string_function not in pandas_string_methods: raise KeyError(f\"{string_function} is not a Pandas string method.\") result = getattr(df[column_name].str, string_function)(**kwargs) if isinstance(result, pd.DataFrame): raise ValueError( \"\"\" The outcome of the processed text is a DataFrame, which is not supported in `process_text`. \"\"\" ) return df.assign(**{column_name: result}) remove_columns remove_columns(df, column_names) Remove the set of columns specified in column_names . This method does not mutate the original DataFrame. Intended to be the method-chaining alternative to del df[col] . Method chaining syntax: df = pd.DataFrame(...).remove_columns(column_names=['col1', 'col2']) Parameters: Name Type Description Default df DataFrame A pandas DataFrame required column_names Union[str, Iterable[str], Hashable] The columns to remove. required Returns: Type Description DataFrame A pandas DataFrame. Source code in janitor/functions/remove_columns.py @pf.register_dataframe_method @deprecated_alias(columns=\"column_names\") def remove_columns( df: pd.DataFrame, column_names: Union[str, Iterable[str], Hashable] ) -> pd.DataFrame: \"\"\"Remove the set of columns specified in `column_names`. This method does not mutate the original DataFrame. Intended to be the method-chaining alternative to `del df[col]`. Method chaining syntax: df = pd.DataFrame(...).remove_columns(column_names=['col1', 'col2']) :param df: A pandas DataFrame :param column_names: The columns to remove. :returns: A pandas DataFrame. \"\"\" return df.drop(columns=column_names) remove_empty remove_empty(df) Drop all rows and columns that are completely null. This method also resets the index(by default) since it doesn't make sense to preserve the index of a completely empty row. This method mutates the original DataFrame. Implementation is inspired from StackOverflow . Functional usage syntax: df = remove_empty(df) Method chaining syntax: import pandas as pd import janitor df = pd.DataFrame(...).remove_empty() Parameters: Name Type Description Default df DataFrame The pandas DataFrame object. required Returns: Type Description DataFrame A pandas DataFrame. Source code in janitor/functions/remove_empty.py @pf.register_dataframe_method def remove_empty(df: pd.DataFrame) -> pd.DataFrame: \"\"\"Drop all rows and columns that are completely null. This method also resets the index(by default) since it doesn't make sense to preserve the index of a completely empty row. This method mutates the original DataFrame. Implementation is inspired from [StackOverflow][so]. [so]: https://stackoverflow.com/questions/38884538/python-pandas-find-all-rows-where-all-values-are-nan Functional usage syntax: ```python df = remove_empty(df) ``` Method chaining syntax: ```python import pandas as pd import janitor df = pd.DataFrame(...).remove_empty() ``` :param df: The pandas DataFrame object. :returns: A pandas DataFrame. \"\"\" # noqa: E501 nanrows = df.index[df.isna().all(axis=1)] df = df.drop(index=nanrows).reset_index(drop=True) nancols = df.columns[df.isna().all(axis=0)] df = df.drop(columns=nancols) return df rename_columns rename_column(df, old_column_name, new_column_name) Rename a column in place. This method does not mutate the original DataFrame. Functional usage syntax: df = rename_column(df, \"old_column_name\", \"new_column_name\") Method chaining syntax: import pandas as pd import janitor df = ( pd.DataFrame(...) .rename_column(\"old_column_name\", \"new_column_name\") ) This is just syntactic sugar/a convenience function for renaming one column at a time. If you are convinced that there are multiple columns in need of changing, then use the pandas.DataFrame.rename method. Parameters: Name Type Description Default df DataFrame The pandas DataFrame object. required old_column_name str The old column name. required new_column_name str The new column name. required Returns: Type Description DataFrame A pandas DataFrame with renamed columns. Source code in janitor/functions/rename_columns.py @pf.register_dataframe_method @deprecated_alias(old=\"old_column_name\", new=\"new_column_name\") def rename_column( df: pd.DataFrame, old_column_name: str, new_column_name: str ) -> pd.DataFrame: \"\"\"Rename a column in place. This method does not mutate the original DataFrame. Functional usage syntax: ```python df = rename_column(df, \"old_column_name\", \"new_column_name\") ``` Method chaining syntax: ```python import pandas as pd import janitor df = ( pd.DataFrame(...) .rename_column(\"old_column_name\", \"new_column_name\") ) ``` This is just syntactic sugar/a convenience function for renaming one column at a time. If you are convinced that there are multiple columns in need of changing, then use the `pandas.DataFrame.rename` method. :param df: The pandas DataFrame object. :param old_column_name: The old column name. :param new_column_name: The new column name. :returns: A pandas DataFrame with renamed columns. \"\"\" # noqa: E501 check_column(df, [old_column_name]) return df.rename(columns={old_column_name: new_column_name}) rename_columns(df, new_column_names=None, function=None) Rename columns. Functional usage syntax: df = rename_columns(df, {\"old_column_name\": \"new_column_name\"}) df = rename_columns(df, function = str.upper) df = rename_columns( df, function = lambda x : x.lower() if x.startswith(\"double\") else x ) Method chaining syntax: import pandas as pd import janitor df = pd.DataFrame(...).rename_columns({\"old_column_name\": \"new_column_name\"}) This is just syntactic sugar/a convenience function for renaming multiple columns at a time. If you need to rename single column, then use the rename_column method. One of the new_column_names or function are a required parameter. If both are provided then new_column_names takes priority and function is never executed. Parameters: Name Type Description Default df DataFrame The pandas DataFrame object. required new_column_names Optional[Dict] A dictionary of old and new column names. None function Callable A function which should be applied to all the columns None Returns: Type Description DataFrame A pandas DataFrame with renamed columns. Exceptions: Type Description ValueError if both new_column_names and function are None Source code in janitor/functions/rename_columns.py @pf.register_dataframe_method def rename_columns( df: pd.DataFrame, new_column_names: Union[Dict, None] = None, function: Callable = None, ) -> pd.DataFrame: \"\"\"Rename columns. Functional usage syntax: ```python df = rename_columns(df, {\"old_column_name\": \"new_column_name\"}) df = rename_columns(df, function = str.upper) df = rename_columns( df, function = lambda x : x.lower() if x.startswith(\"double\") else x ) ``` Method chaining syntax: ```python import pandas as pd import janitor df = pd.DataFrame(...).rename_columns({\"old_column_name\": \"new_column_name\"}) ``` This is just syntactic sugar/a convenience function for renaming multiple columns at a time. If you need to rename single column, then use the `rename_column` method. One of the new_column_names or function are a required parameter. If both are provided then new_column_names takes priority and function is never executed. :param df: The pandas DataFrame object. :param new_column_names: A dictionary of old and new column names. :param function: A function which should be applied to all the columns :returns: A pandas DataFrame with renamed columns. :raises ValueError: if both new_column_names and function are None \"\"\" # noqa: E501 if new_column_names is None and function is None: raise ValueError( \"One of new_column_names or function must be provided\" ) if new_column_names is not None: check_column(df, new_column_names) return df.rename(columns=new_column_names) return df.rename(mapper=function, axis=\"columns\") reorder_columns reorder_columns(df, column_order) Reorder DataFrame columns by specifying desired order as list of col names. Columns not specified retain their order and follow after specified cols. Validates column_order to ensure columns are all present in DataFrame. This method does not mutate the original DataFrame. Functional usage syntax: Given DataFrame with column names col1 , col2 , col3 : df = reorder_columns(df, ['col2', 'col3']) Method chaining syntax: import pandas as pd import janitor df = pd.DataFrame(...).reorder_columns(['col2', 'col3']) The column order of df is now col2 , col3 , col1 . Internally, this function uses DataFrame.reindex with copy=False to avoid unnecessary data duplication. Parameters: Name Type Description Default df DataFrame DataFrame to reorder required column_order Union[Iterable[str], pandas.core.indexes.base.Index, Hashable] A list of column names or Pandas Index specifying their order in the returned DataFrame . required Returns: Type Description DataFrame A pandas DataFrame with reordered columns. Exceptions: Type Description IndexError if a column within column_order is not found within the DataFrame. Source code in janitor/functions/reorder_columns.py @pf.register_dataframe_method def reorder_columns( df: pd.DataFrame, column_order: Union[Iterable[str], pd.Index, Hashable] ) -> pd.DataFrame: \"\"\"Reorder DataFrame columns by specifying desired order as list of col names. Columns not specified retain their order and follow after specified cols. Validates column_order to ensure columns are all present in DataFrame. This method does not mutate the original DataFrame. Functional usage syntax: Given `DataFrame` with column names `col1`, `col2`, `col3`: ```python df = reorder_columns(df, ['col2', 'col3']) ``` Method chaining syntax: ```python import pandas as pd import janitor df = pd.DataFrame(...).reorder_columns(['col2', 'col3']) ``` The column order of `df` is now `col2`, `col3`, `col1`. Internally, this function uses `DataFrame.reindex` with `copy=False` to avoid unnecessary data duplication. :param df: `DataFrame` to reorder :param column_order: A list of column names or Pandas `Index` specifying their order in the returned `DataFrame`. :returns: A pandas DataFrame with reordered columns. :raises IndexError: if a column within `column_order` is not found within the DataFrame. \"\"\" check(\"column_order\", column_order, [list, tuple, pd.Index]) if any(col not in df.columns for col in column_order): raise IndexError( \"A column in `column_order` was not found in the DataFrame.\" ) # if column_order is a Pandas index, needs conversion to list: column_order = list(column_order) return df.reindex( columns=( column_order + [col for col in df.columns if col not in column_order] ), copy=False, ) round_to_fraction round_to_fraction(df, column_name=None, denominator=None, digits=inf) Round all values in a column to a fraction. This method mutates the original DataFrame. Taken from the R package . Also, optionally round to a specified number of digits. Method-chaining usage: # Round to two decimal places df = pd.DataFrame(...).round_to_fraction('a', 2) Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable Name of column to round to fraction. None denominator float The denominator of the fraction for rounding None digits float The number of digits for rounding after rounding to the fraction. Default is np.inf (i.e. no subsequent rounding) inf Returns: Type Description DataFrame A pandas DataFrame with a column's values rounded. Source code in janitor/functions/round_to_fraction.py @pf.register_dataframe_method @deprecated_alias(col_name=\"column_name\") def round_to_fraction( df: pd.DataFrame, column_name: Hashable = None, denominator: float = None, digits: float = np.inf, ) -> pd.DataFrame: \"\"\" Round all values in a column to a fraction. This method mutates the original DataFrame. Taken from [the R package](https://github.com/sfirke/janitor/issues/235). Also, optionally round to a specified number of digits. Method-chaining usage: ```python # Round to two decimal places df = pd.DataFrame(...).round_to_fraction('a', 2) ``` :param df: A pandas DataFrame. :param column_name: Name of column to round to fraction. :param denominator: The denominator of the fraction for rounding :param digits: The number of digits for rounding after rounding to the fraction. Default is np.inf (i.e. no subsequent rounding) :returns: A pandas DataFrame with a column's values rounded. \"\"\" if denominator: check(\"denominator\", denominator, [float, int]) if digits: check(\"digits\", digits, [float, int]) df[column_name] = round(df[column_name] * denominator, 0) / denominator if not np.isinf(digits): df[column_name] = round(df[column_name], digits) return df row_to_names row_to_names(df, row_number=None, remove_row=False, remove_rows_above=False, reset_index=False) Elevates a row to be the column names of a DataFrame. This method mutates the original DataFrame. Contains options to remove the elevated row from the DataFrame along with removing the rows above the selected row. Method chaining usage: df = ( pd.DataFrame(...) .row_to_names( row_number=0, remove_row=False, remove_rows_above=False, reset_index=False, ) ) Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required row_number int The row containing the variable names None remove_row bool Whether the row should be removed from the DataFrame. Defaults to False. False remove_rows_above bool Whether the rows above the selected row should be removed from the DataFrame. Defaults to False. False reset_index bool Whether the index should be reset on the returning DataFrame. Defaults to False. False Returns: Type Description DataFrame A pandas DataFrame with set column names. Source code in janitor/functions/row_to_names.py @pf.register_dataframe_method def row_to_names( df: pd.DataFrame, row_number: int = None, remove_row: bool = False, remove_rows_above: bool = False, reset_index: bool = False, ) -> pd.DataFrame: \"\"\"Elevates a row to be the column names of a DataFrame. This method mutates the original DataFrame. Contains options to remove the elevated row from the DataFrame along with removing the rows above the selected row. Method chaining usage: df = ( pd.DataFrame(...) .row_to_names( row_number=0, remove_row=False, remove_rows_above=False, reset_index=False, ) ) :param df: A pandas DataFrame. :param row_number: The row containing the variable names :param remove_row: Whether the row should be removed from the DataFrame. Defaults to False. :param remove_rows_above: Whether the rows above the selected row should be removed from the DataFrame. Defaults to False. :param reset_index: Whether the index should be reset on the returning DataFrame. Defaults to False. :returns: A pandas DataFrame with set column names. \"\"\" # :Setup: # ```python # import pandas as pd # import janitor # data_dict = { # \"a\": [1, 2, 3] * 3, # \"Bell__Chart\": [1, 2, 3] * 3, # \"decorated-elephant\": [1, 2, 3] * 3, # \"animals\": [\"rabbit\", \"leopard\", \"lion\"] * 3, # \"cities\": [\"Cambridge\", \"Shanghai\", \"Basel\"] * 3 # } # :Example: Move first row to column names: # ```python # example_dataframe = pd.DataFrame(data_dict) # example_dataframe.row_to_names(0) # :Output: # ```python # 1 1 1 rabbit Cambridge # 0 1 1 1 rabbit Cambridge # 1 2 2 2 leopard Shanghai # 2 3 3 3 lion Basel # 3 1 1 1 rabbit Cambridge # 4 2 2 2 leopard Shanghai # 5 3 3 3 lion Basel # 6 1 1 1 rabbit Cambridge # 7 2 2 2 leopard Shanghai # :Example: Move first row to column names and # remove row while resetting the index: # ```python # example_dataframe = pd.DataFrame(data_dict) # example_dataframe.row_to_names(0, remove_row=True,\\ # reset_index=True) # :Output: # ```python # 1 1 1 rabbit Cambridge # 0 2 2 2 leopard Shanghai # 1 3 3 3 lion Basel # 2 1 1 1 rabbit Cambridge # 3 2 2 2 leopard Shanghai # 4 3 3 3 lion Basel # 5 1 1 1 rabbit Cambridge # 6 2 2 2 leopard Shanghai # 7 3 3 3 lion Basel # :Example: Move first row to column names and remove # row without resetting the index: # ```python # example_dataframe = pd.DataFrame(data_dict) # example_dataframe.row_to_names(0, remove_row=True) # :Output: # ```python # 1 1 1 rabbit Cambridge # 1 2 2 2 leopard Shanghai # 2 3 3 3 lion Basel # 3 1 1 1 rabbit Cambridge # 4 2 2 2 leopard Shanghai # 5 3 3 3 lion Basel # 6 1 1 1 rabbit Cambridge # 7 2 2 2 leopard Shanghai # 8 3 3 3 lion Basel # :Example: Move first row to column names, remove row # and remove rows above selected row without resetting # index: # ```python # example_dataframe = pd.DataFrame(data_dict) # example_dataframe.row_to_names(2, remove_row=True, \\ # remove_rows_above=True, reset_index= True) # :Output: # ```python # 3 3 3 lion Basel # 0 1 1 1 rabbit Cambridge # 1 2 2 2 leopard Shanghai # 2 3 3 3 lion Basel # 3 1 1 1 rabbit Cambridge # 4 2 2 2 leopard Shanghai # 5 3 3 3 lion Basel # :Example: Move first row to column names, remove row, # and remove rows above selected row without resetting # index: # ```python # example_dataframe = pd.DataFrame(data_dict) # example_dataframe.row_to_names(2, remove_row=True, \\ # remove_rows_above=True) # :Output: # ```python # 3 3 3 lion Basel # 3 1 1 1 rabbit Cambridge # 4 2 2 2 leopard Shanghai # 5 3 3 3 lion Basel # 6 1 1 1 rabbit Cambridge # 7 2 2 2 leopard Shanghai # 8 3 3 3 lion Basel check(\"row_number\", row_number, [int]) warnings.warn( \"The function row_to_names will, in the official 1.0 release, \" \"change its behaviour to reset the dataframe's index by default. \" \"You can prepare for this change right now by explicitly setting \" \"`reset_index=True` when calling on `row_to_names`.\" ) df.columns = df.iloc[row_number, :] df.columns.name = None if remove_row: df = df.drop(df.index[row_number]) if remove_rows_above: df = df.drop(df.index[range(row_number)]) if reset_index: df = df.reset_index(drop=[\"index\"]) return df select_columns select_columns(df, *args, *, invert=False) Method-chainable selection of columns. Not applicable to MultiIndex columns. It accepts a string, shell-like glob strings (*string*) , regex, slice, array-like object, or a list of the previous options. This method does not mutate the original DataFrame. Optional ability to invert selection of columns available as well. import pandas as pd import janitor import numpy as np import datetime import re from janitor import patterns from pandas.api.types import is_datetime64_dtype df = pd.DataFrame( { \"id\": [0, 1], \"Name\": [\"ABC\", \"XYZ\"], \"code\": [1, 2], \"code1\": [4, np.nan], \"code2\": [\"8\", 5], \"type\": [\"S\", \"R\"], \"type1\": [\"E\", np.nan], \"type2\": [\"T\", \"U\"], \"code3\": pd.Series([\"a\", \"b\"], dtype=\"category\"), \"type3\": pd.to_datetime([np.datetime64(\"2018-01-01\"), datetime.datetime(2018, 1, 1)]), } ) df id Name code code1 code2 type type1 type2 code3 type3 0 0 ABC 1 4.0 8 S E T a 2018-01-01 1 1 XYZ 2 NaN 5 R NaN U b 2018-01-01 Select by string: df.select_columns(\"id\") id 0 0 1 1 Select via shell-like glob strings ( * ) is possible: df.select_columns(\"type*\") type type1 type2 type3 0 S E T 2018-01-01 1 R NaN U 2018-01-01 Select by slice: df.select_columns(slice(\"code1\", \"type1\")) code1 code2 type type1 0 4.0 8 S E 1 NaN 5 R NaN Select by Callable (the callable is applied to every column and should return a single True or False per column): df.select_columns(is_datetime64_dtype) type3 0 2018-01-01 1 2018-01-01 df.select_columns(lambda x: x.name.startswith(\"code\") or x.name.endswith(\"1\")) code code1 code2 type1 code3 0 1 4.0 8 E a 1 2 NaN 5 NaN b df.select_columns(lambda x: x.isna().any()) code1 type1 0 4.0 E 1 NaN NaN Select by regular expression: df.select_columns(re.compile(\"\\d+\")) code1 code2 type1 type2 code3 type3 0 4.0 8 E T a 2018-01-01 1 NaN 5 NaN U b 2018-01-01 # same as above, with janitor.patterns # simply a wrapper around re.compile df.select_columns(patterns(\"\\d+\")) code1 code2 type1 type2 code3 type3 0 4.0 8 E T a 2018-01-01 1 NaN 5 NaN U b 2018-01-01 Select a combination of the above (you can combine any of the previous options): df.select_columns(\"id\", \"code*\", slice(\"code\", \"code2\")) id code code1 code2 code3 0 0 1 4.0 8 a 1 1 2 NaN 5 b You can also pass a sequence of booleans: df.select_columns([True, False, True, True, True, False, False, False, True, False]) id code code1 code2 code3 0 0 1 4.0 8 a 1 1 2 NaN 5 b Setting invert to True returns the complement of the columns provided: df.select_columns(\"id\", \"code*\", slice(\"code\", \"code2\"), invert = True) Name type type1 type2 type3 0 ABC S E T 2018-01-01 1 XYZ R NaN U 2018-01-01 Functional usage example: import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.select_columns('a', 'b', 'col_*', invert=True) Method-chaining example: df = (pd.DataFrame(...) .select_columns('a', 'b', 'col_*', invert=True)) Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required args Valid inputs include: - an exact column name to look for - a shell-style glob string (e.g., *_thing_* ) - a regular expression - a callable which is applicable to each Series in the dataframe - variable arguments of all the aforementioned. - a sequence of booleans. () invert bool Whether or not to invert the selection. This will result in the selection of the complement of the columns provided. False Returns: Type Description DataFrame A pandas DataFrame with the specified columns selected. Source code in janitor/functions/select_columns.py @pf.register_dataframe_method @deprecated_alias(search_cols=\"search_column_names\") def select_columns( df: pd.DataFrame, *args, invert: bool = False, ) -> pd.DataFrame: \"\"\" Method-chainable selection of columns. Not applicable to MultiIndex columns. It accepts a string, shell-like glob strings `(*string*)`, regex, slice, array-like object, or a list of the previous options. This method does not mutate the original DataFrame. Optional ability to invert selection of columns available as well. ```python import pandas as pd import janitor import numpy as np import datetime import re from janitor import patterns from pandas.api.types import is_datetime64_dtype df = pd.DataFrame( { \"id\": [0, 1], \"Name\": [\"ABC\", \"XYZ\"], \"code\": [1, 2], \"code1\": [4, np.nan], \"code2\": [\"8\", 5], \"type\": [\"S\", \"R\"], \"type1\": [\"E\", np.nan], \"type2\": [\"T\", \"U\"], \"code3\": pd.Series([\"a\", \"b\"], dtype=\"category\"), \"type3\": pd.to_datetime([np.datetime64(\"2018-01-01\"), datetime.datetime(2018, 1, 1)]), } ) df id Name code code1 code2 type type1 type2 code3 type3 0 0 ABC 1 4.0 8 S E T a 2018-01-01 1 1 XYZ 2 NaN 5 R NaN U b 2018-01-01 ``` - Select by string: ``` df.select_columns(\"id\") id 0 0 1 1 ``` - Select via shell-like glob strings (`*`) is possible: ```python df.select_columns(\"type*\") type type1 type2 type3 0 S E T 2018-01-01 1 R NaN U 2018-01-01 ``` - Select by slice: ```python df.select_columns(slice(\"code1\", \"type1\")) code1 code2 type type1 0 4.0 8 S E 1 NaN 5 R NaN ``` - Select by `Callable` (the callable is applied to every column and should return a single `True` or `False` per column): ```python df.select_columns(is_datetime64_dtype) type3 0 2018-01-01 1 2018-01-01 df.select_columns(lambda x: x.name.startswith(\"code\") or x.name.endswith(\"1\")) code code1 code2 type1 code3 0 1 4.0 8 E a 1 2 NaN 5 NaN b df.select_columns(lambda x: x.isna().any()) code1 type1 0 4.0 E 1 NaN NaN ``` - Select by regular expression: ```python df.select_columns(re.compile(\"\\\\d+\")) code1 code2 type1 type2 code3 type3 0 4.0 8 E T a 2018-01-01 1 NaN 5 NaN U b 2018-01-01 # same as above, with janitor.patterns # simply a wrapper around re.compile df.select_columns(patterns(\"\\\\d+\")) code1 code2 type1 type2 code3 type3 0 4.0 8 E T a 2018-01-01 1 NaN 5 NaN U b 2018-01-01 ``` - Select a combination of the above (you can combine any of the previous options): ```python df.select_columns(\"id\", \"code*\", slice(\"code\", \"code2\")) id code code1 code2 code3 0 0 1 4.0 8 a 1 1 2 NaN 5 b ``` - You can also pass a sequence of booleans: ```python df.select_columns([True, False, True, True, True, False, False, False, True, False]) id code code1 code2 code3 0 0 1 4.0 8 a 1 1 2 NaN 5 b ``` - Setting `invert` to `True` returns the complement of the columns provided: ```python df.select_columns(\"id\", \"code*\", slice(\"code\", \"code2\"), invert = True) Name type type1 type2 type3 0 ABC S E T 2018-01-01 1 XYZ R NaN U 2018-01-01 ``` Functional usage example: ```python import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.select_columns('a', 'b', 'col_*', invert=True) ``` Method-chaining example: ```python df = (pd.DataFrame(...) .select_columns('a', 'b', 'col_*', invert=True)) ``` :param df: A pandas DataFrame. :param args: Valid inputs include: - an exact column name to look for - a shell-style glob string (e.g., `*_thing_*`) - a regular expression - a callable which is applicable to each Series in the dataframe - variable arguments of all the aforementioned. - a sequence of booleans. :param invert: Whether or not to invert the selection. This will result in the selection of the complement of the columns provided. :returns: A pandas DataFrame with the specified columns selected. \"\"\" # applicable for any # list-like object (ndarray, Series, pd.Index, ...) # excluding tuples, which are returned as is search_column_names = [] for arg in args: if is_list_like(arg) and (not isinstance(arg, tuple)): search_column_names.extend([*arg]) else: search_column_names.append(arg) if len(search_column_names) == 1: search_column_names = search_column_names[0] full_column_list = _select_column_names(search_column_names, df) if invert: return df.drop(columns=full_column_list) return df.loc[:, full_column_list] shuffle shuffle(df, random_state=None, reset_index=True) Shuffle the rows of the DataFrame. This method does not mutate the original DataFrame. Super-sugary syntax! Underneath the hood, we use df.sample(frac=1) , with the option to set the random state. Example usage: df = pd.DataFrame(...).shuffle() Parameters: Name Type Description Default df DataFrame A pandas DataFrame required random_state (optional) A seed for the random number generator. None reset_index bool (optional) Resets index to default integers True Returns: Type Description DataFrame A shuffled pandas DataFrame. Source code in janitor/functions/shuffle.py @pf.register_dataframe_method def shuffle( df: pd.DataFrame, random_state=None, reset_index: bool = True ) -> pd.DataFrame: \"\"\"Shuffle the rows of the DataFrame. This method does not mutate the original DataFrame. Super-sugary syntax! Underneath the hood, we use `df.sample(frac=1)`, with the option to set the random state. Example usage: ```python df = pd.DataFrame(...).shuffle() ``` :param df: A pandas DataFrame :param random_state: (optional) A seed for the random number generator. :param reset_index: (optional) Resets index to default integers :returns: A shuffled pandas DataFrame. \"\"\" result = df.sample(frac=1, random_state=random_state) if reset_index: result = result.reset_index(drop=True) return result sort_column_value_order sort_column_value_order(df, column, column_value_order, columns=None) This function adds precedence to certain values in a specified column, then sorts based on that column and any other specified columns. Example: SalesMonth Company2 Company3 Company1 150.0 Jan 180.0 400.0 200.0 Feb 250.0 500.0 200.0 Feb 250.0 500.0 300.0 Mar NaN 600.0 400.0 April 500.0 675.0 Given the current DataFrame, we want to order the sales month in desc order. To achieve this we would assign the later months with smaller values with the latest month, such as April with the precedence of 0. df = sort_column_value_order( df, 'SalesMonth', {'April':1,'Mar':2,'Feb':3,'Jan':4} ) The returned DataFrame will look as follows. SalesMonth Company2 Company3 Company1 400.0 April 500.0 675.0 300.0 Mar NaN 600.0 200.0 Feb 250.0 500.0 200.0 Feb 250.0 500.0 150.0 Jan 180.0 400.0 Parameters: Name Type Description Default df DataFrame This is our DataFrame that we are manipulating required column str This is a column name as a string we are using to specify which column to sort by required column_value_order dict This is a dictionary of values that will represent precedence of the values in the specified column required columns This is a list of additional columns that we can sort by None Returns: Type Description DataFrame This function returns a Pandas DataFrame Exceptions: Type Description ValueError raises error if chosen Column Name is not in Dataframe, or if column_value_order dictionary is empty. Source code in janitor/functions/sort_column_value_order.py @pf.register_dataframe_method def sort_column_value_order( df: pd.DataFrame, column: str, column_value_order: dict, columns=None ) -> pd.DataFrame: \"\"\" This function adds precedence to certain values in a specified column, then sorts based on that column and any other specified columns. Example: SalesMonth Company2 Company3 Company1 150.0 Jan 180.0 400.0 200.0 Feb 250.0 500.0 200.0 Feb 250.0 500.0 300.0 Mar NaN 600.0 400.0 April 500.0 675.0 Given the current DataFrame, we want to order the sales month in desc order. To achieve this we would assign the later months with smaller values with the latest month, such as April with the precedence of 0. df = sort_column_value_order( df, 'SalesMonth', {'April':1,'Mar':2,'Feb':3,'Jan':4} ) The returned DataFrame will look as follows. SalesMonth Company2 Company3 Company1 400.0 April 500.0 675.0 300.0 Mar NaN 600.0 200.0 Feb 250.0 500.0 200.0 Feb 250.0 500.0 150.0 Jan 180.0 400.0 :param df: This is our DataFrame that we are manipulating :param column: This is a column name as a string we are using to specify which column to sort by :param column_value_order: This is a dictionary of values that will represent precedence of the values in the specified column :param columns: This is a list of additional columns that we can sort by :raises ValueError: raises error if chosen Column Name is not in Dataframe, or if column_value_order dictionary is empty. :return: This function returns a Pandas DataFrame \"\"\" if len(column_value_order) > 0: if column in df.columns: df[\"cond_order\"] = df[column].replace(column_value_order) if columns is None: new_df = df.sort_values(\"cond_order\") del new_df[\"cond_order\"] else: new_df = df.sort_values(columns + [\"cond_order\"]) del new_df[\"cond_order\"] return new_df else: raise ValueError(\"Column Name not in DataFrame\") else: raise ValueError(\"column_value_order dictionary cannot be empty\") sort_naturally sort_naturally(df, column_name, **natsorted_kwargs) Sort a DataFrame by a column using natural sorting. Natural sorting is distinct from the default lexiographical sorting provided by pandas . For example, given the following list of items: [\"A1\", \"A11\", \"A3\", \"A2\", \"A10\"] lexicographical sorting would give us: [\"A1\", \"A10\", \"A11\", \"A2\", \"A3\"] By contrast, \"natural\" sorting would give us: [\"A1\", \"A2\", \"A3\", \"A10\", \"A11\"] This function thus provides natural sorting on a single column of a dataframe. To accomplish this, we do a natural sort on the unique values that are present in the dataframe. Then, we reconstitute the entire dataframe in the naturally sorted order. Natural sorting is provided by the Python package natsort All keyword arguments to natsort should be provided after the column name to sort by is provided. They are passed through to the natsorted function. Functional usage syntax: import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.sort_naturally( df=df, column_name='alphanumeric_column', ) Method chaining usage syntax: import pandas as pd import janitor df = pd.DataFrame(...) df = df.sort_naturally( column_name='alphanumeric_column', ) Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name str The column on which natural sorting should take place. required natsorted_kwargs Keyword arguments to be passed to natsort's natsorted function. {} Returns: Type Description DataFrame A sorted pandas DataFrame. Source code in janitor/functions/sort_naturally.py @pf.register_dataframe_method def sort_naturally( df: pd.DataFrame, column_name: str, **natsorted_kwargs ) -> pd.DataFrame: \"\"\"Sort a DataFrame by a column using *natural* sorting. Natural sorting is distinct from the default lexiographical sorting provided by `pandas`. For example, given the following list of items: [\"A1\", \"A11\", \"A3\", \"A2\", \"A10\"] lexicographical sorting would give us: [\"A1\", \"A10\", \"A11\", \"A2\", \"A3\"] By contrast, \"natural\" sorting would give us: [\"A1\", \"A2\", \"A3\", \"A10\", \"A11\"] This function thus provides *natural* sorting on a single column of a dataframe. To accomplish this, we do a natural sort on the unique values that are present in the dataframe. Then, we reconstitute the entire dataframe in the naturally sorted order. Natural sorting is provided by the Python package [natsort](https://natsort.readthedocs.io/en/master/index.html) All keyword arguments to `natsort` should be provided after the column name to sort by is provided. They are passed through to the `natsorted` function. Functional usage syntax: ```python import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.sort_naturally( df=df, column_name='alphanumeric_column', ) ``` Method chaining usage syntax: ```python import pandas as pd import janitor df = pd.DataFrame(...) df = df.sort_naturally( column_name='alphanumeric_column', ) ``` :param df: A pandas DataFrame. :param column_name: The column on which natural sorting should take place. :param natsorted_kwargs: Keyword arguments to be passed to natsort's `natsorted` function. :returns: A sorted pandas DataFrame. \"\"\" new_order = index_natsorted(df[column_name], **natsorted_kwargs) return df.iloc[new_order, :] take_first take_first(df, subset, by, ascending=True) Take the first row within each group specified by subset . This method does not mutate the original DataFrame. import pandas as pd import janitor data = { \"a\": [\"x\", \"x\", \"y\", \"y\"], \"b\": [0, 1, 2, 3] } df = pd.DataFrame(data) df.take_first(subset=\"a\", by=\"b\") Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required subset Union[Hashable, Iterable[Hashable]] Column(s) defining the group. required by Hashable Column to sort by. required ascending bool Whether or not to sort in ascending order, bool . True Returns: Type Description DataFrame A pandas DataFrame. Source code in janitor/functions/take_first.py @pf.register_dataframe_method def take_first( df: pd.DataFrame, subset: Union[Hashable, Iterable[Hashable]], by: Hashable, ascending: bool = True, ) -> pd.DataFrame: \"\"\" Take the first row within each group specified by `subset`. This method does not mutate the original DataFrame. ```python import pandas as pd import janitor data = { \"a\": [\"x\", \"x\", \"y\", \"y\"], \"b\": [0, 1, 2, 3] } df = pd.DataFrame(data) df.take_first(subset=\"a\", by=\"b\") ``` :param df: A pandas DataFrame. :param subset: Column(s) defining the group. :param by: Column to sort by. :param ascending: Whether or not to sort in ascending order, `bool`. :returns: A pandas DataFrame. \"\"\" result = df.sort_values(by=by, ascending=ascending).drop_duplicates( subset=subset, keep=\"first\" ) return result then then(df, func) Add an arbitrary function to run in the pyjanitor method chain. This method does not mutate the original DataFrame. Parameters: Name Type Description Default df DataFrame A pandas dataframe. required func Callable A function you would like to run in the method chain. It should take one parameter and return one parameter, each being the DataFrame object. After that, do whatever you want in the middle. Go crazy. required Returns: Type Description DataFrame A pandas DataFrame. Source code in janitor/functions/then.py @pf.register_dataframe_method def then(df: pd.DataFrame, func: Callable) -> pd.DataFrame: \"\"\" Add an arbitrary function to run in the `pyjanitor` method chain. This method does not mutate the original DataFrame. :param df: A pandas dataframe. :param func: A function you would like to run in the method chain. It should take one parameter and return one parameter, each being the DataFrame object. After that, do whatever you want in the middle. Go crazy. :returns: A pandas DataFrame. \"\"\" df = func(df) return df to_datetime to_datetime(df, column_name, **kwargs) Method-chainable pd.to_datetime . This method mutates the original DataFrame. Functional usage syntax: df = to_datetime(df, 'col1', format='%Y%m%d') Method chaining syntax: import pandas as pd import janitor df = pd.DataFrame(...).to_datetime('col1', format='%Y%m%d') Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable Column name. required kwargs provide any kwargs that pd.to_datetime can take. {} Returns: Type Description DataFrame A pandas DataFrame with updated datetime data. Source code in janitor/functions/to_datetime.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def to_datetime( df: pd.DataFrame, column_name: Hashable, **kwargs ) -> pd.DataFrame: \"\"\" Method-chainable `pd.to_datetime`. This method mutates the original DataFrame. Functional usage syntax: ```python df = to_datetime(df, 'col1', format='%Y%m%d') ``` Method chaining syntax: ```python import pandas as pd import janitor df = pd.DataFrame(...).to_datetime('col1', format='%Y%m%d') ``` :param df: A pandas DataFrame. :param column_name: Column name. :param kwargs: provide any kwargs that `pd.to_datetime` can take. :returns: A pandas DataFrame with updated datetime data. \"\"\" df[column_name] = pd.to_datetime(df[column_name], **kwargs) return df toset toset(series) Return a set of the values. These are each a scalar type, which is a Python scalar (for str, int, float) or a pandas scalar (for Timestamp/Timedelta/Interval/Period) Functional usage syntax: import pandas as pd import janitor as jn series = pd.Series(...) s = jn.functions.toset(series=series) Method chaining usage example: import pandas as pd import janitor series = pd.Series(...) s = series.toset() Parameters: Name Type Description Default series Series A pandas series. required Returns: Type Description Set A set of values. Source code in janitor/functions/toset.py @pf.register_series_method def toset(series: pd.Series) -> Set: \"\"\"Return a set of the values. These are each a scalar type, which is a Python scalar (for str, int, float) or a pandas scalar (for Timestamp/Timedelta/Interval/Period) Functional usage syntax: import pandas as pd import janitor as jn series = pd.Series(...) s = jn.functions.toset(series=series) Method chaining usage example: import pandas as pd import janitor series = pd.Series(...) s = series.toset() :param series: A pandas series. :returns: A set of values. \"\"\" return set(series.tolist()) transform_columns transform_column(df, column_name, function, dest_column_name=None, elementwise=True) Transform the given column in-place using the provided function. Functions can be applied one of two ways: Element-wise (default; `elementwise=True``) Column-wise (alternative; `elementwise=False``) If the function is applied \"elementwise\", then the first argument of the function signature should be the individual element of each function. This is the default behaviour of `transform_column``, because it is easy to understand. For example: def elemwise_func(x): modified_x = ... # do stuff here return modified_x df.transform_column(column_name=\"my_column\", function=elementwise_func) On the other hand, columnwise application of a function behaves as if the function takes in a pandas Series and emits back a sequence that is of identical length to the original. One place where this is desirable is to gain access to pandas native string methods, which are super fast! def columnwise_func(s: pd.Series) -> pd.Series: return s.str[0:5] df.transform_column( column_name=\"my_column\", lambda s: s.str[0:5], elementwise=False ) This method does not mutate the original DataFrame. Let's say we wanted to apply a log10 transform a column of data. Originally one would write code like this: # YOU NO LONGER NEED TO WRITE THIS! df[column_name] = df[column_name].apply(np.log10) With the method chaining syntax, we can do the following instead: df = ( pd.DataFrame(...) .transform_column(column_name, np.log10) ) With the functional syntax: df = pd.DataFrame(...) df = transform_column(df, column_name, np.log10) Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable The column to transform. required function Callable A function to apply on the column. required dest_column_name Optional[str] The column name to store the transformation result in. Defaults to None, which will result in the original column name being overwritten. If a name is provided here, then a new column with the transformed values will be created. None elementwise bool Whether to apply the function elementwise or not. If elementwise is True, then the function's first argument should be the data type of each datum in the column of data, and should return a transformed datum. If elementwise is False, then the function's should expect a pandas Series passed into it, and return a pandas Series. True Returns: Type Description DataFrame A pandas DataFrame with a transformed column. Source code in janitor/functions/transform_columns.py @pf.register_dataframe_method @deprecated_alias(col_name=\"column_name\", dest_col_name=\"dest_column_name\") def transform_column( df: pd.DataFrame, column_name: Hashable, function: Callable, dest_column_name: Optional[str] = None, elementwise: bool = True, ) -> pd.DataFrame: \"\"\"Transform the given column in-place using the provided function. Functions can be applied one of two ways: - Element-wise (default; `elementwise=True``) - Column-wise (alternative; `elementwise=False``) If the function is applied \"elementwise\", then the first argument of the function signature should be the individual element of each function. This is the default behaviour of `transform_column``, because it is easy to understand. For example: def elemwise_func(x): modified_x = ... # do stuff here return modified_x df.transform_column(column_name=\"my_column\", function=elementwise_func) On the other hand, columnwise application of a function behaves as if the function takes in a pandas Series and emits back a sequence that is of identical length to the original. One place where this is desirable is to gain access to `pandas` native string methods, which are super fast! def columnwise_func(s: pd.Series) -> pd.Series: return s.str[0:5] df.transform_column( column_name=\"my_column\", lambda s: s.str[0:5], elementwise=False ) This method does not mutate the original DataFrame. Let's say we wanted to apply a log10 transform a column of data. Originally one would write code like this: # YOU NO LONGER NEED TO WRITE THIS! df[column_name] = df[column_name].apply(np.log10) With the method chaining syntax, we can do the following instead: df = ( pd.DataFrame(...) .transform_column(column_name, np.log10) ) With the functional syntax: df = pd.DataFrame(...) df = transform_column(df, column_name, np.log10) :param df: A pandas DataFrame. :param column_name: The column to transform. :param function: A function to apply on the column. :param dest_column_name: The column name to store the transformation result in. Defaults to None, which will result in the original column name being overwritten. If a name is provided here, then a new column with the transformed values will be created. :param elementwise: Whether to apply the function elementwise or not. If elementwise is True, then the function's first argument should be the data type of each datum in the column of data, and should return a transformed datum. If elementwise is False, then the function's should expect a pandas Series passed into it, and return a pandas Series. :returns: A pandas DataFrame with a transformed column. \"\"\" if dest_column_name is None: dest_column_name = column_name if elementwise: result = df[column_name].apply(function) else: result = function(df[column_name]) df = df.assign(**{dest_column_name: result}) return df transform_columns(df, column_names, function, suffix=None, elementwise=True, new_column_names=None) Transform multiple columns through the same transformation. This method mutates the original DataFrame. Super syntactic sugar! Basically wraps transform_column and calls it repeatedly over all column names provided. User can optionally supply either a suffix to create a new set of columns with the specified suffix, or provide a dictionary mapping each original column name to its corresponding new column name. Note that all column names must be strings. A few examples below. Firstly, to just log10 transform a list of columns without creating new columns to hold the transformed values: df = ( pd.DataFrame(...) .transform_columns(['col1', 'col2', 'col3'], np.log10) ) Secondly, to add a '_log' suffix when creating a new column, which we think is going to be the most common use case: df = ( pd.DataFrame(...) .transform_columns( ['col1', 'col2', 'col3'], np.log10, suffix=\"_log\" ) ) Finally, to provide new names explicitly: df = ( pd.DataFrame(...) .transform_column( ['col1', 'col2', 'col3'], np.log10, new_column_names={ 'col1': 'transform1', 'col2': 'transform2', 'col3': 'transform3', } ) ) Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_names Union[List[str], Tuple[str]] An iterable of columns to transform. required function Callable A function to apply on each column. required suffix Optional[str] (optional) Suffix to use when creating new columns to hold the transformed values. None elementwise bool Passed on to transform_column ; whether or not to apply the transformation function elementwise (True) or columnwise (False). True new_column_names Optional[Dict[str, str]] (optional) An explicit mapping of old column names to new column names. None Returns: Type Description DataFrame A pandas DataFrame with transformed columns. Exceptions: Type Description ValueError if both suffix and new_column_names are specified Source code in janitor/functions/transform_columns.py @pf.register_dataframe_method @deprecated_alias(columns=\"column_names\", new_names=\"new_column_names\") def transform_columns( df: pd.DataFrame, column_names: Union[List[str], Tuple[str]], function: Callable, suffix: Optional[str] = None, elementwise: bool = True, new_column_names: Optional[Dict[str, str]] = None, ) -> pd.DataFrame: \"\"\"Transform multiple columns through the same transformation. This method mutates the original DataFrame. Super syntactic sugar! Basically wraps `transform_column` and calls it repeatedly over all column names provided. User can optionally supply either a suffix to create a new set of columns with the specified suffix, or provide a dictionary mapping each original column name to its corresponding new column name. Note that all column names must be strings. A few examples below. Firstly, to just log10 transform a list of columns without creating new columns to hold the transformed values: df = ( pd.DataFrame(...) .transform_columns(['col1', 'col2', 'col3'], np.log10) ) Secondly, to add a '_log' suffix when creating a new column, which we think is going to be the most common use case: df = ( pd.DataFrame(...) .transform_columns( ['col1', 'col2', 'col3'], np.log10, suffix=\"_log\" ) ) Finally, to provide new names explicitly: df = ( pd.DataFrame(...) .transform_column( ['col1', 'col2', 'col3'], np.log10, new_column_names={ 'col1': 'transform1', 'col2': 'transform2', 'col3': 'transform3', } ) ) :param df: A pandas DataFrame. :param column_names: An iterable of columns to transform. :param function: A function to apply on each column. :param suffix: (optional) Suffix to use when creating new columns to hold the transformed values. :param elementwise: Passed on to `transform_column`; whether or not to apply the transformation function elementwise (True) or columnwise (False). :param new_column_names: (optional) An explicit mapping of old column names to new column names. :returns: A pandas DataFrame with transformed columns. :raises ValueError: if both `suffix` and `new_column_names` are specified \"\"\" dest_column_names = dict(zip(column_names, column_names)) check(\"column_names\", column_names, [list, tuple]) if suffix is not None and new_column_names is not None: raise ValueError( \"only one of suffix or new_column_names should be specified\" ) if suffix: # If suffix is specified... check(\"suffix\", suffix, [str]) for col in column_names: dest_column_names[col] = col + suffix if new_column_names: # If new_column_names is specified... check(\"new_column_names\", new_column_names, [dict]) dest_column_names = new_column_names # Now, transform columns. for old_col, new_col in dest_column_names.items(): df = transform_column( df, old_col, function, new_col, elementwise=elementwise ) return df truncate_datetime truncate_datetime_dataframe(df, datepart) Truncate times down to a user-specified precision of year, month, day, hour, minute, or second. Call on datetime object to truncate it. Calling on existing df will not alter the contents of said df. Note: Truncating down to a Month or Day will yields 0s, as there is no 0 month or 0 day in most datetime systems. Parameters: Name Type Description Default df DataFrame The dataframe on which to truncate datetime. required datepart str Truncation precision, YEAR, MONTH, DAY, HOUR, MINUTE, SECOND. (String is automagically capitalized) required Returns: Type Description DataFrame a truncated datetime object to the precision specified by datepart. Source code in janitor/functions/truncate_datetime.py @pf.register_dataframe_method def truncate_datetime_dataframe( df: pd.DataFrame, datepart: str ) -> pd.DataFrame: \"\"\" Truncate times down to a user-specified precision of year, month, day, hour, minute, or second. Call on datetime object to truncate it. Calling on existing df will not alter the contents of said df. Note: Truncating down to a Month or Day will yields 0s, as there is no 0 month or 0 day in most datetime systems. :param df: The dataframe on which to truncate datetime. :param datepart: Truncation precision, YEAR, MONTH, DAY, HOUR, MINUTE, SECOND. (String is automagically capitalized) :returns: a truncated datetime object to the precision specified by datepart. \"\"\" for i in df.columns: for j in df.index: try: df[i][j] = _truncate_datetime(datepart, df[i][j]) except KeyError: pass except TypeError: pass except AttributeError: pass return df update_where update_where(df, conditions, target_column_name, target_val) Add multiple conditions to update a column in the dataframe. This method does not mutate the original DataFrame. Example usage: data = { \"a\": [1, 2, 3, 4], \"b\": [5, 6, 7, 8], \"c\": [0, 0, 0, 0] } df = pd.DataFrame(data) a b c 0 1 5 0 1 2 6 0 2 3 7 0 3 4 8 0 df.update_where(conditions = (df.a > 2) & (df.b < 8), target_column_name = 'c', target_val = 10) a b c 0 1 5 0 1 2 6 0 2 3 7 10 3 4 8 0 update_where also supports pandas query style string expressions: df.update_where(conditions = \"a > 2 and b < 8\", target_column_name = 'c', target_val = 10) a b c 0 1 5 0 1 2 6 0 2 3 7 10 3 4 8 0 Parameters: Name Type Description Default df DataFrame The pandas DataFrame object. required conditions Any Conditions used to update a target column and target value. required target_column_name Hashable Column to be updated. If column does not exist in DataFrame, a new column will be created; note that entries that do not get set in the new column will be null. required target_val Any Value to be updated required Returns: Type Description DataFrame A pandas DataFrame. Exceptions: Type Description ValueError if conditions does not return a boolean array-like data structure. .. # noqa: DAR402 Source code in janitor/functions/update_where.py @pf.register_dataframe_method @deprecated_alias(target_col=\"target_column_name\") def update_where( df: pd.DataFrame, conditions: Any, target_column_name: Hashable, target_val: Any, ) -> pd.DataFrame: \"\"\" Add multiple conditions to update a column in the dataframe. This method does not mutate the original DataFrame. Example usage: ```python data = { \"a\": [1, 2, 3, 4], \"b\": [5, 6, 7, 8], \"c\": [0, 0, 0, 0] } df = pd.DataFrame(data) a b c 0 1 5 0 1 2 6 0 2 3 7 0 3 4 8 0 df.update_where(conditions = (df.a > 2) & (df.b < 8), target_column_name = 'c', target_val = 10) a b c 0 1 5 0 1 2 6 0 2 3 7 10 3 4 8 0 ``` `update_where` also supports pandas *query* style string expressions: ```python df.update_where(conditions = \"a > 2 and b < 8\", target_column_name = 'c', target_val = 10) a b c 0 1 5 0 1 2 6 0 2 3 7 10 3 4 8 0 ``` :param df: The pandas DataFrame object. :param conditions: Conditions used to update a target column and target value. :param target_column_name: Column to be updated. If column does not exist in DataFrame, a new column will be created; note that entries that do not get set in the new column will be null. :param target_val: Value to be updated :returns: A pandas DataFrame. :raises ValueError: if `conditions` does not return a boolean array-like data structure. .. # noqa: DAR402 \"\"\" df = df.copy() # use query mode if a string expression is passed if isinstance(conditions, str): conditions = df.eval(conditions) if not is_bool_dtype(conditions): raise ValueError( \"\"\" Kindly ensure that `conditions` passed evaluates to a Boolean dtype. \"\"\" ) df.loc[conditions, target_column_name] = target_val return df utils Utility functions for all of the functions submodule. patterns(regex_pattern) This function converts a string into a compiled regular expression; it can be used to select columns in the index or columns_names arguments of pivot_longer function. Parameters: Name Type Description Default regex_pattern Union[str, Pattern] string to be converted to compiled regular expression. required Returns: Type Description Pattern A compile regular expression from provided regex_pattern . Source code in janitor/functions/utils.py def patterns(regex_pattern: Union[str, Pattern]) -> Pattern: \"\"\" This function converts a string into a compiled regular expression; it can be used to select columns in the index or columns_names arguments of `pivot_longer` function. :param regex_pattern: string to be converted to compiled regular expression. :returns: A compile regular expression from provided `regex_pattern`. \"\"\" check(\"regular expression\", regex_pattern, [str, Pattern]) return re.compile(regex_pattern) unionize_dataframe_categories(*dataframes, *, column_names=None) Given a group of dataframes which contain some categorical columns, for each categorical column present, find all the possible categories across all the dataframes which have that column. Update each dataframes' corresponding column with a new categorical object that contains the original data but has labels for all the possible categories from all dataframes. This is useful when concatenating a list of dataframes which all have the same categorical columns into one dataframe. If, for a given categorical column, all input dataframes do not have at least one instance of all the possible categories, Pandas will change the output dtype of that column from category to object , losing out on dramatic speed gains you get from the former format. Usage example for concatenation of categorical column-containing dataframes: Instead of: concatenated_df = pd.concat([df1, df2, df3], ignore_index=True) which in your case has resulted in category -> object conversion, use: unionized_dataframes = unionize_dataframe_categories(df1, df2, df2) concatenated_df = pd.concat(unionized_dataframes, ignore_index=True) Parameters: Name Type Description Default dataframes The dataframes you wish to unionize the categorical objects for. () column_names Optional[Iterable[pandas.core.dtypes.dtypes.CategoricalDtype]] If supplied, only unionize this subset of columns. None Returns: Type Description List[pandas.core.frame.DataFrame] A list of the category-unioned dataframes in the same order they were provided. Exceptions: Type Description TypeError If any of the inputs are not pandas DataFrames. Source code in janitor/functions/utils.py def unionize_dataframe_categories( *dataframes, column_names: Optional[Iterable[pd.CategoricalDtype]] = None ) -> List[pd.DataFrame]: \"\"\" Given a group of dataframes which contain some categorical columns, for each categorical column present, find all the possible categories across all the dataframes which have that column. Update each dataframes' corresponding column with a new categorical object that contains the original data but has labels for all the possible categories from all dataframes. This is useful when concatenating a list of dataframes which all have the same categorical columns into one dataframe. If, for a given categorical column, all input dataframes do not have at least one instance of all the possible categories, Pandas will change the output dtype of that column from `category` to `object`, losing out on dramatic speed gains you get from the former format. Usage example for concatenation of categorical column-containing dataframes: Instead of: ```python concatenated_df = pd.concat([df1, df2, df3], ignore_index=True) ``` which in your case has resulted in `category` -> `object` conversion, use: ```python unionized_dataframes = unionize_dataframe_categories(df1, df2, df2) concatenated_df = pd.concat(unionized_dataframes, ignore_index=True) ``` :param dataframes: The dataframes you wish to unionize the categorical objects for. :param column_names: If supplied, only unionize this subset of columns. :returns: A list of the category-unioned dataframes in the same order they were provided. :raises TypeError: If any of the inputs are not pandas DataFrames. \"\"\" if any(not isinstance(df, pd.DataFrame) for df in dataframes): raise TypeError(\"Inputs must all be dataframes.\") if column_names is None: # Find all columns across all dataframes that are categorical column_names = set() for dataframe in dataframes: column_names = column_names.union( [ column_name for column_name in dataframe.columns if isinstance( dataframe[column_name].dtype, pd.CategoricalDtype ) ] ) else: column_names = [column_names] # For each categorical column, find all possible values across the DFs category_unions = { column_name: union_categoricals( [df[column_name] for df in dataframes if column_name in df.columns] ) for column_name in column_names } # Make a shallow copy of all DFs and modify the categorical columns # such that they can encode the union of all possible categories for each. refactored_dfs = [] for df in dataframes: df = df.copy(deep=False) for column_name, categorical in category_unions.items(): if column_name in df.columns: df[column_name] = pd.Categorical( df[column_name], categories=categorical.categories ) refactored_dfs.append(df) return refactored_dfs","title":"Functions"},{"location":"api/functions/#functions","text":"","title":"Functions"},{"location":"api/functions/#janitor.functions--general-functions","text":"pyjanitor's general-purpose data cleaning functions. NOTE: Instructions for future contributors: Place the source code of the functions in a file named after the function. Place utility functions in the same file. If you use a utility function from another source file, please refactor it out to janitor.functions.utils . Import the function into this file so that it shows up in the top-level API. Sort the imports in alphabetical order. Try to group related functions together (e.g. see convert_date.py ) Never import utils.","title":"General Functions"},{"location":"api/functions/#janitor.functions.add_columns","text":"","title":"add_columns"},{"location":"api/functions/#janitor.functions.add_columns.add_column","text":"Add a column to the dataframe. This method does not mutate the original DataFrame. Intended to be the method-chaining alternative to:: df[column_name] = value Method chaining syntax adding a column with only a single value: # This will add a column with only one value. df = pd.DataFrame(...).add_column(column_name=\"new_column\", 2) Method chaining syntax adding a column with more than one value: # This will add a column with an iterable of values. vals = [1, 2, 5, ..., 3, 4] # of same length as the dataframe. df = pd.DataFrame(...).add_column(column_name=\"new_column\", vals) Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name str Name of the new column. Should be a string, in order for the column name to be compatible with the Feather binary format (this is a useful thing to have). required value Union[List[Any], Tuple[Any], Any] Either a single value, or a list/tuple of values. required fill_remaining bool If value is a tuple or list that is smaller than the number of rows in the DataFrame, repeat the list or tuple (R-style) to the end of the DataFrame. False Returns: Type Description DataFrame A pandas DataFrame with an added column. Exceptions: Type Description ValueError if attempting to add a column that already exists. ValueError if value has more elements that number of rows in the DataFrame. ValueError if attempting to add an iterable of values with a length not equal to the number of DataFrame rows. ValueError if value has length of `0``. Source code in janitor/functions/add_columns.py @pf.register_dataframe_method @deprecated_alias(col_name=\"column_name\") def add_column( df: pd.DataFrame, column_name: str, value: Union[List[Any], Tuple[Any], Any], fill_remaining: bool = False, ) -> pd.DataFrame: \"\"\"Add a column to the dataframe. This method does not mutate the original DataFrame. Intended to be the method-chaining alternative to:: df[column_name] = value Method chaining syntax adding a column with only a single value: # This will add a column with only one value. df = pd.DataFrame(...).add_column(column_name=\"new_column\", 2) Method chaining syntax adding a column with more than one value: # This will add a column with an iterable of values. vals = [1, 2, 5, ..., 3, 4] # of same length as the dataframe. df = pd.DataFrame(...).add_column(column_name=\"new_column\", vals) :param df: A pandas DataFrame. :param column_name: Name of the new column. Should be a string, in order for the column name to be compatible with the Feather binary format (this is a useful thing to have). :param value: Either a single value, or a list/tuple of values. :param fill_remaining: If value is a tuple or list that is smaller than the number of rows in the DataFrame, repeat the list or tuple (R-style) to the end of the DataFrame. :returns: A pandas DataFrame with an added column. :raises ValueError: if attempting to add a column that already exists. :raises ValueError: if `value` has more elements that number of rows in the DataFrame. :raises ValueError: if attempting to add an iterable of values with a length not equal to the number of DataFrame rows. :raises ValueError: if `value` has length of `0``. \"\"\" # TODO: Convert examples to notebook. # :Setup: # ```python # import pandas as pd # import janitor # data = { # \"a\": [1, 2, 3] * 3, # \"Bell__Chart\": [1, 2, 3] * 3, # \"decorated-elephant\": [1, 2, 3] * 3, # \"animals\": [\"rabbit\", \"leopard\", \"lion\"] * 3, # \"cities\": [\"Cambridge\", \"Shanghai\", \"Basel\"] * 3, # } # df = pd.DataFrame(data) # :Example 1: Create a new column with a single value: # ```python # df.add_column(\"city_pop\", 100000) # :Output: # ```python # a Bell__Chart decorated-elephant animals cities city_pop # 0 1 1 1 rabbit Cambridge 100000 # 1 2 2 2 leopard Shanghai 100000 # 2 3 3 3 lion Basel 100000 # 3 1 1 1 rabbit Cambridge 100000 # 4 2 2 2 leopard Shanghai 100000 # 5 3 3 3 lion Basel 100000 # 6 1 1 1 rabbit Cambridge 100000 # 7 2 2 2 leopard Shanghai 100000 # 8 3 3 3 lion Basel 100000 # :Example 2: Create a new column with an iterator which fills to the # column # size: # ```python # df.add_column(\"city_pop\", range(3), fill_remaining=True) # :Output: # ```python # a Bell__Chart decorated-elephant animals cities city_pop # 0 1 1 1 rabbit Cambridge 0 # 1 2 2 2 leopard Shanghai 1 # 2 3 3 3 lion Basel 2 # 3 1 1 1 rabbit Cambridge 0 # 4 2 2 2 leopard Shanghai 1 # 5 3 3 3 lion Basel 2 # 6 1 1 1 rabbit Cambridge 0 # 7 2 2 2 leopard Shanghai 1 # 8 3 3 3 lion Basel 2 # :Example 3: Add new column based on mutation of other columns: # ```python # df.add_column(\"city_pop\", df.Bell__Chart - 2 * df.a) # :Output: # ```python # a Bell__Chart decorated-elephant animals cities city_pop # 0 1 1 1 rabbit Cambridge -1 # 1 2 2 2 leopard Shanghai -2 # 2 3 3 3 lion Basel -3 # 3 1 1 1 rabbit Cambridge -1 # 4 2 2 2 leopard Shanghai -2 # 5 3 3 3 lion Basel -3 # 6 1 1 1 rabbit Cambridge -1 # 7 2 2 2 leopard Shanghai -2 # 8 3 3 3 lion Basel -3 df = df.copy() check(\"column_name\", column_name, [str]) if column_name in df.columns: raise ValueError( f\"Attempted to add column that already exists: \" f\"{column_name}.\" ) nrows = df.shape[0] if hasattr(value, \"__len__\") and not isinstance( value, (str, bytes, bytearray) ): # if `value` is a list, ndarray, etc. if len(value) > nrows: raise ValueError( \"`value` has more elements than number of rows \" f\"in your `DataFrame`. vals: {len(value)}, \" f\"df: {nrows}\" ) if len(value) != nrows and not fill_remaining: raise ValueError( \"Attempted to add iterable of values with length\" \" not equal to number of DataFrame rows\" ) if len(value) == 0: raise ValueError( \"`value` has to be an iterable of minimum length 1\" ) len_value = len(value) elif fill_remaining: # relevant if a scalar val was passed, yet fill_remaining == True len_value = 1 value = [value] nrows = df.shape[0] if fill_remaining: times_to_loop = int(np.ceil(nrows / len_value)) fill_values = list(value) * times_to_loop df[column_name] = fill_values[:nrows] else: df[column_name] = value return df","title":"add_column()"},{"location":"api/functions/#janitor.functions.add_columns.add_columns","text":"Add multiple columns to the dataframe. This method does not mutate the original DataFrame. Method to augment add_column with ability to add multiple columns in one go. This replaces the need for multiple add_column calls. Usage is through supplying kwargs where the key is the col name and the values correspond to the values of the new DataFrame column. Values passed can be scalar or iterable (list, ndarray, etc.) Usage example: x = 3 y = np.arange(0, 10) df = pd.DataFrame(...).add_columns(x=x, y=y) Parameters: Name Type Description Default df DataFrame A pandas dataframe. required fill_remaining bool If value is a tuple or list that is smaller than the number of rows in the DataFrame, repeat the list or tuple (R-style) to the end of the DataFrame. (Passed to add_column ) False kwargs column, value pairs which are looped through in add_column calls. {} Returns: Type Description DataFrame A pandas DataFrame with added columns. Source code in janitor/functions/add_columns.py @pf.register_dataframe_method def add_columns( df: pd.DataFrame, fill_remaining: bool = False, **kwargs ) -> pd.DataFrame: \"\"\"Add multiple columns to the dataframe. This method does not mutate the original DataFrame. Method to augment `add_column` with ability to add multiple columns in one go. This replaces the need for multiple `add_column` calls. Usage is through supplying kwargs where the key is the col name and the values correspond to the values of the new DataFrame column. Values passed can be scalar or iterable (list, ndarray, etc.) Usage example: x = 3 y = np.arange(0, 10) df = pd.DataFrame(...).add_columns(x=x, y=y) :param df: A pandas dataframe. :param fill_remaining: If value is a tuple or list that is smaller than the number of rows in the DataFrame, repeat the list or tuple (R-style) to the end of the DataFrame. (Passed to `add_column`) :param kwargs: column, value pairs which are looped through in `add_column` calls. :returns: A pandas DataFrame with added columns. \"\"\" # Note: error checking can pretty much be handled in `add_column` for col_name, values in kwargs.items(): df = df.add_column(col_name, values, fill_remaining=fill_remaining) return df","title":"add_columns()"},{"location":"api/functions/#janitor.functions.also","text":"Implementation source for chainable function also .","title":"also"},{"location":"api/functions/#janitor.functions.also.also","text":"Run a function with side effects. THis function allows you to run an arbitrary function in the pyjanitor method chain. Doing so will let you do things like save the dataframe to disk midway while continuing to modify the dataframe afterwards. Example usage: df = ( pd.DataFrame(...) .query(...) .also(lambda df: print(f\"DataFrame shape is: {df.shape}\")) .transform_column(...) .also(lambda df: df.to_csv(\"midpoint.csv\")) .also( lambda df: print( f\"Column col_name has these values: {set(df['col_name'].unique())}\" ) ) .group_add(...) ) Parameters: Name Type Description Default df DataFrame A pandas dataframe. required func Callable A function you would like to run in the method chain. It should take one DataFrame object as a parameter and have no return. If there is a return, it will be ignored. required args Optional arguments for func . () kwargs Optional keyword arguments for func . {} Returns: Type Description DataFrame The input pandas DataFrame. Source code in janitor/functions/also.py @pf.register_dataframe_method def also(df: pd.DataFrame, func: Callable, *args, **kwargs) -> pd.DataFrame: \"\"\" Run a function with side effects. THis function allows you to run an arbitrary function in the `pyjanitor` method chain. Doing so will let you do things like save the dataframe to disk midway while continuing to modify the dataframe afterwards. Example usage: ```python df = ( pd.DataFrame(...) .query(...) .also(lambda df: print(f\"DataFrame shape is: {df.shape}\")) .transform_column(...) .also(lambda df: df.to_csv(\"midpoint.csv\")) .also( lambda df: print( f\"Column col_name has these values: {set(df['col_name'].unique())}\" ) ) .group_add(...) ) ``` :param df: A pandas dataframe. :param func: A function you would like to run in the method chain. It should take one DataFrame object as a parameter and have no return. If there is a return, it will be ignored. :param args: Optional arguments for `func`. :param kwargs: Optional keyword arguments for `func`. :returns: The input pandas DataFrame. \"\"\" # noqa: E501 func(df.copy(), *args, **kwargs) return df","title":"also()"},{"location":"api/functions/#janitor.functions.bin_numeric","text":"","title":"bin_numeric"},{"location":"api/functions/#janitor.functions.bin_numeric.bin_numeric","text":"Generate a new column that labels bins for a specified numeric column. This method mutates the original DataFrame. Makes use of pandas cut() function to bin data of one column, generating a new column with the results. import pandas as pd import janitor df = ( pd.DataFrame(...) .bin_numeric( from_column_name='col1', to_column_name='col1_binned', num_bins=3, labels=['1-2', '3-4', '5-6'] ) ) Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required from_column_name Hashable The column whose data you want binned. required to_column_name Hashable The new column to be created with the binned data. required num_bins int The number of bins to be utilized. 5 labels Optional[str] Optionally rename numeric bin ranges with labels. Number of label names must match number of bins specified. None Returns: Type Description DataFrame A pandas DataFrame. Exceptions: Type Description ValueError if number of labels do not match number of bins. Source code in janitor/functions/bin_numeric.py @pf.register_dataframe_method @deprecated_alias(from_column=\"from_column_name\", to_column=\"to_column_name\") def bin_numeric( df: pd.DataFrame, from_column_name: Hashable, to_column_name: Hashable, num_bins: int = 5, labels: Optional[str] = None, ) -> pd.DataFrame: \"\"\" Generate a new column that labels bins for a specified numeric column. This method mutates the original DataFrame. Makes use of pandas `cut()` function to bin data of one column, generating a new column with the results. ```python import pandas as pd import janitor df = ( pd.DataFrame(...) .bin_numeric( from_column_name='col1', to_column_name='col1_binned', num_bins=3, labels=['1-2', '3-4', '5-6'] ) ) ``` :param df: A pandas DataFrame. :param from_column_name: The column whose data you want binned. :param to_column_name: The new column to be created with the binned data. :param num_bins: The number of bins to be utilized. :param labels: Optionally rename numeric bin ranges with labels. Number of label names must match number of bins specified. :return: A pandas DataFrame. :raises ValueError: if number of labels do not match number of bins. \"\"\" if not labels: df[str(to_column_name)] = pd.cut( df[str(from_column_name)], bins=num_bins ) else: if not len(labels) == num_bins: raise ValueError(\"Number of labels must match number of bins.\") df[str(to_column_name)] = pd.cut( df[str(from_column_name)], bins=num_bins, labels=labels ) return df","title":"bin_numeric()"},{"location":"api/functions/#janitor.functions.case_when","text":"","title":"case_when"},{"location":"api/functions/#janitor.functions.case_when.case_when","text":"Convenience function for creating a column, based on a condition, or multiple conditions. It is similar to SQL and dplyr's case_when, with inspiration from pydatatable if_else function. If your scenario requires direct replacement of values, pandas' replace method or map method should be better suited and more efficient; if the conditions check if a value is within a range of values, pandas' cut or qcut should be more efficient; np.where/np.select are also performant options. This function relies on pd.Series.mask method. When multiple conditions are satisfied, the first one is used. The variable *args parameters takes arguments of the form : condition0 , value0 , condition1 , value1 , ..., default . If condition0 evaluates to True , then assign value0 to column_name , if condition1 evaluates to True , then assign value1 to column_name , and so on. If none of the conditions evaluate to True , assign default to column_name . This function can be likened to SQL's case_when : CASE WHEN condition0 THEN value0 WHEN condition1 THEN value1 --- more conditions ELSE default END AS column_name compared to python's if-elif-else : if condition0: value0 elif condition1: value1 # more elifs else: default Functional usage syntax: import pandas as pd import janitor as jn df = pd.DataFrame(...) right = pd.DataFrame(...) df = jn.case_when( df, condition0, result0, condition1, result1, ..., default, column_name = 'column', ) Method chaining syntax: .. code-block:: python df = df.case_when( condition0, result0, condition1, result1, ..., default, column_name = 'column', ) Parameters: Name Type Description Default df DataFrame A Pandas dataframe. required args Variable argument of conditions and expected values. Takes the form condition0 , value0 , condition1 , value1 , ..., default . condition can be a 1-D boolean array, a callable, or a string. If condition is a callable, it should evaluate to a 1-D boolean array. The array should have the same length as the DataFrame. If it is a string, it is computed on the dataframe, via df.eval , and should return a 1-D boolean array. result can be a scalar, a 1-D array, or a callable. If result is a callable, it should evaluate to a 1-D array. For a 1-D array, it should have the same length as the DataFrame. The default argument applies if none of condition0 , condition1 , ..., evaluates to True . Value can be a scalar, a callabe, or a 1-D array. if default is a callable, it should evaluate to a 1-D array. The 1-D array should be the same length as the DataFrame. () column_name str Name of column to assign results to. A new column is created, if it does not already exist in the DataFrame. required Returns: Type Description DataFrame A pandas DataFrame. Exceptions: Type Description ValueError if the condition fails to evaluate. Source code in janitor/functions/case_when.py @pf.register_dataframe_method def case_when(df: pd.DataFrame, *args, column_name: str) -> pd.DataFrame: \"\"\" Convenience function for creating a column, based on a condition, or multiple conditions. It is similar to SQL and dplyr's case_when, with inspiration from `pydatatable` if_else function. If your scenario requires direct replacement of values, pandas' `replace` method or `map` method should be better suited and more efficient; if the conditions check if a value is within a range of values, pandas' `cut` or `qcut` should be more efficient; `np.where/np.select` are also performant options. This function relies on `pd.Series.mask` method. When multiple conditions are satisfied, the first one is used. The variable `*args` parameters takes arguments of the form : `condition0`, `value0`, `condition1`, `value1`, ..., `default`. If `condition0` evaluates to `True`, then assign `value0` to `column_name`, if `condition1` evaluates to `True`, then assign `value1` to `column_name`, and so on. If none of the conditions evaluate to `True`, assign `default` to `column_name`. This function can be likened to SQL's `case_when`: ```sql CASE WHEN condition0 THEN value0 WHEN condition1 THEN value1 --- more conditions ELSE default END AS column_name ``` compared to python's `if-elif-else`: ```python if condition0: value0 elif condition1: value1 # more elifs else: default ``` Functional usage syntax: ```python import pandas as pd import janitor as jn df = pd.DataFrame(...) right = pd.DataFrame(...) df = jn.case_when( df, condition0, result0, condition1, result1, ..., default, column_name = 'column', ) ``` Method chaining syntax: .. code-block:: python ```python df = df.case_when( condition0, result0, condition1, result1, ..., default, column_name = 'column', ) ``` :param df: A Pandas dataframe. :param args: Variable argument of conditions and expected values. Takes the form `condition0`, `value0`, `condition1`, `value1`, ..., `default`. `condition` can be a 1-D boolean array, a callable, or a string. If `condition` is a callable, it should evaluate to a 1-D boolean array. The array should have the same length as the DataFrame. If it is a string, it is computed on the dataframe, via `df.eval`, and should return a 1-D boolean array. `result` can be a scalar, a 1-D array, or a callable. If `result` is a callable, it should evaluate to a 1-D array. For a 1-D array, it should have the same length as the DataFrame. The `default` argument applies if none of `condition0`, `condition1`, ..., evaluates to `True`. Value can be a scalar, a callabe, or a 1-D array. if `default` is a callable, it should evaluate to a 1-D array. The 1-D array should be the same length as the DataFrame. :param column_name: Name of column to assign results to. A new column is created, if it does not already exist in the DataFrame. :raises ValueError: if the condition fails to evaluate. :returns: A pandas DataFrame. \"\"\" conditions, targets, default = _case_when_checks(df, args, column_name) if len(conditions) == 1: default = default.mask(conditions[0], targets[0]) return df.assign(**{column_name: default}) # ensures value assignment is on a first come basis conditions = conditions[::-1] targets = targets[::-1] for condition, value, index in zip(conditions, targets, count()): try: default = default.mask(condition, value) # error `feedoff` idea from SO # https://stackoverflow.com/a/46091127/7175713 except Exception as e: raise ValueError( f\"\"\" condition{index} and value{index} failed to evaluate. Original error message: {e} \"\"\" ) from e return df.assign(**{column_name: default})","title":"case_when()"},{"location":"api/functions/#janitor.functions.change_type","text":"","title":"change_type"},{"location":"api/functions/#janitor.functions.change_type.change_type","text":"Change the type of a column. This method mutates the original DataFrame. Exceptions that are raised can be ignored. For example, if one has a mixed dtype column that has non-integer strings and integers, and you want to coerce everything to integers, you can optionally ignore the non-integer strings and replace them with NaN or keep the original value Intended to be the method-chaining alternative to: df[col] = df[col].astype(dtype) Method chaining syntax: df = pd.DataFrame(...).change_type('col1', str) Parameters: Name Type Description Default df DataFrame A pandas dataframe. required column_name Hashable A column in the dataframe. required dtype type The datatype to convert to. Should be one of the standard Python types, or a numpy datatype. required ignore_exception bool one of `{False, \"fillna\", \"keep_values\"}``. False Returns: Type Description DataFrame A pandas DataFrame with changed column types. Exceptions: Type Description ValueError if unknown option provided for `ignore_exception``. Source code in janitor/functions/change_type.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def change_type( df: pd.DataFrame, column_name: Hashable, dtype: type, ignore_exception: bool = False, ) -> pd.DataFrame: \"\"\"Change the type of a column. This method mutates the original DataFrame. Exceptions that are raised can be ignored. For example, if one has a mixed dtype column that has non-integer strings and integers, and you want to coerce everything to integers, you can optionally ignore the non-integer strings and replace them with `NaN` or keep the original value Intended to be the method-chaining alternative to: ```python df[col] = df[col].astype(dtype) ``` Method chaining syntax: ```python df = pd.DataFrame(...).change_type('col1', str) ``` :param df: A pandas dataframe. :param column_name: A column in the dataframe. :param dtype: The datatype to convert to. Should be one of the standard Python types, or a numpy datatype. :param ignore_exception: one of `{False, \"fillna\", \"keep_values\"}``. :returns: A pandas DataFrame with changed column types. :raises ValueError: if unknown option provided for `ignore_exception``. \"\"\" if not ignore_exception: df[column_name] = df[column_name].astype(dtype) elif ignore_exception == \"keep_values\": df[column_name] = df[column_name].astype(dtype, errors=\"ignore\") elif ignore_exception == \"fillna\": # returns None when conversion def convert(x, dtype): try: return dtype(x) except ValueError: return None df[column_name] = df[column_name].apply(lambda x: convert(x, dtype)) else: raise ValueError(\"unknown option for ignore_exception\") return df","title":"change_type()"},{"location":"api/functions/#janitor.functions.clean_names","text":"Functions for cleaning columns names.","title":"clean_names"},{"location":"api/functions/#janitor.functions.clean_names.clean_names","text":"Clean column names. Takes all column names, converts them to lowercase, then replaces all spaces with underscores. By default, column names are converted to string types. This can be switched off by passing in enforce_string=False . This method does not mutate the original DataFrame. Functional usage syntax: df = clean_names(df) Method chaining syntax: ```python import pandas as pd import janitor df = pd.DataFrame(...).clean_names() Examples of transformation: Columns before: First Name, Last Name, Employee Status, Subject Columns after: first_name, last_name, employee_status, subject Parameters: Name Type Description Default df DataFrame The pandas DataFrame object. required strip_underscores Union[str, bool] (optional) Removes the outer underscores from all column names. Default None keeps outer underscores. Values can be either 'left', 'right' or 'both' or the respective shorthand 'l', 'r' and True. None case_type str (optional) Whether to make columns lower or uppercase. Current case may be preserved with 'preserve', while snake case conversion (from CamelCase or camelCase only) can be turned on using \"snake\". Default 'lower' makes all characters lowercase. 'lower' remove_special bool (optional) Remove special characters from columns. Only letters, numbers and underscores are preserved. False strip_accents bool Whether or not to remove accents from columns names. True preserve_original_columns bool (optional) Preserve original names. This is later retrievable using df.original_columns . True enforce_string bool Whether or not to convert all column names to string type. Defaults to True, but can be turned off. Columns with >1 levels will not be converted by default. True truncate_limit int (optional) Truncates formatted column names to the specified length. Default None does not truncate. None Returns: Type Description DataFrame A pandas DataFrame. Source code in janitor/functions/clean_names.py @pf.register_dataframe_method def clean_names( df: pd.DataFrame, strip_underscores: Optional[Union[str, bool]] = None, case_type: str = \"lower\", remove_special: bool = False, strip_accents: bool = True, preserve_original_columns: bool = True, enforce_string: bool = True, truncate_limit: int = None, ) -> pd.DataFrame: \"\"\" Clean column names. Takes all column names, converts them to lowercase, then replaces all spaces with underscores. By default, column names are converted to string types. This can be switched off by passing in `enforce_string=False`. This method does not mutate the original DataFrame. Functional usage syntax: ```python df = clean_names(df) ``` Method chaining syntax: ```python import pandas as pd import janitor df = pd.DataFrame(...).clean_names() Examples of transformation: - Columns before: First Name, Last Name, Employee Status, Subject - Columns after: first_name, last_name, employee_status, subject :param df: The pandas DataFrame object. :param strip_underscores: (optional) Removes the outer underscores from all column names. Default None keeps outer underscores. Values can be either 'left', 'right' or 'both' or the respective shorthand 'l', 'r' and True. :param case_type: (optional) Whether to make columns lower or uppercase. Current case may be preserved with 'preserve', while snake case conversion (from CamelCase or camelCase only) can be turned on using \"snake\". Default 'lower' makes all characters lowercase. :param remove_special: (optional) Remove special characters from columns. Only letters, numbers and underscores are preserved. :param strip_accents: Whether or not to remove accents from columns names. :param preserve_original_columns: (optional) Preserve original names. This is later retrievable using `df.original_columns`. :param enforce_string: Whether or not to convert all column names to string type. Defaults to True, but can be turned off. Columns with >1 levels will not be converted by default. :param truncate_limit: (optional) Truncates formatted column names to the specified length. Default None does not truncate. :returns: A pandas DataFrame. \"\"\" original_column_names = list(df.columns) if enforce_string: df = df.rename(columns=str) df = df.rename(columns=lambda x: _change_case(x, case_type)) df = df.rename(columns=_normalize_1) if remove_special: df = df.rename(columns=_remove_special) if strip_accents: df = df.rename(columns=_strip_accents) df = df.rename(columns=lambda x: re.sub(\"_+\", \"_\", x)) # noqa: PD005 df = _strip_underscores(df, strip_underscores) df = df.rename(columns=lambda x: x[:truncate_limit]) # Store the original column names, if enabled by user if preserve_original_columns: df.__dict__[\"original_columns\"] = original_column_names return df","title":"clean_names()"},{"location":"api/functions/#janitor.functions.coalesce","text":"","title":"coalesce"},{"location":"api/functions/#janitor.functions.coalesce.coalesce","text":"Coalesce two or more columns of data in order of column names provided. This finds the first non-missing value at each position. This method does not mutate the original DataFrame. TODO: Turn the example in this docstring into a Jupyter notebook. Example: import pandas as pd import janitor as jn df = pd.DataFrame({\"A\": [1, 2, np.nan], \"B\": [np.nan, 10, np.nan], \"C\": [5, 10, 7]}) A B C 0 1.0 NaN 5 1 2.0 10.0 10 2 NaN NaN 7 df.coalesce('A', 'B', 'C', target_column_name = 'D') A B C D 0 1.0 NaN 5 1.0 1 2.0 10.0 10 2.0 2 NaN NaN 7 7.0 If no target column is provided, then the first column is updated, with the null values removed: df.coalesce('A', 'B', 'C') A B C 0 1.0 NaN 5 1 2.0 10.0 10 2 7.0 NaN 7 If nulls remain, you can fill it with the default_value : df = pd.DataFrame({'s1':[np.nan,np.nan,6,9,9], 's2':[np.nan,8,7,9,9]}) s1 s2 0 NaN NaN 1 NaN 8.0 2 6.0 7.0 3 9.0 9.0 4 9.0 9.0 df.coalesce('s1', 's2', target_column_name = 's3', default_value = 0) s1 s2 s3 0 NaN NaN 0.0 1 NaN 8.0 8.0 2 6.0 7.0 6.0 3 9.0 9.0 9.0 4 9.0 9.0 9.0 Functional usage syntax: df = coalesce(df, 'col1', 'col2', target_column_name ='col3') Method chaining syntax: import pandas as pd import janitor df = pd.DataFrame(...).coalesce('col1', 'col2') The first example will create a new column called col3 with values from col2 inserted where values from col1 are NaN . The second example will update the values of col1 , since it is the first column in column_names . This is more syntactic diabetes! For R users, this should look familiar to dplyr 's coalesce function; for Python users, the interface should be more intuitive than the pandas.Series.combine_first method. Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_names A list of column names. () target_column_name Optional[str] The new column name after combining. If None , then the first column in column_names is updated, with the Null values replaced. None default_value Union[int, float, str] A scalar to replace any remaining nulls after coalescing. None Returns: Type Description DataFrame A pandas DataFrame with coalesced columns. Exceptions: Type Description ValueError if length of column_names is less than 2. Source code in janitor/functions/coalesce.py @pf.register_dataframe_method @deprecated_alias(columns=\"column_names\", new_column_name=\"target_column_name\") def coalesce( df: pd.DataFrame, *column_names, target_column_name: Optional[str] = None, default_value: Optional[Union[int, float, str]] = None, ) -> pd.DataFrame: \"\"\" Coalesce two or more columns of data in order of column names provided. This finds the first non-missing value at each position. This method does not mutate the original DataFrame. TODO: Turn the example in this docstring into a Jupyter notebook. Example: ```python import pandas as pd import janitor as jn df = pd.DataFrame({\"A\": [1, 2, np.nan], \"B\": [np.nan, 10, np.nan], \"C\": [5, 10, 7]}) A B C 0 1.0 NaN 5 1 2.0 10.0 10 2 NaN NaN 7 df.coalesce('A', 'B', 'C', target_column_name = 'D') A B C D 0 1.0 NaN 5 1.0 1 2.0 10.0 10 2.0 2 NaN NaN 7 7.0 ``` If no target column is provided, then the first column is updated, with the null values removed: ```python df.coalesce('A', 'B', 'C') A B C 0 1.0 NaN 5 1 2.0 10.0 10 2 7.0 NaN 7 ``` If nulls remain, you can fill it with the `default_value`: ```python df = pd.DataFrame({'s1':[np.nan,np.nan,6,9,9], 's2':[np.nan,8,7,9,9]}) s1 s2 0 NaN NaN 1 NaN 8.0 2 6.0 7.0 3 9.0 9.0 4 9.0 9.0 df.coalesce('s1', 's2', target_column_name = 's3', default_value = 0) s1 s2 s3 0 NaN NaN 0.0 1 NaN 8.0 8.0 2 6.0 7.0 6.0 3 9.0 9.0 9.0 4 9.0 9.0 9.0 ``` Functional usage syntax: ```python df = coalesce(df, 'col1', 'col2', target_column_name ='col3') ``` Method chaining syntax: ```python import pandas as pd import janitor df = pd.DataFrame(...).coalesce('col1', 'col2') ``` The first example will create a new column called `col3` with values from `col2` inserted where values from `col1` are `NaN`. The second example will update the values of `col1`, since it is the first column in `column_names`. This is more syntactic diabetes! For R users, this should look familiar to `dplyr`'s `coalesce` function; for Python users, the interface should be more intuitive than the `pandas.Series.combine_first` method. :param df: A pandas DataFrame. :param column_names: A list of column names. :param target_column_name: The new column name after combining. If `None`, then the first column in `column_names` is updated, with the Null values replaced. :param default_value: A scalar to replace any remaining nulls after coalescing. :returns: A pandas DataFrame with coalesced columns. :raises ValueError: if length of `column_names` is less than 2. \"\"\" if not column_names: return df if len(column_names) < 2: raise ValueError( \"\"\" The number of columns to coalesce should be a minimum of 2. \"\"\" ) column_names = [*column_names] column_names = _select_column_names(column_names, df) if target_column_name: check(\"target_column_name\", target_column_name, [str]) if default_value: check(\"default_value\", default_value, [int, float, str]) if target_column_name is None: target_column_name = column_names[0] # bfill/ffill combo is faster than combine_first outcome = ( df.filter(column_names) .bfill(axis=\"columns\") .ffill(axis=\"columns\") .iloc[:, 0] ) if outcome.hasnans and (default_value is not None): outcome = outcome.fillna(default_value) return df.assign(**{target_column_name: outcome})","title":"coalesce()"},{"location":"api/functions/#janitor.functions.collapse_levels","text":"","title":"collapse_levels"},{"location":"api/functions/#janitor.functions.collapse_levels.collapse_levels","text":"Flatten multi-level column dataframe to a single level. This method mutates the original DataFrame. Given a DataFrame containing multi-level columns, flatten to single- level by string-joining the column labels in each level. After a groupby / aggregate operation where .agg() is passed a list of multiple aggregation functions, a multi-level DataFrame is returned with the name of the function applied in the second level. It is sometimes convenient for later indexing to flatten out this multi-level configuration back into a single level. This function does this through a simple string-joining of all the names across different levels in a single column. Method chaining syntax given two value columns [max_speed, type] : data = {\"class\": [\"bird\", \"bird\", \"bird\", \"mammal\", \"mammal\"], \"max_speed\": [389, 389, 24, 80, 21], \"type\": [\"falcon\", \"falcon\", \"parrot\", \"Lion\", \"Monkey\"]} df = ( pd.DataFrame(data) .groupby('class') .agg(['mean', 'median']) .collapse_levels(sep='_') ) Before applying .collapse_levels , the .agg operation returns a multi-level column DataFrame whose columns are (level 1, level 2) : [('class', ''), ('max_speed', 'mean'), ('max_speed', 'median'), ('type', 'mean'), ('type', 'median')] .collapse_levels then flattens the column names to: ['class', 'max_speed_mean', 'max_speed_median', 'type_mean', 'type_median'] Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required sep str String separator used to join the column level names '_' Returns: Type Description DataFrame A flattened pandas DataFrame. Source code in janitor/functions/collapse_levels.py @pf.register_dataframe_method def collapse_levels(df: pd.DataFrame, sep: str = \"_\") -> pd.DataFrame: \"\"\" Flatten multi-level column dataframe to a single level. This method mutates the original DataFrame. Given a `DataFrame` containing multi-level columns, flatten to single- level by string-joining the column labels in each level. After a `groupby` / `aggregate` operation where `.agg()` is passed a list of multiple aggregation functions, a multi-level DataFrame is returned with the name of the function applied in the second level. It is sometimes convenient for later indexing to flatten out this multi-level configuration back into a single level. This function does this through a simple string-joining of all the names across different levels in a single column. Method chaining syntax given two value columns `[max_speed, type]`: ```python data = {\"class\": [\"bird\", \"bird\", \"bird\", \"mammal\", \"mammal\"], \"max_speed\": [389, 389, 24, 80, 21], \"type\": [\"falcon\", \"falcon\", \"parrot\", \"Lion\", \"Monkey\"]} df = ( pd.DataFrame(data) .groupby('class') .agg(['mean', 'median']) .collapse_levels(sep='_') ) ``` Before applying `.collapse_levels`, the `.agg` operation returns a multi-level column DataFrame whose columns are `(level 1, level 2)`: [('class', ''), ('max_speed', 'mean'), ('max_speed', 'median'), ('type', 'mean'), ('type', 'median')] `.collapse_levels` then flattens the column names to: ['class', 'max_speed_mean', 'max_speed_median', 'type_mean', 'type_median'] :param df: A pandas DataFrame. :param sep: String separator used to join the column level names :returns: A flattened pandas DataFrame. \"\"\" check(\"sep\", sep, [str]) # if already single-level, just return the DataFrame if not isinstance(df.columns, pd.MultiIndex): return df df.columns = [ sep.join(str(el) for el in tup if str(el) != \"\") for tup in df # noqa: PD011 ] return df","title":"collapse_levels()"},{"location":"api/functions/#janitor.functions.complete","text":"","title":"complete"},{"location":"api/functions/#janitor.functions.complete.complete","text":"It is modeled after tidyr's complete function, and is a wrapper around expand_grid and pd.merge . Combinations of column names or a list/tuple of column names, or even a dictionary of column names and new values are possible. It can also handle duplicated data. MultiIndex columns are not supported. Functional usage syntax: import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.complete( df = df, column_label, (column1, column2, ...), {column1: new_values, ...}, by = label/list_of_labels ) Method chaining syntax: df = ( pd.DataFrame(...) .complete( column_label, (column1, column2, ...), {column1: new_values, ...}, by = label/list_of_labels ) Parameters: Name Type Description Default df DataFrame A pandas dataframe. required *columns This refers to the columns to be completed. It could be column labels (string type), a list/tuple of column labels, or a dictionary that pairs column labels with new values. () sort bool Sort DataFrame based on *columns. Default is False . False by Union[list, str] label or list of labels to group by. The explicit missing rows are returned per group. None Returns: Type Description DataFrame A pandas DataFrame with explicit missing rows, if any. Source code in janitor/functions/complete.py @pf.register_dataframe_method def complete( df: pd.DataFrame, *columns, sort: bool = False, by: Optional[Union[list, str]] = None, ) -> pd.DataFrame: \"\"\" It is modeled after tidyr's `complete` function, and is a wrapper around `expand_grid` and `pd.merge`. Combinations of column names or a list/tuple of column names, or even a dictionary of column names and new values are possible. It can also handle duplicated data. MultiIndex columns are not supported. Functional usage syntax: ```python import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.complete( df = df, column_label, (column1, column2, ...), {column1: new_values, ...}, by = label/list_of_labels ) ``` Method chaining syntax: ```python df = ( pd.DataFrame(...) .complete( column_label, (column1, column2, ...), {column1: new_values, ...}, by = label/list_of_labels ) ``` :param df: A pandas dataframe. :param *columns: This refers to the columns to be completed. It could be column labels (string type), a list/tuple of column labels, or a dictionary that pairs column labels with new values. :param sort: Sort DataFrame based on *columns. Default is `False`. :param by: label or list of labels to group by. The explicit missing rows are returned per group. :returns: A pandas DataFrame with explicit missing rows, if any. \"\"\" if not columns: return df df = df.copy() return _computations_complete(df, columns, sort, by)","title":"complete()"},{"location":"api/functions/#janitor.functions.concatenate_columns","text":"","title":"concatenate_columns"},{"location":"api/functions/#janitor.functions.concatenate_columns.concatenate_columns","text":"Concatenates the set of columns into a single column. Used to quickly generate an index based on a group of columns. This method mutates the original DataFrame. Functional usage syntax: df = concatenate_columns(df, column_names=['col1', 'col2'], new_column_name='id', sep='-') Method chaining syntax: df = (pd.DataFrame(...). concatenate_columns(column_names=['col1', 'col2'], new_column_name='id', sep='-')) Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_names List[Hashable] A list of columns to concatenate together. required new_column_name The name of the new column. required sep str The separator between each column's data. '-' ignore_empty bool Ignore null values if exists. True Returns: Type Description DataFrame A pandas DataFrame with concatenated columns. Exceptions: Type Description JanitorError if at least two columns are not provided within `column_names``. Source code in janitor/functions/concatenate_columns.py @pf.register_dataframe_method @deprecated_alias(columns=\"column_names\") def concatenate_columns( df: pd.DataFrame, column_names: List[Hashable], new_column_name, sep: str = \"-\", ignore_empty: bool = True, ) -> pd.DataFrame: \"\"\"Concatenates the set of columns into a single column. Used to quickly generate an index based on a group of columns. This method mutates the original DataFrame. Functional usage syntax: df = concatenate_columns(df, column_names=['col1', 'col2'], new_column_name='id', sep='-') Method chaining syntax: df = (pd.DataFrame(...). concatenate_columns(column_names=['col1', 'col2'], new_column_name='id', sep='-')) :param df: A pandas DataFrame. :param column_names: A list of columns to concatenate together. :param new_column_name: The name of the new column. :param sep: The separator between each column's data. :param ignore_empty: Ignore null values if exists. :returns: A pandas DataFrame with concatenated columns. :raises JanitorError: if at least two columns are not provided within `column_names``. \"\"\" if len(column_names) < 2: raise JanitorError(\"At least two columns must be specified\") df[new_column_name] = ( df[column_names].fillna(\"\").astype(str).agg(sep.join, axis=1) ) if ignore_empty: def remove_empty_string(x): return sep.join(x for x in x.split(sep) if x) df[new_column_name] = df[new_column_name].transform( remove_empty_string ) return df","title":"concatenate_columns()"},{"location":"api/functions/#janitor.functions.conditional_join","text":"","title":"conditional_join"},{"location":"api/functions/#janitor.functions.conditional_join.conditional_join","text":"This is a convenience function that operates similarly to pd.merge , but allows joins on inequality operators, or a combination of equi and non-equi joins. If the join is solely on equality, pd.merge function is more efficient and should be used instead. If you are interested in nearest joins, or rolling joins, pd.merge_asof covers that. There is also the IntervalIndex, which is usually more efficient for range joins, especially if the intervals do not overlap. This function returns rows, if any, where values from df meet the condition(s) for values from right . The conditions are passed in as a variable argument of tuples, where the tuple is of the form (left_on, right_on, op) ; left_on is the column label from df , right_on is the column label from right , while op is the operator. The operator can be any of == , != , <= , < , >= , > . A binary search is used to get the relevant rows for non-equi joins; this avoids a cartesian join, and makes the process less memory intensive. For equi-joins, Pandas internal merge function (a hash join) is used. The join is done only on the columns. MultiIndex columns are not supported. For non-equi joins, only numeric and date columns are supported. Only inner , left , and right joins are supported. If the columns from df and right have nothing in common, a single index column is returned; else, a MultiIndex column is returned. Functional usage syntax: import pandas as pd import janitor as jn df = pd.DataFrame(...) right = pd.DataFrame(...) df = jn.conditional_join( df, right, (col_from_df, col_from_right, join_operator), (col_from_df, col_from_right, join_operator), ..., how = 'inner' # or left/right sort_by_appearance = True # or False ) Method chaining syntax: .. code-block:: python df.conditional_join( right, (col_from_df, col_from_right, join_operator), (col_from_df, col_from_right, join_operator), ..., how = 'inner' # or left/right sort_by_appearance = True # or False ) Parameters: Name Type Description Default df DataFrame A Pandas DataFrame. required right Union[pandas.core.frame.DataFrame, pandas.core.series.Series] Named Series or DataFrame to join to. required conditions Variable argument of tuple(s) of the form (left_on, right_on, op) , where left_on is the column label from df , right_on is the column label from right , while op is the operator. The operator can be any of == , != , <= , < , >= , > . () how str Indicates the type of join to be performed. It can be one of inner , left , right . Full join is not supported. Defaults to inner . 'inner' sort_by_appearance bool Default is False . If True, values from right that meet the join condition will be returned in the final dataframe in the same order that they were before the join. False Returns: Type Description DataFrame A pandas DataFrame of the two merged Pandas objects. Source code in janitor/functions/conditional_join.py @pf.register_dataframe_method def conditional_join( df: pd.DataFrame, right: Union[pd.DataFrame, pd.Series], *conditions, how: str = \"inner\", sort_by_appearance: bool = False, ) -> pd.DataFrame: \"\"\" This is a convenience function that operates similarly to ``pd.merge``, but allows joins on inequality operators, or a combination of equi and non-equi joins. If the join is solely on equality, `pd.merge` function is more efficient and should be used instead. If you are interested in nearest joins, or rolling joins, `pd.merge_asof` covers that. There is also the IntervalIndex, which is usually more efficient for range joins, especially if the intervals do not overlap. This function returns rows, if any, where values from `df` meet the condition(s) for values from `right`. The conditions are passed in as a variable argument of tuples, where the tuple is of the form `(left_on, right_on, op)`; `left_on` is the column label from `df`, `right_on` is the column label from `right`, while `op` is the operator. The operator can be any of `==`, `!=`, `<=`, `<`, `>=`, `>`. A binary search is used to get the relevant rows for non-equi joins; this avoids a cartesian join, and makes the process less memory intensive. For equi-joins, Pandas internal merge function (a hash join) is used. The join is done only on the columns. MultiIndex columns are not supported. For non-equi joins, only numeric and date columns are supported. Only `inner`, `left`, and `right` joins are supported. If the columns from `df` and `right` have nothing in common, a single index column is returned; else, a MultiIndex column is returned. Functional usage syntax: ```python import pandas as pd import janitor as jn df = pd.DataFrame(...) right = pd.DataFrame(...) df = jn.conditional_join( df, right, (col_from_df, col_from_right, join_operator), (col_from_df, col_from_right, join_operator), ..., how = 'inner' # or left/right sort_by_appearance = True # or False ) ``` Method chaining syntax: .. code-block:: python df.conditional_join( right, (col_from_df, col_from_right, join_operator), (col_from_df, col_from_right, join_operator), ..., how = 'inner' # or left/right sort_by_appearance = True # or False ) :param df: A Pandas DataFrame. :param right: Named Series or DataFrame to join to. :param conditions: Variable argument of tuple(s) of the form `(left_on, right_on, op)`, where `left_on` is the column label from `df`, `right_on` is the column label from `right`, while `op` is the operator. The operator can be any of `==`, `!=`, `<=`, `<`, `>=`, `>`. :param how: Indicates the type of join to be performed. It can be one of `inner`, `left`, `right`. Full join is not supported. Defaults to `inner`. :param sort_by_appearance: Default is `False`. If True, values from `right` that meet the join condition will be returned in the final dataframe in the same order that they were before the join. :returns: A pandas DataFrame of the two merged Pandas objects. \"\"\" return _conditional_join_compute( df, right, conditions, how, sort_by_appearance )","title":"conditional_join()"},{"location":"api/functions/#janitor.functions.convert_date","text":"","title":"convert_date"},{"location":"api/functions/#janitor.functions.convert_date.convert_excel_date","text":"Convert Excel's serial date format into Python datetime format. This method mutates the original DataFrame. Implementation is also from Stack Overflow Functional usage syntax: df = convert_excel_date(df, column_name='date') Method chaining syntax: import pandas as pd import janitor df = pd.DataFrame(...).convert_excel_date('date') Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable A column name. required Returns: Type Description DataFrame A pandas DataFrame with corrected dates. Exceptions: Type Description ValueError if there are non numeric values in the column. Source code in janitor/functions/convert_date.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def convert_excel_date( df: pd.DataFrame, column_name: Hashable ) -> pd.DataFrame: \"\"\" Convert Excel's serial date format into Python datetime format. This method mutates the original DataFrame. Implementation is also from [Stack Overflow](https://stackoverflow.com/questions/38454403/convert-excel-style-date-with-pandas) Functional usage syntax: ```python df = convert_excel_date(df, column_name='date') ``` Method chaining syntax: ```python import pandas as pd import janitor df = pd.DataFrame(...).convert_excel_date('date') ``` :param df: A pandas DataFrame. :param column_name: A column name. :returns: A pandas DataFrame with corrected dates. :raises ValueError: if there are non numeric values in the column. \"\"\" # noqa: E501 if not is_numeric_dtype(df[column_name]): raise ValueError( \"There are non-numeric values in the column. \\ All values must be numeric\" ) df[column_name] = pd.TimedeltaIndex( df[column_name], unit=\"d\" ) + dt.datetime( 1899, 12, 30 ) # noqa: W503 return df","title":"convert_excel_date()"},{"location":"api/functions/#janitor.functions.convert_date.convert_matlab_date","text":"Convert Matlab's serial date number into Python datetime format. Implementation is also from Stack Overflow This method mutates the original DataFrame. Functional usage syntax: df = convert_matlab_date(df, column_name='date') Method chaining syntax: import pandas as pd import janitor df = pd.DataFrame(...).convert_matlab_date('date') Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable A column name. required Returns: Type Description DataFrame A pandas DataFrame with corrected dates. Source code in janitor/functions/convert_date.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def convert_matlab_date( df: pd.DataFrame, column_name: Hashable ) -> pd.DataFrame: \"\"\" Convert Matlab's serial date number into Python datetime format. Implementation is also from [Stack Overflow](https://stackoverflow.com/questions/13965740/converting-matlabs-datenum-format-to-python) This method mutates the original DataFrame. Functional usage syntax: ```python df = convert_matlab_date(df, column_name='date') ``` Method chaining syntax: ```python import pandas as pd import janitor df = pd.DataFrame(...).convert_matlab_date('date') ``` :param df: A pandas DataFrame. :param column_name: A column name. :returns: A pandas DataFrame with corrected dates. \"\"\" # noqa: E501 days = pd.Series([dt.timedelta(v % 1) for v in df[column_name]]) df[column_name] = ( df[column_name].astype(int).apply(dt.datetime.fromordinal) + days - dt.timedelta(days=366) ) return df","title":"convert_matlab_date()"},{"location":"api/functions/#janitor.functions.convert_date.convert_unix_date","text":"Convert unix epoch time into Python datetime format. Note that this ignores local tz and convert all timestamps to naive datetime based on UTC! This method mutates the original DataFrame. Functional usage syntax: df = convert_unix_date(df, column_name='date') Method chaining syntax: import pandas as pd import janitor df = pd.DataFrame(...).convert_unix_date('date') Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable A column name. required Returns: Type Description DataFrame A pandas DataFrame with corrected dates. Source code in janitor/functions/convert_date.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def convert_unix_date(df: pd.DataFrame, column_name: Hashable) -> pd.DataFrame: \"\"\" Convert unix epoch time into Python datetime format. Note that this ignores local tz and convert all timestamps to naive datetime based on UTC! This method mutates the original DataFrame. Functional usage syntax: ```python df = convert_unix_date(df, column_name='date') ``` Method chaining syntax: ```python import pandas as pd import janitor df = pd.DataFrame(...).convert_unix_date('date') ``` :param df: A pandas DataFrame. :param column_name: A column name. :returns: A pandas DataFrame with corrected dates. \"\"\" try: df[column_name] = pd.to_datetime(df[column_name], unit=\"s\") except OutOfBoundsDatetime: # Indicates time is in milliseconds. df[column_name] = pd.to_datetime(df[column_name], unit=\"ms\") return df","title":"convert_unix_date()"},{"location":"api/functions/#janitor.functions.count_cumulative_unique","text":"","title":"count_cumulative_unique"},{"location":"api/functions/#janitor.functions.count_cumulative_unique.count_cumulative_unique","text":"Generates a running total of cumulative unique values in a given column. Functional usage syntax: import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.functions.count_cumulative_unique( df=df, column_name='animals', dest_column_name='animals_unique_count', case_sensitive=True ) Method chaining usage example: import pandas as pd import janitor df = pd.DataFrame(...) df = df.count_cumulative_unique( column_name='animals', dest_column_name='animals_unique_count', case_sensitive=True ) A new column will be created containing a running count of unique values in the specified column. If case_sensitive is True , then the case of any letters will matter (i.e., a != A ); otherwise, the case of any letters will not matter. This method mutates the original DataFrame. Parameters: Name Type Description Default df DataFrame A pandas dataframe. required column_name Hashable Name of the column containing values from which a running count of unique values will be created. required dest_column_name str The name of the new column containing the cumulative count of unique values that will be created. required case_sensitive bool Whether or not uppercase and lowercase letters will be considered equal (e.g., 'A' != 'a' if True ). True Returns: Type Description DataFrame A pandas DataFrame with a new column containing a cumulative count of unique values from another column. Source code in janitor/functions/count_cumulative_unique.py @pf.register_dataframe_method def count_cumulative_unique( df: pd.DataFrame, column_name: Hashable, dest_column_name: str, case_sensitive: bool = True, ) -> pd.DataFrame: \"\"\"Generates a running total of cumulative unique values in a given column. Functional usage syntax: ```python import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.functions.count_cumulative_unique( df=df, column_name='animals', dest_column_name='animals_unique_count', case_sensitive=True ) ``` Method chaining usage example: ```python import pandas as pd import janitor df = pd.DataFrame(...) df = df.count_cumulative_unique( column_name='animals', dest_column_name='animals_unique_count', case_sensitive=True ) ``` A new column will be created containing a running count of unique values in the specified column. If `case_sensitive` is `True`, then the case of any letters will matter (i.e., `a != A`); otherwise, the case of any letters will not matter. This method mutates the original DataFrame. :param df: A pandas dataframe. :param column_name: Name of the column containing values from which a running count of unique values will be created. :param dest_column_name: The name of the new column containing the cumulative count of unique values that will be created. :param case_sensitive: Whether or not uppercase and lowercase letters will be considered equal (e.g., 'A' != 'a' if `True`). :returns: A pandas DataFrame with a new column containing a cumulative count of unique values from another column. \"\"\" if not case_sensitive: # Make it so that the the same uppercase and lowercase # letter are treated as one unique value df[column_name] = df[column_name].astype(str).map(str.lower) df[dest_column_name] = ( ( df[[column_name]] .drop_duplicates() .assign(dummyabcxyz=1) .dummyabcxyz.cumsum() ) .reindex(df.index) .ffill() .astype(int) ) return df","title":"count_cumulative_unique()"},{"location":"api/functions/#janitor.functions.currency_column_to_numeric","text":"","title":"currency_column_to_numeric"},{"location":"api/functions/#janitor.functions.currency_column_to_numeric.currency_column_to_numeric","text":"Convert currency column to numeric. This method does not mutate the original DataFrame. This method allows one to take a column containing currency values, inadvertently imported as a string, and cast it as a float. This is usually the case when reading CSV files that were modified in Excel. Empty strings (i.e. '' ) are retained as NaN values. Parameters: Name Type Description Default df DataFrame The DataFrame required column_name The column to modify required cleaning_style Optional[str] What style of cleaning to perform. If None, standard cleaning is applied. Options are: * 'accounting': Replaces numbers in parentheses with negatives, removes commas. None cast_non_numeric Optional[dict] A dict of how to coerce certain strings. For example, if there are values of 'REORDER' in the DataFrame, {'REORDER': 0} will cast all instances of 'REORDER' to 0. None fill_all_non_numeric Union[float, int] Similar to cast_non_numeric , but fills all strings to the same value. For example, fill_all_non_numeric=1, will make everything that doesn't coerce to a currency 1. None remove_non_numeric bool Will remove rows of a DataFrame that contain non-numeric values in the column_name column. Defaults to False . False Returns: Type Description DataFrame A pandas DataFrame. Source code in janitor/functions/currency_column_to_numeric.py @pf.register_dataframe_method @deprecated_alias(col_name=\"column_name\", type=\"cleaning_style\") def currency_column_to_numeric( df: pd.DataFrame, column_name, cleaning_style: Optional[str] = None, cast_non_numeric: Optional[dict] = None, fill_all_non_numeric: Optional[Union[float, int]] = None, remove_non_numeric: bool = False, ) -> pd.DataFrame: \"\"\"Convert currency column to numeric. This method does not mutate the original DataFrame. This method allows one to take a column containing currency values, inadvertently imported as a string, and cast it as a float. This is usually the case when reading CSV files that were modified in Excel. Empty strings (i.e. `''`) are retained as `NaN` values. :param df: The DataFrame :param column_name: The column to modify :param cleaning_style: What style of cleaning to perform. If None, standard cleaning is applied. Options are: * 'accounting': Replaces numbers in parentheses with negatives, removes commas. :param cast_non_numeric: A dict of how to coerce certain strings. For example, if there are values of 'REORDER' in the DataFrame, {'REORDER': 0} will cast all instances of 'REORDER' to 0. :param fill_all_non_numeric: Similar to `cast_non_numeric`, but fills all strings to the same value. For example, fill_all_non_numeric=1, will make everything that doesn't coerce to a currency 1. :param remove_non_numeric: Will remove rows of a DataFrame that contain non-numeric values in the `column_name` column. Defaults to `False`. :returns: A pandas DataFrame. \"\"\" check(\"column_name\", column_name, [str]) column_series = df[column_name] if cleaning_style == \"accounting\": df.loc[:, column_name] = df[column_name].apply( _clean_accounting_column ) return df if cast_non_numeric: check(\"cast_non_numeric\", cast_non_numeric, [dict]) _make_cc_patrial = partial( _currency_column_to_numeric, cast_non_numeric=cast_non_numeric ) column_series = column_series.apply(_make_cc_patrial) if remove_non_numeric: df = df.loc[column_series != \"\", :] # _replace_empty_string_with_none is applied here after the check on # remove_non_numeric since \"\" is our indicator that a string was coerced # in the original column column_series = _replace_empty_string_with_none(column_series) if fill_all_non_numeric is not None: check(\"fill_all_non_numeric\", fill_all_non_numeric, [int, float]) column_series = column_series.fillna(fill_all_non_numeric) column_series = _replace_original_empty_string_with_none(column_series) df = df.assign(**{column_name: pd.to_numeric(column_series)}) return df","title":"currency_column_to_numeric()"},{"location":"api/functions/#janitor.functions.deconcatenate_column","text":"","title":"deconcatenate_column"},{"location":"api/functions/#janitor.functions.deconcatenate_column.deconcatenate_column","text":"De-concatenates a single column into multiple columns. The column to de-concatenate can be either a collection (list, tuple, ...) which can be separated out with pd.Series.tolist()``, or a string to slice based on sep``. To determine this behaviour automatically, the first element in the column specified is inspected. If it is a string, then sep must be specified. Else, the function assumes that it is an iterable type (e.g. list or `tuple``), and will attempt to deconcatenate by splitting the list. Given a column with string values, this is the inverse of the concatenate_columns function. Used to quickly split columns out of a single column. The keyword argument preserve_position`` takes True or False boolean that controls whether the new_column_names will take the original position of the to-be-deconcatenated `column_name : When preserve_position=False (default), df.columns change from [..., column_name, ...] to [..., column_name, ..., new_column_names] . In other words, the deconcatenated new columns are appended to the right of the original dataframe and the original column_name is NOT dropped. When preserve_position=True , df.column change from [..., column_name, ...] to [..., new_column_names, ...] . In other words, the deconcatenated new column will REPLACE the original column_name at its original position, and column_name itself is dropped. The keyword argument autoname accepts a base string and then automatically creates numbered column names based off the base string. For example, if col is passed in as the argument to autoname``, and 4 columns are created, then the resulting columns will be named col1, col2, col3, col4``. Numbering is always 1-indexed, not 0-indexed, in order to make the column names human-friendly. This method does not mutate the original DataFrame. Functional usage syntax: df = deconcatenate_column( df, column_name='id', new_column_names=['col1', 'col2'], sep='-', preserve_position=True ) Method chaining syntax: df = (pd.DataFrame(...). deconcatenate_column( column_name='id', new_column_names=['col1', 'col2'], sep='-', preserve_position=True )) Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable The column to split. required sep Optional[str] The separator delimiting the column's data. None new_column_names Union[List[str], Tuple[str]] A list of new column names post-splitting. None autoname str A base name for automatically naming the new columns. Takes precedence over new_column_names if both are provided. None preserve_position bool Boolean for whether or not to preserve original position of the column upon de-concatenation, default to False False Returns: Type Description DataFrame A pandas DataFrame with a deconcatenated column. Exceptions: Type Description ValueError if column_name is not present in the DataFrame. ValueError if sep is not provided and the column values are of type `str``. ValueError if either new_column_names or `autoname`` is not supplied. JanitorError if incorrect number of names is provided within `new_column_names``. Source code in janitor/functions/deconcatenate_column.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def deconcatenate_column( df: pd.DataFrame, column_name: Hashable, sep: Optional[str] = None, new_column_names: Optional[Union[List[str], Tuple[str]]] = None, autoname: str = None, preserve_position: bool = False, ) -> pd.DataFrame: \"\"\"De-concatenates a single column into multiple columns. The column to de-concatenate can be either a collection (list, tuple, ...) which can be separated out with `pd.Series.tolist()``, or a string to slice based on `sep``. To determine this behaviour automatically, the first element in the column specified is inspected. If it is a string, then `sep` must be specified. Else, the function assumes that it is an iterable type (e.g. `list` or `tuple``), and will attempt to deconcatenate by splitting the list. Given a column with string values, this is the inverse of the `concatenate_columns` function. Used to quickly split columns out of a single column. The keyword argument `preserve_position`` takes `True` or `False` boolean that controls whether the `new_column_names`` will take the original position of the to-be-deconcatenated `column_name``: - When `preserve_position=False` (default), `df.columns` change from `[..., column_name, ...]` to `[..., column_name, ..., new_column_names]`. In other words, the deconcatenated new columns are appended to the right of the original dataframe and the original `column_name` is NOT dropped. - When `preserve_position=True`, `df.column` change from `[..., column_name, ...]` to `[..., new_column_names, ...]`. In other words, the deconcatenated new column will REPLACE the original `column_name` at its original position, and `column_name` itself is dropped. The keyword argument `autoname` accepts a base string and then automatically creates numbered column names based off the base string. For example, if `col` is passed in as the argument to `autoname``, and 4 columns are created, then the resulting columns will be named `col1, col2, col3, col4``. Numbering is always 1-indexed, not 0-indexed, in order to make the column names human-friendly. This method does not mutate the original DataFrame. Functional usage syntax: df = deconcatenate_column( df, column_name='id', new_column_names=['col1', 'col2'], sep='-', preserve_position=True ) Method chaining syntax: df = (pd.DataFrame(...). deconcatenate_column( column_name='id', new_column_names=['col1', 'col2'], sep='-', preserve_position=True )) :param df: A pandas DataFrame. :param column_name: The column to split. :param sep: The separator delimiting the column's data. :param new_column_names: A list of new column names post-splitting. :param autoname: A base name for automatically naming the new columns. Takes precedence over `new_column_names` if both are provided. :param preserve_position: Boolean for whether or not to preserve original position of the column upon de-concatenation, default to False :returns: A pandas DataFrame with a deconcatenated column. :raises ValueError: if `column_name` is not present in the DataFrame. :raises ValueError: if `sep` is not provided and the column values are of type `str``. :raises ValueError: if either `new_column_names` or `autoname`` is not supplied. :raises JanitorError: if incorrect number of names is provided within `new_column_names``. \"\"\" if column_name not in df.columns: raise ValueError(f\"column name {column_name} not present in DataFrame\") if isinstance(df[column_name].iloc[0], str): if sep is None: raise ValueError( \"`sep` must be specified if the column values \" \"are of type `str`.\" ) df_deconcat = df[column_name].str.split(sep, expand=True) else: df_deconcat = pd.DataFrame( df[column_name].to_list(), columns=new_column_names, index=df.index ) if new_column_names is None and autoname is None: raise ValueError( \"One of `new_column_names` or `autoname` must be supplied.\" ) if autoname: new_column_names = [ f\"{autoname}{i}\" for i in range(1, df_deconcat.shape[1] + 1) ] if not len(new_column_names) == df_deconcat.shape[1]: raise JanitorError( f\"you need to provide {len(df_deconcat.shape[1])} names \" \"to `new_column_names`\" ) df_deconcat.columns = new_column_names df_new = pd.concat([df, df_deconcat], axis=1) if preserve_position: df_original = df.copy() cols = list(df_original.columns) index_original = cols.index(column_name) for i, col_new in enumerate(new_column_names): cols.insert(index_original + i, col_new) df_new = df_new.select_columns(cols).drop(columns=column_name) return df_new","title":"deconcatenate_column()"},{"location":"api/functions/#janitor.functions.drop_constant_columns","text":"","title":"drop_constant_columns"},{"location":"api/functions/#janitor.functions.drop_constant_columns.drop_constant_columns","text":"Finds and drops the constant columns from a Pandas DataFrame. This method does not mutate the original DataFrame. Functional usage syntax: import pandas as pd import janitor as jn data_dict = { \"a\": [1, 1, 1] * 3, \"Bell__Chart\": [1, 2, 3] * 3, \"decorated-elephant\": [1, 1, 1] * 3, \"animals\": [\"rabbit\", \"leopard\", \"lion\"] * 3, \"cities\": [\"Cambridge\", \"Shanghai\", \"Basel\"] * 3 } df = pd.DataFrame(data_dict) df = jn.functions.drop_constant_columns(df) Method chaining usage example: import pandas as pd import janitor df = pd.DataFrame(...) df = df.drop_constant_columns() Parameters: Name Type Description Default df DataFrame Input Pandas DataFrame required Returns: Type Description DataFrame The Pandas DataFrame with the constant columns dropped. Source code in janitor/functions/drop_constant_columns.py @pf.register_dataframe_method def drop_constant_columns( df: pd.DataFrame, ) -> pd.DataFrame: \"\"\" Finds and drops the constant columns from a Pandas DataFrame. This method does not mutate the original DataFrame. Functional usage syntax: ```python import pandas as pd import janitor as jn data_dict = { \"a\": [1, 1, 1] * 3, \"Bell__Chart\": [1, 2, 3] * 3, \"decorated-elephant\": [1, 1, 1] * 3, \"animals\": [\"rabbit\", \"leopard\", \"lion\"] * 3, \"cities\": [\"Cambridge\", \"Shanghai\", \"Basel\"] * 3 } df = pd.DataFrame(data_dict) df = jn.functions.drop_constant_columns(df) ``` Method chaining usage example: ```python import pandas as pd import janitor df = pd.DataFrame(...) df = df.drop_constant_columns() ``` :param df: Input Pandas DataFrame :returns: The Pandas DataFrame with the constant columns dropped. \"\"\" # Find the constant columns constant_columns = [] for col in df.columns: if len(df[col].unique()) == 1: constant_columns.append(col) # Drop constant columns from df and return it return df.drop(labels=constant_columns, axis=1)","title":"drop_constant_columns()"},{"location":"api/functions/#janitor.functions.drop_duplicate_columns","text":"","title":"drop_duplicate_columns"},{"location":"api/functions/#janitor.functions.drop_duplicate_columns.drop_duplicate_columns","text":"Remove a duplicated column specified by column_name, its index. This method does not mutate the original DataFrame. Column order 0 is to remove the first column, order 1 is to remove the second column, and etc The corresponding tidyverse R's library is: select(-<column_name>_<nth_index + 1>) Method chaining syntax: df = pd.DataFrame({ \"a\": range(10), \"b\": range(10), \"A\": range(10, 20), \"a*\": range(20, 30), }).clean_names(remove_special=True) # remove a duplicated second 'a' column df.drop_duplicate_columns(column_name=\"a\", nth_index=1) Parameters: Name Type Description Default df DataFrame A pandas DataFrame required column_name Hashable Column to be removed required nth_index int Among the duplicated columns, select the nth column to drop. 0 Returns: Type Description DataFrame A pandas DataFrame Source code in janitor/functions/drop_duplicate_columns.py @pf.register_dataframe_method def drop_duplicate_columns( df: pd.DataFrame, column_name: Hashable, nth_index: int = 0 ) -> pd.DataFrame: \"\"\"Remove a duplicated column specified by column_name, its index. This method does not mutate the original DataFrame. Column order 0 is to remove the first column, order 1 is to remove the second column, and etc The corresponding tidyverse R's library is: `select(-<column_name>_<nth_index + 1>)` Method chaining syntax: df = pd.DataFrame({ \"a\": range(10), \"b\": range(10), \"A\": range(10, 20), \"a*\": range(20, 30), }).clean_names(remove_special=True) # remove a duplicated second 'a' column df.drop_duplicate_columns(column_name=\"a\", nth_index=1) :param df: A pandas DataFrame :param column_name: Column to be removed :param nth_index: Among the duplicated columns, select the nth column to drop. :return: A pandas DataFrame \"\"\" cols = df.columns.to_list() col_indexes = [ col_idx for col_idx, col_name in enumerate(cols) if col_name == column_name ] # given that a column could be duplicated, # user could opt based on its order removed_col_idx = col_indexes[nth_index] # get the column indexes without column that is being removed filtered_cols = [ c_i for c_i, c_v in enumerate(cols) if c_i != removed_col_idx ] return df.iloc[:, filtered_cols]","title":"drop_duplicate_columns()"},{"location":"api/functions/#janitor.functions.dropnotnull","text":"","title":"dropnotnull"},{"location":"api/functions/#janitor.functions.dropnotnull.dropnotnull","text":"Drop rows that do not have null values in the given column. This method does not mutate the original DataFrame. Example usage: df = pd.DataFrame(...).dropnotnull('column3') Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable The column name to drop rows from. required Returns: Type Description DataFrame A pandas DataFrame with dropped rows. Source code in janitor/functions/dropnotnull.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def dropnotnull(df: pd.DataFrame, column_name: Hashable) -> pd.DataFrame: \"\"\" Drop rows that do not have null values in the given column. This method does not mutate the original DataFrame. Example usage: ```python df = pd.DataFrame(...).dropnotnull('column3') ``` :param df: A pandas DataFrame. :param column_name: The column name to drop rows from. :returns: A pandas DataFrame with dropped rows. \"\"\" return df[pd.isna(df[column_name])]","title":"dropnotnull()"},{"location":"api/functions/#janitor.functions.encode_categorical","text":"","title":"encode_categorical"},{"location":"api/functions/#janitor.functions.encode_categorical.encode_categorical","text":"Encode the specified columns with Pandas' category dtype . It is syntactic sugar around pd.Categorical . This method does not mutate the original DataFrame. Note: In versions < 0.20.11, this method mutates the original DataFrame. TODO: The big chunk of examples below should be moved into a Jupyter notebook. This will keep the docstring consistent and to-the-point. Examples: col1 col2 col3 0 2.0 a 2020-01-01 1 1.0 b 2020-01-02 2 3.0 c 2020-01-03 3 1.0 d 2020-01-04 4 NaN a 2020-01-05 df.dtypes col1 float64 col2 object col3 datetime64[ns] dtype: object Specific columns can be converted to category type: df = ( pd.DataFrame(...) .encode_categorical( column_names=['col1', 'col2', 'col3'] ) ) df.dtypes col1 category col2 category col3 category dtype: object Note that for the code above, the categories were inferred from the columns, and is unordered: df['col3'] 0 2020-01-01 1 2020-01-02 2 2020-01-03 3 2020-01-04 4 2020-01-05 Name: col3, dtype: category Categories (5, datetime64[ns]): [2020-01-01, 2020-01-02, 2020-01-03, 2020-01-04, 2020-01-05] Explicit categories can be provided, and ordered via the `kwargs`` parameter: df = (pd.DataFrame(...) .encode_categorical( col1 = ([3, 2, 1, 4], \"appearance\"), col2 = (['a','d','c','b'], \"sort\") ) ) df['col1'] 0 2 1 1 2 3 3 1 4 NaN Name: col1, dtype: category Categories (4, int64): [3 < 2 < 1 < 4] df['col2'] 0 a 1 b 2 c 3 d 4 a Name: col2, dtype: category Categories (4, object): [a < b < c < d] When the order parameter is \"appearance\", the categories argument is used as-is; if the order is \"sort\", the categories argument is sorted in ascending order; if order is `None``, then the categories argument is applied unordered. A User Warning will be generated if some or all of the unique values in the column are not present in the provided categories argument. df = (pd.DataFrame(...) .encode_categorical( col1 = ( categories = [4, 5, 6], order = \"appearance\" ) ) UserWarning: None of the values in col1 are in [4, 5, 6]; this might create nulls for all your values in the new categorical column. df['col1'] 0 NaN 1 NaN 2 NaN 3 NaN 4 NaN Name: col1, dtype: category Categories (3, int64): [4 < 5 < 6] .. note:: if categories is None in the kwargs tuple, then the values for categories are inferred from the column; if order is None, then the values for categories are applied unordered. .. note:: column_names and kwargs parameters cannot be used at the same time. Functional usage syntax: ```python import pandas as pd import janitor as jn With `column_names``:: categorical_cols = ['col1', 'col2', 'col4'] df = jn.encode_categorical( df, columns = categorical_cols) # one way With `kwargs``:: df = jn.encode_categorical( df, col1 = (categories, order), col2 = (categories = [values], order=\"sort\" # or \"appearance\" or None ) Method chaining syntax: With `column_names``:: categorical_cols = ['col1', 'col2', 'col4'] df = (pd.DataFrame(...) .encode_categorical(columns=categorical_cols) ) With `kwargs``:: df = ( pd.DataFrame(...) .encode_categorical( col1 = (categories, order), col2 = (categories = [values]/None, order=\"sort\" # or \"appearance\" or None ) ) Parameters: Name Type Description Default df DataFrame The pandas DataFrame object. required column_names Union[str, Iterable[str], Hashable] A column name or an iterable (list or tuple) of column names. None kwargs A pairing of column name to a tuple of ( categories , order ). This is useful in creating categorical columns that are ordered, or if the user needs to explicitly specify the categories. {} Returns: Type Description DataFrame A pandas DataFrame. Exceptions: Type Description ValueError if both column_names and kwargs are provided. Source code in janitor/functions/encode_categorical.py @pf.register_dataframe_method @deprecated_alias(columns=\"column_names\") def encode_categorical( df: pd.DataFrame, column_names: Union[str, Iterable[str], Hashable] = None, **kwargs, ) -> pd.DataFrame: \"\"\"Encode the specified columns with Pandas' [category dtype][cat]. [cat]: http://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html It is syntactic sugar around `pd.Categorical`. This method does not mutate the original DataFrame. Note: In versions < 0.20.11, this method mutates the original DataFrame. TODO: The big chunk of examples below should be moved into a Jupyter notebook. This will keep the docstring consistent and to-the-point. Examples: ```python col1 col2 col3 0 2.0 a 2020-01-01 1 1.0 b 2020-01-02 2 3.0 c 2020-01-03 3 1.0 d 2020-01-04 4 NaN a 2020-01-05 df.dtypes col1 float64 col2 object col3 datetime64[ns] dtype: object ``` Specific columns can be converted to category type: ```python df = ( pd.DataFrame(...) .encode_categorical( column_names=['col1', 'col2', 'col3'] ) ) df.dtypes col1 category col2 category col3 category dtype: object ``` Note that for the code above, the categories were inferred from the columns, and is unordered: df['col3'] 0 2020-01-01 1 2020-01-02 2 2020-01-03 3 2020-01-04 4 2020-01-05 Name: col3, dtype: category Categories (5, datetime64[ns]): [2020-01-01, 2020-01-02, 2020-01-03, 2020-01-04, 2020-01-05] Explicit categories can be provided, and ordered via the `kwargs`` parameter: df = (pd.DataFrame(...) .encode_categorical( col1 = ([3, 2, 1, 4], \"appearance\"), col2 = (['a','d','c','b'], \"sort\") ) ) df['col1'] 0 2 1 1 2 3 3 1 4 NaN Name: col1, dtype: category Categories (4, int64): [3 < 2 < 1 < 4] df['col2'] 0 a 1 b 2 c 3 d 4 a Name: col2, dtype: category Categories (4, object): [a < b < c < d] When the `order` parameter is \"appearance\", the categories argument is used as-is; if the `order` is \"sort\", the categories argument is sorted in ascending order; if `order` is `None``, then the categories argument is applied unordered. A User Warning will be generated if some or all of the unique values in the column are not present in the provided `categories` argument. ```python df = (pd.DataFrame(...) .encode_categorical( col1 = ( categories = [4, 5, 6], order = \"appearance\" ) ) UserWarning: None of the values in col1 are in [4, 5, 6]; this might create nulls for all your values in the new categorical column. df['col1'] 0 NaN 1 NaN 2 NaN 3 NaN 4 NaN Name: col1, dtype: category Categories (3, int64): [4 < 5 < 6] ``` .. note:: if `categories` is None in the `kwargs` tuple, then the values for `categories` are inferred from the column; if `order` is None, then the values for categories are applied unordered. .. note:: `column_names` and `kwargs` parameters cannot be used at the same time. Functional usage syntax: ```python import pandas as pd import janitor as jn - With `column_names``:: categorical_cols = ['col1', 'col2', 'col4'] df = jn.encode_categorical( df, columns = categorical_cols) # one way - With `kwargs``:: df = jn.encode_categorical( df, col1 = (categories, order), col2 = (categories = [values], order=\"sort\" # or \"appearance\" or None ) Method chaining syntax: - With `column_names``:: categorical_cols = ['col1', 'col2', 'col4'] df = (pd.DataFrame(...) .encode_categorical(columns=categorical_cols) ) - With `kwargs``:: df = ( pd.DataFrame(...) .encode_categorical( col1 = (categories, order), col2 = (categories = [values]/None, order=\"sort\" # or \"appearance\" or None ) ) :param df: The pandas DataFrame object. :param column_names: A column name or an iterable (list or tuple) of column names. :param kwargs: A pairing of column name to a tuple of (`categories`, `order`). This is useful in creating categorical columns that are ordered, or if the user needs to explicitly specify the categories. :returns: A pandas DataFrame. :raises ValueError: if both ``column_names`` and ``kwargs`` are provided. \"\"\" # noqa: E501 if all((column_names, kwargs)): raise ValueError( \"\"\" Only one of `column_names` or `kwargs` can be provided. \"\"\" ) # column_names deal with only category dtype (unordered) # kwargs takes care of scenarios where user wants an ordered category # or user supplies specific categories to create the categorical if column_names is not None: check(\"column_names\", column_names, [list, tuple, Hashable]) if isinstance(column_names, (list, tuple)): check_column(df, column_names) dtypes = {col: \"category\" for col in column_names} return df.astype(dtypes) if isinstance(column_names, Hashable): check_column(df, [column_names]) return df.astype({column_names: \"category\"}) return _computations_as_categorical(df, **kwargs)","title":"encode_categorical()"},{"location":"api/functions/#janitor.functions.expand_column","text":"","title":"expand_column"},{"location":"api/functions/#janitor.functions.expand_column.expand_column","text":"Expand a categorical column with multiple labels into dummy-coded columns. Super sugary syntax that wraps :py:meth: pandas.Series.str.get_dummies . This method does not mutate the original DataFrame. Functional usage syntax: df = expand_column( df, column_name='col_name', sep=', ' # note space in sep ) Method chaining syntax: import pandas as pd import janitor df = ( pd.DataFrame(...) .expand_column( column_name='col_name', sep=', ' ) ) Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable Which column to expand. required sep str The delimiter, same to :py:meth: ~pandas.Series.str.get_dummies 's sep , default as | . '|' concat bool Whether to return the expanded column concatenated to the original dataframe ( concat=True ), or to return it standalone ( concat=False ). True Returns: Type Description DataFrame A pandas DataFrame with an expanded column. Source code in janitor/functions/expand_column.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def expand_column( df: pd.DataFrame, column_name: Hashable, sep: str = \"|\", concat: bool = True, ) -> pd.DataFrame: \"\"\"Expand a categorical column with multiple labels into dummy-coded columns. Super sugary syntax that wraps :py:meth:`pandas.Series.str.get_dummies`. This method does not mutate the original DataFrame. Functional usage syntax: df = expand_column( df, column_name='col_name', sep=', ' # note space in sep ) Method chaining syntax: import pandas as pd import janitor df = ( pd.DataFrame(...) .expand_column( column_name='col_name', sep=', ' ) ) :param df: A pandas DataFrame. :param column_name: Which column to expand. :param sep: The delimiter, same to :py:meth:`~pandas.Series.str.get_dummies`'s `sep`, default as `|`. :param concat: Whether to return the expanded column concatenated to the original dataframe (`concat=True`), or to return it standalone (`concat=False`). :returns: A pandas DataFrame with an expanded column. \"\"\" expanded_df = df[column_name].str.get_dummies(sep=sep) if concat: df = df.join(expanded_df) return df return expanded_df","title":"expand_column()"},{"location":"api/functions/#janitor.functions.expand_grid","text":"","title":"expand_grid"},{"location":"api/functions/#janitor.functions.expand_grid.expand_grid","text":"Creates a DataFrame from a cartesian combination of all inputs. It is not restricted to DataFrame; it can work with any list-like structure that is 1 or 2 dimensional. If method-chaining to a DataFrame, a string argument to df_key parameter must be provided. Data types are preserved in this function, including Pandas' extension array dtypes. The output will always be a DataFrame, usually a MultiIndex, with the keys of the others dictionary serving as the top level columns. If a DataFrame with MultiIndex columns is part of the arguments in others , the columns are flattened, before the final cartesian DataFrame is generated. If a Pandas Series/DataFrame is passed, and has a labeled index, or a MultiIndex index, the index is discarded; the final DataFrame will have a RangeIndex. The MultiIndexed DataFrame can be flattened using pyjanitor's collapse_levels method; the user can also decide to drop any of the levels, via Pandas' droplevel method. Functional usage syntax: import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.expand_grid(df=df, df_key=\"...\", others={...}) Method-chaining usage syntax: import pandas as pd import janitor as jn df = pd.DataFrame(...).expand_grid(df_key=\"bla\",others={...}) Usage independent of a DataFrame .. code-block:: python ```python import pandas as pd from janitor import expand_grid df = expand_grid(others = {\"x\":range(1,4), \"y\":[1,2]}) Parameters: Name Type Description Default df Optional[pandas.core.frame.DataFrame] A pandas DataFrame. None df_key Optional[str] name of key for the dataframe. It becomes part of the column names of the dataframe. None others Optional[Dict] A dictionary that contains the data to be combined with the dataframe. If no dataframe exists, all inputs in others will be combined to create a DataFrame. None Returns: Type Description DataFrame A pandas DataFrame of the cartesian product. Exceptions: Type Description KeyError if there is a DataFrame and df_key is not provided. Source code in janitor/functions/expand_grid.py @pf.register_dataframe_method def expand_grid( df: Optional[pd.DataFrame] = None, df_key: Optional[str] = None, *, others: Optional[Dict] = None, ) -> pd.DataFrame: \"\"\" Creates a DataFrame from a cartesian combination of all inputs. It is not restricted to DataFrame; it can work with any list-like structure that is 1 or 2 dimensional. If method-chaining to a DataFrame, a string argument to `df_key` parameter must be provided. Data types are preserved in this function, including Pandas' extension array dtypes. The output will always be a DataFrame, usually a MultiIndex, with the keys of the `others` dictionary serving as the top level columns. If a DataFrame with MultiIndex columns is part of the arguments in `others`, the columns are flattened, before the final cartesian DataFrame is generated. If a Pandas Series/DataFrame is passed, and has a labeled index, or a MultiIndex index, the index is discarded; the final DataFrame will have a RangeIndex. The MultiIndexed DataFrame can be flattened using pyjanitor's `collapse_levels` method; the user can also decide to drop any of the levels, via Pandas' `droplevel` method. Functional usage syntax: ```python import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.expand_grid(df=df, df_key=\"...\", others={...}) ``` Method-chaining usage syntax: ```python import pandas as pd import janitor as jn df = pd.DataFrame(...).expand_grid(df_key=\"bla\",others={...}) ``` Usage independent of a DataFrame .. code-block:: python ```python import pandas as pd from janitor import expand_grid df = expand_grid(others = {\"x\":range(1,4), \"y\":[1,2]}) :param df: A pandas DataFrame. :param df_key: name of key for the dataframe. It becomes part of the column names of the dataframe. :param others: A dictionary that contains the data to be combined with the dataframe. If no dataframe exists, all inputs in `others` will be combined to create a DataFrame. :returns: A pandas DataFrame of the cartesian product. :raises KeyError: if there is a DataFrame and `df_key` is not provided. \"\"\" if not others: if df is not None: return df return check(\"others\", others, [dict]) # if there is a DataFrame, for the method chaining, # it must have a key, to create a name value pair if df is not None: df = df.copy() if not df_key: raise KeyError( \"\"\" Using `expand_grid` as part of a DataFrame method chain requires that a string argument be provided for the `df_key` parameter. \"\"\" ) check(\"df_key\", df_key, [str]) others = {**{df_key: df}, **others} return _computations_expand_grid(others)","title":"expand_grid()"},{"location":"api/functions/#janitor.functions.factorize_columns","text":"","title":"factorize_columns"},{"location":"api/functions/#janitor.functions.factorize_columns.factorize_columns","text":"Converts labels into numerical data This method will create a new column with the string _enc appended after the original column's name. This can be overriden with the suffix parameter. Internally this method uses pandas factorize method. It takes in an optional suffix and keyword arguments also. An empty string as suffix will override the existing column. This method mutates the original DataFrame. Functional usage syntax: df = factorize_columns( df, column_names=\"my_categorical_column\", suffix=\"_enc\" ) # one way Method chaining syntax: import pandas as pd import janitor categorical_cols = ['col1', 'col2', 'col4'] df = ( pd.DataFrame(...) .factorize_columns( column_names=categorical_cols, suffix=\"_enc\" ) ) Parameters: Name Type Description Default df DataFrame The pandas DataFrame object. required column_names Union[str, Iterable[str], Hashable] A column name or an iterable (list or tuple) of column names. required suffix str Suffix to be used for the new column. Default value is _enc. An empty string suffix means, it will override the existing column '_enc' **kwargs Keyword arguments. It takes any of the keyword arguments, which the pandas factorize method takes like sort,na_sentinel,size_hint {} Returns: Type Description DataFrame A pandas DataFrame. Source code in janitor/functions/factorize_columns.py @pf.register_dataframe_method def factorize_columns( df: pd.DataFrame, column_names: Union[str, Iterable[str], Hashable], suffix: str = \"_enc\", **kwargs, ) -> pd.DataFrame: \"\"\" Converts labels into numerical data This method will create a new column with the string `_enc` appended after the original column's name. This can be overriden with the suffix parameter. Internally this method uses pandas `factorize` method. It takes in an optional suffix and keyword arguments also. An empty string as suffix will override the existing column. This method mutates the original DataFrame. Functional usage syntax: ```python df = factorize_columns( df, column_names=\"my_categorical_column\", suffix=\"_enc\" ) # one way ``` Method chaining syntax: ```python import pandas as pd import janitor categorical_cols = ['col1', 'col2', 'col4'] df = ( pd.DataFrame(...) .factorize_columns( column_names=categorical_cols, suffix=\"_enc\" ) ) ``` :param df: The pandas DataFrame object. :param column_names: A column name or an iterable (list or tuple) of column names. :param suffix: Suffix to be used for the new column. Default value is _enc. An empty string suffix means, it will override the existing column :param **kwargs: Keyword arguments. It takes any of the keyword arguments, which the pandas factorize method takes like sort,na_sentinel,size_hint :returns: A pandas DataFrame. \"\"\" df = _factorize(df, column_names, suffix, **kwargs) return df","title":"factorize_columns()"},{"location":"api/functions/#janitor.functions.fill","text":"","title":"fill"},{"location":"api/functions/#janitor.functions.fill.fill_direction","text":"Provide a method-chainable function for filling missing values in selected columns. It is a wrapper for pd.Series.ffill and pd.Series.bfill , and pairs the column name with one of up , down , updown , and downup . import pandas as pd import janitor as jn df text code 0 ragnar NaN 1 NaN 2.0 2 sammywemmy 3.0 3 NaN NaN 4 ginger 5.0 Fill on a single column: df.fill_direction(code = 'up') text code 0 ragnar 2.0 1 NaN 2.0 2 sammywemmy 3.0 3 NaN 5.0 4 ginger 5.0 Fill on multiple columns: df.fill_direction(text = 'down', code = 'down') text code 0 ragnar NaN 1 ragnar 2.0 2 sammywemmy 3.0 3 sammywemmy 3.0 4 ginger 5.0 Fill multiple columns in different directions: df.fill_direction(text = 'up', code = 'down') text code 0 ragnar NaN 1 sammywemmy 2.0 2 sammywemmy 3.0 3 ginger 3.0 4 ginger 5.0 Functional usage syntax: import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.fill_direction( df = df, column_1 = direction_1, column_2 = direction_2, ) Method-chaining usage syntax: import pandas as pd import janitor as jn df = pd.DataFrame(...) .fill_direction( column_1 = direction_1, column_2 = direction_2, ) Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required kwargs Key - value pairs of columns and directions. Directions can be either down , up , updown (fill up then down) and downup (fill down then up). {} Returns: Type Description DataFrame A pandas DataFrame with modified column(s). Exceptions: Type Description ValueError if column supplied is not in the DataFrame. ValueError if direction supplied is not one of down , up , updown , or downup . Source code in janitor/functions/fill.py @pf.register_dataframe_method def fill_direction(df: pd.DataFrame, **kwargs) -> pd.DataFrame: \"\"\" Provide a method-chainable function for filling missing values in selected columns. It is a wrapper for `pd.Series.ffill` and `pd.Series.bfill`, and pairs the column name with one of `up`, `down`, `updown`, and `downup`. ```python import pandas as pd import janitor as jn df text code 0 ragnar NaN 1 NaN 2.0 2 sammywemmy 3.0 3 NaN NaN 4 ginger 5.0 ``` Fill on a single column: ```python df.fill_direction(code = 'up') text code 0 ragnar 2.0 1 NaN 2.0 2 sammywemmy 3.0 3 NaN 5.0 4 ginger 5.0 ``` Fill on multiple columns: ```python df.fill_direction(text = 'down', code = 'down') text code 0 ragnar NaN 1 ragnar 2.0 2 sammywemmy 3.0 3 sammywemmy 3.0 4 ginger 5.0 ``` Fill multiple columns in different directions: ```python df.fill_direction(text = 'up', code = 'down') text code 0 ragnar NaN 1 sammywemmy 2.0 2 sammywemmy 3.0 3 ginger 3.0 4 ginger 5.0 ``` Functional usage syntax: ```python import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.fill_direction( df = df, column_1 = direction_1, column_2 = direction_2, ) ``` Method-chaining usage syntax: ```python import pandas as pd import janitor as jn df = pd.DataFrame(...) .fill_direction( column_1 = direction_1, column_2 = direction_2, ) ``` :param df: A pandas DataFrame. :param kwargs: Key - value pairs of columns and directions. Directions can be either `down`, `up`, `updown` (fill up then down) and `downup` (fill down then up). :returns: A pandas DataFrame with modified column(s). :raises ValueError: if column supplied is not in the DataFrame. :raises ValueError: if direction supplied is not one of `down`, `up`, `updown`, or `downup`. \"\"\" if not kwargs: return df fill_types = {fill.name for fill in _FILLTYPE} for column_name, fill_type in kwargs.items(): check(\"column_name\", column_name, [str]) check(\"fill_type\", fill_type, [str]) if fill_type.upper() not in fill_types: raise ValueError( \"\"\" fill_type should be one of up, down, updown, or downup. \"\"\" ) check_column(df, kwargs) new_values = {} for column_name, fill_type in kwargs.items(): direction = _FILLTYPE[f\"{fill_type.upper()}\"].value if len(direction) == 1: direction = methodcaller(direction[0]) output = direction(df[column_name]) else: direction = [methodcaller(entry) for entry in direction] output = _chain_func(df[column_name], *direction) new_values[column_name] = output return df.assign(**new_values)","title":"fill_direction()"},{"location":"api/functions/#janitor.functions.fill.fill_empty","text":"Fill NaN values in specified columns with a given value. Super sugary syntax that wraps pandas.DataFrame.fillna . This method mutates the original DataFrame. Functional usage syntax: df = fill_empty(df, column_names=[col1, col2], value=0) Method chaining syntax: import pandas as pd import janitor df = pd.DataFrame(...).fill_empty(column_names=col1, value=0) Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_names Union[str, Iterable[str], Hashable] column_names: A column name or an iterable (list or tuple) of column names. If a single column name is passed in, then only that column will be filled; if a list or tuple is passed in, then those columns will all be filled with the same value. required value The value that replaces the NaN values. required Returns: Type Description DataFrame A pandas DataFrame with NaN values filled. Source code in janitor/functions/fill.py @pf.register_dataframe_method @deprecated_alias(columns=\"column_names\") def fill_empty( df: pd.DataFrame, column_names: Union[str, Iterable[str], Hashable], value ) -> pd.DataFrame: \"\"\" Fill `NaN` values in specified columns with a given value. Super sugary syntax that wraps `pandas.DataFrame.fillna`. This method mutates the original DataFrame. Functional usage syntax: ```python df = fill_empty(df, column_names=[col1, col2], value=0) ``` Method chaining syntax: ```python import pandas as pd import janitor df = pd.DataFrame(...).fill_empty(column_names=col1, value=0) ``` :param df: A pandas DataFrame. :param column_names: column_names: A column name or an iterable (list or tuple) of column names. If a single column name is passed in, then only that column will be filled; if a list or tuple is passed in, then those columns will all be filled with the same value. :param value: The value that replaces the `NaN` values. :returns: A pandas DataFrame with `NaN` values filled. \"\"\" check_column(df, column_names) return _fill_empty(df, column_names, value=value)","title":"fill_empty()"},{"location":"api/functions/#janitor.functions.filter","text":"","title":"filter"},{"location":"api/functions/#janitor.functions.filter.filter_column_isin","text":"Filter a dataframe for values in a column that exist in another iterable. This method does not mutate the original DataFrame. Assumes exact matching; fuzzy matching not implemented. The below example syntax will filter the DataFrame such that we only get rows for which the names are exactly James and John . df = ( pd.DataFrame(...) .clean_names() .filter_column_isin(column_name=\"names\", iterable=[\"James\", \"John\"] ) ) This is the method chaining alternative to: df = df[df['names'].isin(['James', 'John'])] If complement is True , then we will only get rows for which the names are not James or John . Parameters: Name Type Description Default df DataFrame A pandas DataFrame required column_name Hashable The column on which to filter. required iterable Iterable An iterable. Could be a list, tuple, another pandas Series. required complement bool Whether to return the complement of the selection or not. False Returns: Type Description DataFrame A filtered pandas DataFrame. Exceptions: Type Description ValueError if iterable does not have a length of 1 or greater. Source code in janitor/functions/filter.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def filter_column_isin( df: pd.DataFrame, column_name: Hashable, iterable: Iterable, complement: bool = False, ) -> pd.DataFrame: \"\"\" Filter a dataframe for values in a column that exist in another iterable. This method does not mutate the original DataFrame. Assumes exact matching; fuzzy matching not implemented. The below example syntax will filter the DataFrame such that we only get rows for which the `names` are exactly `James` and `John`. ```python df = ( pd.DataFrame(...) .clean_names() .filter_column_isin(column_name=\"names\", iterable=[\"James\", \"John\"] ) ) ``` This is the method chaining alternative to: ```python df = df[df['names'].isin(['James', 'John'])] ``` If `complement` is `True`, then we will only get rows for which the names are not `James` or `John`. :param df: A pandas DataFrame :param column_name: The column on which to filter. :param iterable: An iterable. Could be a list, tuple, another pandas Series. :param complement: Whether to return the complement of the selection or not. :returns: A filtered pandas DataFrame. :raises ValueError: if `iterable` does not have a length of `1` or greater. \"\"\" if len(iterable) == 0: raise ValueError( \"`iterable` kwarg must be given an iterable of length 1 or greater\" ) criteria = df[column_name].isin(iterable) if complement: return df[~criteria] return df[criteria]","title":"filter_column_isin()"},{"location":"api/functions/#janitor.functions.filter.filter_date","text":"Filter a date-based column based on certain criteria. This method does not mutate the original DataFrame. Dates may be finicky and this function builds on top of the magic from the pandas to_datetime function that is able to parse dates well. Additional options to parse the date type of your column may be found at the official pandas documentation Note This method will cast your column to a Timestamp! Note This only affects the format of the start_date and end_date parameters. If there's an issue with the format of the DataFrame being parsed, you would pass {'format': your_format} to column_date_options . Parameters: Name Type Description Default df DataFrame The dataframe to filter on. required column_name Hashable The column which to apply the fraction transformation. required start_date Optional[datetime.date] The beginning date to use to filter the DataFrame. None end_date Optional[datetime.date] The end date to use to filter the DataFrame. None years Optional[List] The years to use to filter the DataFrame. None months Optional[List] The months to use to filter the DataFrame. None days Optional[List] The days to use to filter the DataFrame. None column_date_options Optional[Dict] 'Special options to use when parsing the date column in the original DataFrame. The options may be found at the official Pandas documentation.' None format Optional[str] 'If you're using a format for start_date or end_date that is not recognized natively by pandas' to_datetime function, you may supply the format yourself. Python date and time formats may be found at link . None Returns: Type Description DataFrame A filtered pandas DataFrame. Source code in janitor/functions/filter.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\", start=\"start_date\", end=\"end_date\") def filter_date( df: pd.DataFrame, column_name: Hashable, start_date: Optional[dt.date] = None, end_date: Optional[dt.date] = None, years: Optional[List] = None, months: Optional[List] = None, days: Optional[List] = None, column_date_options: Optional[Dict] = None, format: Optional[str] = None, # skipcq: PYL-W0622 ) -> pd.DataFrame: \"\"\" Filter a date-based column based on certain criteria. This method does not mutate the original DataFrame. Dates may be finicky and this function builds on top of the *magic* from the pandas `to_datetime` function that is able to parse dates well. Additional options to parse the date type of your column may be found at the official pandas [documentation][datetime] [datetime]: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html !!!note This method will cast your column to a Timestamp! !!!note This only affects the format of the `start_date` and `end_date` parameters. If there's an issue with the format of the DataFrame being parsed, you would pass `{'format': your_format}` to `column_date_options`. :param df: The dataframe to filter on. :param column_name: The column which to apply the fraction transformation. :param start_date: The beginning date to use to filter the DataFrame. :param end_date: The end date to use to filter the DataFrame. :param years: The years to use to filter the DataFrame. :param months: The months to use to filter the DataFrame. :param days: The days to use to filter the DataFrame. :param column_date_options: 'Special options to use when parsing the date column in the original DataFrame. The options may be found at the official Pandas documentation.' :param format: 'If you're using a format for `start_date` or `end_date` that is not recognized natively by pandas' `to_datetime` function, you may supply the format yourself. Python date and time formats may be found at [link](http://strftime.org/). :returns: A filtered pandas DataFrame. \"\"\" # noqa: E501 def _date_filter_conditions(conditions): \"\"\"Taken from: https://stackoverflow.com/a/13616382.\"\"\" return reduce(np.logical_and, conditions) if column_date_options: df.loc[:, column_name] = pd.to_datetime( df.loc[:, column_name], **column_date_options ) else: df.loc[:, column_name] = pd.to_datetime(df.loc[:, column_name]) _filter_list = [] if start_date: start_date = pd.to_datetime(start_date, format=format) _filter_list.append(df.loc[:, column_name] >= start_date) if end_date: end_date = pd.to_datetime(end_date, format=format) _filter_list.append(df.loc[:, column_name] <= end_date) if years: _filter_list.append(df.loc[:, column_name].dt.year.isin(years)) if months: _filter_list.append(df.loc[:, column_name].dt.month.isin(months)) if days: _filter_list.append(df.loc[:, column_name].dt.day.isin(days)) if start_date and end_date and start_date > end_date: warnings.warn( f\"Your start date of {start_date} is after your end date of \" f\"{end_date}. Is this intended?\" ) return df.loc[_date_filter_conditions(_filter_list), :]","title":"filter_date()"},{"location":"api/functions/#janitor.functions.filter.filter_on","text":"Return a dataframe filtered on a particular criteria. This method does not mutate the original DataFrame. This is super-sugary syntax that wraps the pandas .query() API, enabling users to use strings to quickly specify filters for filtering their dataframe. The intent is that filter_on as a verb better matches the intent of a pandas user than the verb query . Let's say we wanted to filter students based on whether they failed an exam or not, which is defined as their score (in the \"score\" column) being less than 50. df = (pd.DataFrame(...) .filter_on('score < 50', complement=False) ...) # chain on more data preprocessing. This stands in contrast to the in-place syntax that is usually used: df = pd.DataFrame(...) df = df[df['score'] < 3] As with the filter_string function, a more seamless flow can be expressed in the code. Functional usage syntax: df = filter_on(df, 'score < 50', complement=False) Method chaining syntax: .filter_on('score < 50', complement=False)) ``` Credit to Brant Peterson for the name. Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required criteria str A filtering criteria that returns an array or Series of booleans, on which pandas can filter on. required complement bool Whether to return the complement of the filter or not. False Returns: Type Description DataFrame A filtered pandas DataFrame. Source code in janitor/functions/filter.py @pf.register_dataframe_method def filter_on( df: pd.DataFrame, criteria: str, complement: bool = False ) -> pd.DataFrame: \"\"\" Return a dataframe filtered on a particular criteria. This method does not mutate the original DataFrame. This is super-sugary syntax that wraps the pandas `.query()` API, enabling users to use strings to quickly specify filters for filtering their dataframe. The intent is that `filter_on` as a verb better matches the intent of a pandas user than the verb `query`. Let's say we wanted to filter students based on whether they failed an exam or not, which is defined as their score (in the \"score\" column) being less than 50. ```python df = (pd.DataFrame(...) .filter_on('score < 50', complement=False) ...) # chain on more data preprocessing. ``` This stands in contrast to the in-place syntax that is usually used: ```python df = pd.DataFrame(...) df = df[df['score'] < 3] ``` As with the `filter_string` function, a more seamless flow can be expressed in the code. Functional usage syntax: ```python df = filter_on(df, 'score < 50', complement=False) ``` Method chaining syntax: .filter_on('score < 50', complement=False)) ``` Credit to Brant Peterson for the name. :param df: A pandas DataFrame. :param criteria: A filtering criteria that returns an array or Series of booleans, on which pandas can filter on. :param complement: Whether to return the complement of the filter or not. :returns: A filtered pandas DataFrame. \"\"\" if complement: return df.query(\"not \" + criteria) return df.query(criteria)","title":"filter_on()"},{"location":"api/functions/#janitor.functions.filter.filter_string","text":"Filter a string-based column according to whether it contains a substring. This is super sugary syntax that builds on top of pandas.Series.str.contains . Because this uses internally pandas.Series.str.contains , which allows a regex string to be passed into it, thus search_string can also be a regex pattern. This method does not mutate the original DataFrame. This function allows us to method chain filtering operations: df = (pd.DataFrame(...) .filter_string('column', search_string='pattern', complement=False) ...) # chain on more data preprocessing. This stands in contrast to the in-place syntax that is usually used: df = pd.DataFrame(...) df = df[df['column'].str.contains('pattern')]] As can be seen here, the API design allows for a more seamless flow in expressing the filtering operations. Functional usage syntax: df = filter_string(df, column_name='column', search_string='pattern', complement=False) Method chaining syntax: df = (pd.DataFrame(...) .filter_string(column_name='column', search_string='pattern', complement=False) ...) Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable The column to filter. The column should contain strings. required search_string str A regex pattern or a (sub-)string to search. required complement bool Whether to return the complement of the filter or not. False Returns: Type Description DataFrame A filtered pandas DataFrame. Source code in janitor/functions/filter.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def filter_string( df: pd.DataFrame, column_name: Hashable, search_string: str, complement: bool = False, ) -> pd.DataFrame: \"\"\" Filter a string-based column according to whether it contains a substring. This is super sugary syntax that builds on top of `pandas.Series.str.contains`. Because this uses internally `pandas.Series.str.contains`, which allows a regex string to be passed into it, thus `search_string` can also be a regex pattern. This method does not mutate the original DataFrame. This function allows us to method chain filtering operations: ```python df = (pd.DataFrame(...) .filter_string('column', search_string='pattern', complement=False) ...) # chain on more data preprocessing. ``` This stands in contrast to the in-place syntax that is usually used: ```python df = pd.DataFrame(...) df = df[df['column'].str.contains('pattern')]] ``` As can be seen here, the API design allows for a more seamless flow in expressing the filtering operations. Functional usage syntax: ```python df = filter_string(df, column_name='column', search_string='pattern', complement=False) ``` Method chaining syntax: ```python df = (pd.DataFrame(...) .filter_string(column_name='column', search_string='pattern', complement=False) ...) ``` :param df: A pandas DataFrame. :param column_name: The column to filter. The column should contain strings. :param search_string: A regex pattern or a (sub-)string to search. :param complement: Whether to return the complement of the filter or not. :returns: A filtered pandas DataFrame. \"\"\" # noqa: E501 criteria = df[column_name].str.contains(search_string) if complement: return df[~criteria] return df[criteria]","title":"filter_string()"},{"location":"api/functions/#janitor.functions.find_replace","text":"","title":"find_replace"},{"location":"api/functions/#janitor.functions.find_replace.find_replace","text":"Perform a find-and-replace action on provided columns. Depending on use case, users can choose either exact, full-value matching, or regular-expression-based fuzzy matching (hence allowing substring matching in the latter case). For strings, the matching is always case sensitive. For instance, given a DataFrame containing orders at a coffee shop: df = pd.DataFrame({ 'customer': ['Mary', 'Tom', 'Lila'], 'order': ['ice coffee', 'lemonade', 'regular coffee'] }) df customer order 0 Mary ice coffee 1 Tom lemonade 2 Lila regular coffee Our task is to replace values ice coffee and regular coffee of the order column into latte . Example 1 for exact matching #Functional usage df = find_replace( df, match='exact', order={'ice coffee': 'latte', 'regular coffee': 'latte'}, ) # Method chaining usage df = df.find_replace( match='exact' order={'ice coffee': 'latte', 'regular coffee': 'latte'}, ) Example 2: Regular-expression-based matching # Functional usage df = find_replace( df, match='regex', order={'coffee$': 'latte'}, ) # Method chaining usage df = df.find_replace( match='regex', order={'coffee$': 'latte'}, ) To perform a find and replace on the entire DataFrame, pandas' df.replace() function provides the appropriate functionality. You can find more detail on the replace docs. This function only works with column names that have no spaces or punctuation in them. For example, a column name item_name would work with find_replace , because it is a contiguous string that can be parsed correctly, but item name would not be parsed correctly by the Python interpreter. If you have column names that might not be compatible, we recommend calling on clean_names() as the first method call. If, for whatever reason, that is not possible, then _find_replace is available as a function that you can do a pandas pipe call on. Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required match str Whether or not to perform an exact match or not. Valid values are \"exact\" or \"regex\". 'exact' mappings keyword arguments corresponding to column names that have dictionaries passed in indicating what to find (keys) and what to replace with (values). {} Returns: Type Description DataFrame A pandas DataFrame with replaced values. Source code in janitor/functions/find_replace.py @pf.register_dataframe_method def find_replace( df: pd.DataFrame, match: str = \"exact\", **mappings ) -> pd.DataFrame: \"\"\" Perform a find-and-replace action on provided columns. Depending on use case, users can choose either exact, full-value matching, or regular-expression-based fuzzy matching (hence allowing substring matching in the latter case). For strings, the matching is always case sensitive. For instance, given a DataFrame containing orders at a coffee shop: ```python df = pd.DataFrame({ 'customer': ['Mary', 'Tom', 'Lila'], 'order': ['ice coffee', 'lemonade', 'regular coffee'] }) df customer order 0 Mary ice coffee 1 Tom lemonade 2 Lila regular coffee ``` Our task is to replace values `ice coffee` and `regular coffee` of the `order` column into `latte`. Example 1 for exact matching ```python #Functional usage df = find_replace( df, match='exact', order={'ice coffee': 'latte', 'regular coffee': 'latte'}, ) ``` ```python # Method chaining usage df = df.find_replace( match='exact' order={'ice coffee': 'latte', 'regular coffee': 'latte'}, ) ``` Example 2: Regular-expression-based matching ```python # Functional usage df = find_replace( df, match='regex', order={'coffee$': 'latte'}, ) ``` ```python # Method chaining usage df = df.find_replace( match='regex', order={'coffee$': 'latte'}, ) ``` To perform a find and replace on the entire DataFrame, pandas' `df.replace()` function provides the appropriate functionality. You can find more detail on the [replace] docs. [replace]: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html This function only works with column names that have no spaces or punctuation in them. For example, a column name `item_name` would work with `find_replace`, because it is a contiguous string that can be parsed correctly, but `item name` would not be parsed correctly by the Python interpreter. If you have column names that might not be compatible, we recommend calling on `clean_names()` as the first method call. If, for whatever reason, that is not possible, then `_find_replace` is available as a function that you can do a pandas [pipe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pipe.html) call on. :param df: A pandas DataFrame. :param match: Whether or not to perform an exact match or not. Valid values are \"exact\" or \"regex\". :param mappings: keyword arguments corresponding to column names that have dictionaries passed in indicating what to find (keys) and what to replace with (values). :returns: A pandas DataFrame with replaced values. \"\"\" # noqa: E501 for column_name, mapper in mappings.items(): df = _find_replace(df, column_name, mapper, match=match) return df","title":"find_replace()"},{"location":"api/functions/#janitor.functions.flag_nulls","text":"","title":"flag_nulls"},{"location":"api/functions/#janitor.functions.flag_nulls.flag_nulls","text":"Creates a new column to indicate whether you have null values in a given row. If the columns parameter is not set, looks across the entire DataFrame, otherwise will look only in the columns you set. import pandas as pd import janitor as jn df = pd.DataFrame( {'a': [1, 2, None, 4], 'b': [5.0, None, 7.0, 8.0]}) df.flag_nulls() jn.functions.flag_nulls(df) df.flag_nulls(columns=['b']) Parameters: Name Type Description Default df DataFrame Input Pandas dataframe. required column_name Optional[Hashable] Name for the output column. Defaults to 'null_flag'. 'null_flag' columns Union[str, Iterable[str], Hashable] List of columns to look at for finding null values. If you only want to look at one column, you can simply give its name. If set to None (default), all DataFrame columns are used. None Returns: Type Description DataFrame Input dataframe with the null flag column. Exceptions: Type Description ValueError if column_name is already present in the DataFrame. ValueError if a column within columns is no present in the DataFrame. .. # noqa: DAR402 Source code in janitor/functions/flag_nulls.py @pf.register_dataframe_method def flag_nulls( df: pd.DataFrame, column_name: Optional[Hashable] = \"null_flag\", columns: Optional[Union[str, Iterable[str], Hashable]] = None, ) -> pd.DataFrame: \"\"\"Creates a new column to indicate whether you have null values in a given row. If the columns parameter is not set, looks across the entire DataFrame, otherwise will look only in the columns you set. ```python import pandas as pd import janitor as jn df = pd.DataFrame( {'a': [1, 2, None, 4], 'b': [5.0, None, 7.0, 8.0]}) df.flag_nulls() jn.functions.flag_nulls(df) df.flag_nulls(columns=['b']) ``` :param df: Input Pandas dataframe. :param column_name: Name for the output column. Defaults to 'null_flag'. :param columns: List of columns to look at for finding null values. If you only want to look at one column, you can simply give its name. If set to None (default), all DataFrame columns are used. :returns: Input dataframe with the null flag column. :raises ValueError: if `column_name` is already present in the DataFrame. :raises ValueError: if a column within `columns` is no present in the DataFrame. .. # noqa: DAR402 \"\"\" # Sort out columns input if isinstance(columns, str): columns = [columns] elif columns is None: columns = df.columns elif not isinstance(columns, Iterable): # catches other hashable types columns = [columns] # Input sanitation checks check_column(df, columns) check_column(df, [column_name], present=False) # This algorithm works best for n_rows >> n_cols. See issue #501 null_array = np.zeros(len(df)) for col in columns: null_array = np.logical_or(null_array, pd.isna(df[col])) df = df.copy() df[column_name] = null_array.astype(int) return df","title":"flag_nulls()"},{"location":"api/functions/#janitor.functions.get_dupes","text":"","title":"get_dupes"},{"location":"api/functions/#janitor.functions.get_dupes.get_dupes","text":"Return all duplicate rows. This method does not mutate the original DataFrame. Functional usage syntax: df = pd.DataFrame(...) df = get_dupes(df) Method chaining syntax: import pandas as pd import janitor df = pd.DataFrame(...).get_dupes() Parameters: Name Type Description Default df DataFrame The pandas DataFrame object. required column_names Union[str, Iterable[str], Hashable] (optional) A column name or an iterable (list or tuple) of column names. Following pandas API, this only considers certain columns for identifying duplicates. Defaults to using all columns. None Returns: Type Description DataFrame The duplicate rows, as a pandas DataFrame. Source code in janitor/functions/get_dupes.py @pf.register_dataframe_method @deprecated_alias(columns=\"column_names\") def get_dupes( df: pd.DataFrame, column_names: Optional[Union[str, Iterable[str], Hashable]] = None, ) -> pd.DataFrame: \"\"\" Return all duplicate rows. This method does not mutate the original DataFrame. Functional usage syntax: ```python df = pd.DataFrame(...) df = get_dupes(df) ``` Method chaining syntax: ```python import pandas as pd import janitor df = pd.DataFrame(...).get_dupes() ``` :param df: The pandas DataFrame object. :param column_names: (optional) A column name or an iterable (list or tuple) of column names. Following pandas API, this only considers certain columns for identifying duplicates. Defaults to using all columns. :returns: The duplicate rows, as a pandas DataFrame. \"\"\" dupes = df.duplicated(subset=column_names, keep=False) return df[dupes == True] # noqa: E712","title":"get_dupes()"},{"location":"api/functions/#janitor.functions.groupby_agg","text":"","title":"groupby_agg"},{"location":"api/functions/#janitor.functions.groupby_agg.groupby_agg","text":"Shortcut for assigning a groupby-transform to a new column. This method does not mutate the original DataFrame. Without this function, we would have to write a verbose line: df = df.assign(...=df.groupby(...)[...].transform(...)) Now, this function can be method-chained: import pandas as pd import janitor df = pd.DataFrame(...).groupby_agg(by='group', agg='mean', agg_column_name=\"col1\" new_column_name='col1_mean_by_group', dropna = True/False) Examples:: import pandas as pd import janitor as jn group var1 0 1 1 1 1 1 2 1 1 3 1 1 4 1 2 5 2 1 6 2 2 7 2 2 8 2 2 9 2 3 Let's get the count per group and var1 :: df.groupby_agg( by = ['group', 'var1'], agg = 'size', agg_column_name = 'var1', new_column_name = 'count' ) group var1 size 0 1 1 4 1 1 1 4 2 1 1 4 3 1 1 4 4 1 2 1 5 2 1 1 6 2 2 3 7 2 2 3 8 2 2 3 9 2 3 1 If the data has null values, you can include the null values by passing False to dropna ; this feature was introduced in Pandas 1.1:: name type num nulls 0 black chair 4 1.0 1 black chair 5 1.0 2 black sofa 12 NaN 3 red sofa 4 NaN 4 red plate 3 3.0 Let's get the count, including the null values, grouping on nulls column:: df.groupby_agg( by=\"nulls\", new_column_name=\"num_count\", agg_column_name=\"num\", agg=\"size\", dropna=False, ) name type num nulls num_count 0 black chair 4 1.0 2 1 black chair 5 1.0 2 2 black sofa 12 NaN 2 3 red sofa 4 NaN 2 4 red plate 3 3.0 1 Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required by Union[List, str] Column(s) to groupby on, either a str or a list of str required new_column_name str Name of the aggregation output column. required agg_column_name str Name of the column to aggregate over. required agg Union[Callable, str] How to aggregate. required dropna bool Whether or not to include null values, if present in the by column(s). Default is True. True Returns: Type Description DataFrame A pandas DataFrame. Source code in janitor/functions/groupby_agg.py @pf.register_dataframe_method @deprecated_alias(new_column=\"new_column_name\", agg_column=\"agg_column_name\") def groupby_agg( df: pd.DataFrame, by: Union[List, str], new_column_name: str, agg_column_name: str, agg: Union[Callable, str], dropna: bool = True, ) -> pd.DataFrame: \"\"\" Shortcut for assigning a groupby-transform to a new column. This method does not mutate the original DataFrame. Without this function, we would have to write a verbose line: df = df.assign(...=df.groupby(...)[...].transform(...)) Now, this function can be method-chained: import pandas as pd import janitor df = pd.DataFrame(...).groupby_agg(by='group', agg='mean', agg_column_name=\"col1\" new_column_name='col1_mean_by_group', dropna = True/False) Examples:: import pandas as pd import janitor as jn group var1 0 1 1 1 1 1 2 1 1 3 1 1 4 1 2 5 2 1 6 2 2 7 2 2 8 2 2 9 2 3 Let's get the count per `group` and `var1`:: df.groupby_agg( by = ['group', 'var1'], agg = 'size', agg_column_name = 'var1', new_column_name = 'count' ) group var1 size 0 1 1 4 1 1 1 4 2 1 1 4 3 1 1 4 4 1 2 1 5 2 1 1 6 2 2 3 7 2 2 3 8 2 2 3 9 2 3 1 If the data has null values, you can include the null values by passing `False` to `dropna`; this feature was introduced in Pandas 1.1:: name type num nulls 0 black chair 4 1.0 1 black chair 5 1.0 2 black sofa 12 NaN 3 red sofa 4 NaN 4 red plate 3 3.0 Let's get the count, including the null values, grouping on `nulls` column:: df.groupby_agg( by=\"nulls\", new_column_name=\"num_count\", agg_column_name=\"num\", agg=\"size\", dropna=False, ) name type num nulls num_count 0 black chair 4 1.0 2 1 black chair 5 1.0 2 2 black sofa 12 NaN 2 3 red sofa 4 NaN 2 4 red plate 3 3.0 1 :param df: A pandas DataFrame. :param by: Column(s) to groupby on, either a `str` or a `list` of `str` :param new_column_name: Name of the aggregation output column. :param agg_column_name: Name of the column to aggregate over. :param agg: How to aggregate. :param dropna: Whether or not to include null values, if present in the `by` column(s). Default is True. :returns: A pandas DataFrame. \"\"\" df = df.copy() df[new_column_name] = df.groupby(by, dropna=dropna)[ agg_column_name ].transform(agg) return df","title":"groupby_agg()"},{"location":"api/functions/#janitor.functions.groupby_topk","text":"","title":"groupby_topk"},{"location":"api/functions/#janitor.functions.groupby_topk.groupby_topk","text":"Return top k rows from a groupby of a set of columns. Returns a DataFrame that has the top k values grouped by groupby_column_name and sorted by sort_column_name . Additional parameters to the sorting (such as ascending=True ) can be passed using sort_values_kwargs . List of all sort_values() parameters can be found here . import pandas as pd import janitor as jn age ID result 0 20 1 pass 1 22 2 fail 2 24 3 pass 3 23 4 pass 4 21 5 fail 5 22 6 pass Ascending top 3: df.groupby_topk('result', 'age', 3) age ID result result fail 4 21 5 fail 1 22 2 fail pass 0 20 1 pass 5 22 6 pass 3 23 4 pass Descending top 2: df.groupby_topk('result', 'age', 2, {'ascending':False}) age ID result result fail 1 22 2 fail 4 21 5 fail pass 2 24 3 pass 3 23 4 pass Functional usage syntax: import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.groupby_topk( df = df, groupby_column_name = 'groupby_column', sort_column_name = 'sort_column', k = 5 ) Method-chaining usage syntax: import pandas as pd import janitor as jn df = ( pd.DataFrame(...) .groupby_topk( df = df, groupby_column_name = 'groupby_column', sort_column_name = 'sort_column', k = 5 ) ) Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required groupby_column_name Hashable Column name to group input DataFrame df by. required sort_column_name Hashable Name of the column to sort along the input DataFrame df . required k int Number of top rows to return from each group after sorting. required sort_values_kwargs Dict Arguments to be passed to sort_values function. None Returns: Type Description DataFrame A pandas DataFrame with top k rows that are grouped by groupby_column_name column with each group sorted along the column sort_column_name . Exceptions: Type Description ValueError if k is less than 1. ValueError if groupby_column_name not in DataFrame df . ValueError if sort_column_name not in DataFrame df . KeyError if inplace:True is present in sort_values_kwargs . Source code in janitor/functions/groupby_topk.py @pf.register_dataframe_method def groupby_topk( df: pd.DataFrame, groupby_column_name: Hashable, sort_column_name: Hashable, k: int, sort_values_kwargs: Dict = None, ) -> pd.DataFrame: \"\"\" Return top `k` rows from a groupby of a set of columns. Returns a DataFrame that has the top `k` values grouped by `groupby_column_name` and sorted by `sort_column_name`. Additional parameters to the sorting (such as `ascending=True`) can be passed using `sort_values_kwargs`. List of all sort_values() parameters can be found [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html). ```python import pandas as pd import janitor as jn age ID result 0 20 1 pass 1 22 2 fail 2 24 3 pass 3 23 4 pass 4 21 5 fail 5 22 6 pass ``` Ascending top 3: ```python df.groupby_topk('result', 'age', 3) age ID result result fail 4 21 5 fail 1 22 2 fail pass 0 20 1 pass 5 22 6 pass 3 23 4 pass ``` Descending top 2: ```python df.groupby_topk('result', 'age', 2, {'ascending':False}) age ID result result fail 1 22 2 fail 4 21 5 fail pass 2 24 3 pass 3 23 4 pass ``` Functional usage syntax: ```python import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.groupby_topk( df = df, groupby_column_name = 'groupby_column', sort_column_name = 'sort_column', k = 5 ) ``` Method-chaining usage syntax: ```python import pandas as pd import janitor as jn df = ( pd.DataFrame(...) .groupby_topk( df = df, groupby_column_name = 'groupby_column', sort_column_name = 'sort_column', k = 5 ) ) ``` :param df: A pandas DataFrame. :param groupby_column_name: Column name to group input DataFrame `df` by. :param sort_column_name: Name of the column to sort along the input DataFrame `df`. :param k: Number of top rows to return from each group after sorting. :param sort_values_kwargs: Arguments to be passed to sort_values function. :returns: A pandas DataFrame with top `k` rows that are grouped by `groupby_column_name` column with each group sorted along the column `sort_column_name`. :raises ValueError: if `k` is less than 1. :raises ValueError: if `groupby_column_name` not in DataFrame `df`. :raises ValueError: if `sort_column_name` not in DataFrame `df`. :raises KeyError: if `inplace:True` is present in `sort_values_kwargs`. \"\"\" # noqa: E501 # Convert the default sort_values_kwargs from None to empty Dict sort_values_kwargs = sort_values_kwargs or {} # Check if groupby_column_name and sort_column_name exists in the DataFrame check_column(df, [groupby_column_name, sort_column_name]) # Check if k is greater than 0. if k < 1: raise ValueError( \"Numbers of rows per group to be returned must be greater than 0.\" ) # Check if inplace:True in sort values kwargs because it returns None if ( \"inplace\" in sort_values_kwargs.keys() and sort_values_kwargs[\"inplace\"] ): raise KeyError(\"Cannot use `inplace=True` in `sort_values_kwargs`.\") return df.groupby(groupby_column_name).apply( lambda d: d.sort_values(sort_column_name, **sort_values_kwargs).head(k) )","title":"groupby_topk()"},{"location":"api/functions/#janitor.functions.impute","text":"","title":"impute"},{"location":"api/functions/#janitor.functions.impute.impute","text":"Method-chainable imputation of values in a column. This method mutates the original DataFrame. Underneath the hood, this function calls the .fillna() method available to every pandas.Series object. Method-chaining example: import numpy as np import pandas as pd import janitor data = { \"a\": [1, 2, 3], \"sales\": np.nan, \"score\": [np.nan, 3, 2]} df = ( pd.DataFrame(data) # Impute null values with 0 .impute(column_name='sales', value=0.0) # Impute null values with median .impute(column_name='score', statistic_column_name='median') ) Either one of value or statistic_column_name should be provided. If value is provided, then all null values in the selected column will take on the value provided. If statistic_column_name is provided, then all null values in the selected column will take on the summary statistic value of other non-null values. Currently supported statistics include: mean (also aliased by average ) median mode minimum (also aliased by min ) maximum (also aliased by max ) Parameters: Name Type Description Default df DataFrame A pandas DataFrame required column_name Hashable The name of the column on which to impute values. required value Optional[Any] (optional) The value to impute. None statistic_column_name Optional[str] (optional) The column statistic to impute. None Returns: Type Description DataFrame An imputed pandas DataFrame. Exceptions: Type Description ValueError if both value and statistic are provided. KeyError if statistic is not one of mean , average median , mode , minimum , min , maximum , or max . Source code in janitor/functions/impute.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") @deprecated_alias(statistic=\"statistic_column_name\") def impute( df: pd.DataFrame, column_name: Hashable, value: Optional[Any] = None, statistic_column_name: Optional[str] = None, ) -> pd.DataFrame: \"\"\" Method-chainable imputation of values in a column. This method mutates the original DataFrame. Underneath the hood, this function calls the `.fillna()` method available to every `pandas.Series` object. Method-chaining example: ```python import numpy as np import pandas as pd import janitor data = { \"a\": [1, 2, 3], \"sales\": np.nan, \"score\": [np.nan, 3, 2]} df = ( pd.DataFrame(data) # Impute null values with 0 .impute(column_name='sales', value=0.0) # Impute null values with median .impute(column_name='score', statistic_column_name='median') ) ``` Either one of `value` or `statistic_column_name` should be provided. If `value` is provided, then all null values in the selected column will take on the value provided. If `statistic_column_name` is provided, then all null values in the selected column will take on the summary statistic value of other non-null values. Currently supported statistics include: - `mean` (also aliased by `average`) - `median` - `mode` - `minimum` (also aliased by `min`) - `maximum` (also aliased by `max`) :param df: A pandas DataFrame :param column_name: The name of the column on which to impute values. :param value: (optional) The value to impute. :param statistic_column_name: (optional) The column statistic to impute. :returns: An imputed pandas DataFrame. :raises ValueError: if both `value` and `statistic` are provided. :raises KeyError: if `statistic` is not one of `mean`, `average` `median`, `mode`, `minimum`, `min`, `maximum`, or `max`. \"\"\" # Firstly, we check that only one of `value` or `statistic` are provided. if value is not None and statistic_column_name is not None: raise ValueError( \"Only one of `value` or `statistic` should be provided\" ) # If statistic is provided, then we compute the relevant summary statistic # from the other data. funcs = { \"mean\": np.mean, \"average\": np.mean, # aliased \"median\": np.median, \"mode\": mode, \"minimum\": np.min, \"min\": np.min, # aliased \"maximum\": np.max, \"max\": np.max, # aliased } if statistic_column_name is not None: # Check that the statistic keyword argument is one of the approved. if statistic_column_name not in funcs.keys(): raise KeyError(f\"`statistic` must be one of {funcs.keys()}\") value = funcs[statistic_column_name]( df[column_name].dropna().to_numpy() ) # special treatment for mode, because scipy stats mode returns a # moderesult object. if statistic_column_name == \"mode\": value = value.mode[0] # The code is architected this way - if `value` is not provided but # statistic is, we then overwrite the None value taken on by `value`, and # use it to set the imputation column. if value is not None: df[column_name] = df[column_name].fillna(value) return df","title":"impute()"},{"location":"api/functions/#janitor.functions.jitter","text":"","title":"jitter"},{"location":"api/functions/#janitor.functions.jitter.jitter","text":"Adds Gaussian noise (jitter) to the values of a column. Functional usage syntax: import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.functions.jitter( df=df, column_name='values', dest_column_name='values_jitter', scale=1.0, clip=None, random_state=None, ) Method chaining usage example: import pandas as pd import janitor df = pd.DataFrame(...) df = df.jitter( column_name='values', dest_column_name='values_jitter', scale=1.0, clip=None, random_state=None, ) A new column will be created containing the values of the original column with Gaussian noise added. For each value in the column, a Gaussian distribution is created having a location (mean) equal to the value and a scale (standard deviation) equal to scale . A random value is then sampled from this distribution, which is the jittered value. If a tuple is supplied for clip , then any values of the new column less than clip[0] will be set to clip[0] , and any values greater than clip[1] will be set to clip[1] . Additionally, if a numeric value is supplied for random_state , this value will be used to set the random seed used for sampling. NaN values are ignored in this method. This method mutates the original DataFrame. Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable Name of the column containing values to add Gaussian jitter to. required dest_column_name str The name of the new column containing the jittered values that will be created. required scale number A positive value multiplied by the original column value to determine the scale (standard deviation) of the Gaussian distribution to sample from. (A value of zero results in no jittering.) required clip Optional[Iterable[numpy.number]] An iterable of two values (minimum and maximum) to clip the jittered values to, default to None. None random_state Optional[numpy.number] An integer or 1-d array value used to set the random seed, default to None. None Returns: Type Description DataFrame A pandas DataFrame with a new column containing Gaussian-jittered values from another column. Exceptions: Type Description TypeError if column_name is not numeric. ValueError if scale is not a numerical value greater than 0 . ValueError if clip is not an iterable of length 2 . ValueError if clip[0] is not less than clip[1] . Source code in janitor/functions/jitter.py @pf.register_dataframe_method def jitter( df: pd.DataFrame, column_name: Hashable, dest_column_name: str, scale: np.number, clip: Optional[Iterable[np.number]] = None, random_state: Optional[np.number] = None, ) -> pd.DataFrame: \"\"\" Adds Gaussian noise (jitter) to the values of a column. Functional usage syntax: ```python import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.functions.jitter( df=df, column_name='values', dest_column_name='values_jitter', scale=1.0, clip=None, random_state=None, ) ``` Method chaining usage example: ``` import pandas as pd import janitor df = pd.DataFrame(...) df = df.jitter( column_name='values', dest_column_name='values_jitter', scale=1.0, clip=None, random_state=None, ) ``` A new column will be created containing the values of the original column with Gaussian noise added. For each value in the column, a Gaussian distribution is created having a location (mean) equal to the value and a scale (standard deviation) equal to `scale`. A random value is then sampled from this distribution, which is the jittered value. If a tuple is supplied for `clip`, then any values of the new column less than `clip[0]` will be set to `clip[0]`, and any values greater than `clip[1]` will be set to `clip[1]`. Additionally, if a numeric value is supplied for `random_state`, this value will be used to set the random seed used for sampling. NaN values are ignored in this method. This method mutates the original DataFrame. :param df: A pandas DataFrame. :param column_name: Name of the column containing values to add Gaussian jitter to. :param dest_column_name: The name of the new column containing the jittered values that will be created. :param scale: A positive value multiplied by the original column value to determine the scale (standard deviation) of the Gaussian distribution to sample from. (A value of zero results in no jittering.) :param clip: An iterable of two values (minimum and maximum) to clip the jittered values to, default to None. :param random_state: An integer or 1-d array value used to set the random seed, default to None. :returns: A pandas DataFrame with a new column containing Gaussian-jittered values from another column. :raises TypeError: if `column_name` is not numeric. :raises ValueError: if `scale` is not a numerical value greater than `0`. :raises ValueError: if `clip` is not an iterable of length `2`. :raises ValueError: if `clip[0]` is not less than `clip[1]`. \"\"\" # Check types check(\"scale\", scale, [int, float]) # Check that `column_name` is a numeric column if not np.issubdtype(df[column_name].dtype, np.number): raise TypeError(f\"{column_name} must be a numeric column.\") if scale <= 0: raise ValueError(\"`scale` must be a numeric value greater than 0.\") values = df[column_name] if random_state is not None: np.random.seed(random_state) result = np.random.normal(loc=values, scale=scale) if clip: # Ensure `clip` has length 2 if len(clip) != 2: raise ValueError(\"`clip` must be an iterable of length 2.\") # Ensure the values in `clip` are ordered as min, max if clip[1] < clip[0]: raise ValueError(\"`clip[0]` must be less than `clip[1]`.\") result = np.clip(result, *clip) df[dest_column_name] = result return df","title":"jitter()"},{"location":"api/functions/#janitor.functions.join_apply","text":"","title":"join_apply"},{"location":"api/functions/#janitor.functions.join_apply.join_apply","text":"Join the result of applying a function across dataframe rows. This method does not mutate the original DataFrame. This is a convenience function that allows us to apply arbitrary functions that take any combination of information from any of the columns. The only requirement is that the function signature takes in a row from the DataFrame. The example below shows us how to sum the result of two columns into a new column. df = ( pd.DataFrame({'a':[1, 2, 3], 'b': [2, 3, 4]}) .join_apply(lambda x: 2 * x['a'] + x['b'], new_column_name=\"2a+b\") ) This following example shows us how to use conditionals in the same function. def take_a_if_even(x): if x['a'] % 2: return x['a'] else: return x['b'] df = ( pd.DataFrame({'a': [1, 2, 3], 'b': [2, 3, 4]}) .join_apply(take_a_if_even, 'a_if_even') ) Parameters: Name Type Description Default df DataFrame A pandas DataFrame required func Callable A function that is applied elementwise across all rows of the DataFrame. required new_column_name str New column name. required Returns: Type Description DataFrame A pandas DataFrame with new column appended. Source code in janitor/functions/join_apply.py @pf.register_dataframe_method def join_apply( df: pd.DataFrame, func: Callable, new_column_name: str ) -> pd.DataFrame: \"\"\" Join the result of applying a function across dataframe rows. This method does not mutate the original DataFrame. This is a convenience function that allows us to apply arbitrary functions that take any combination of information from any of the columns. The only requirement is that the function signature takes in a row from the DataFrame. The example below shows us how to sum the result of two columns into a new column. ```python df = ( pd.DataFrame({'a':[1, 2, 3], 'b': [2, 3, 4]}) .join_apply(lambda x: 2 * x['a'] + x['b'], new_column_name=\"2a+b\") ) ``` This following example shows us how to use conditionals in the same function. ```python def take_a_if_even(x): if x['a'] % 2: return x['a'] else: return x['b'] df = ( pd.DataFrame({'a': [1, 2, 3], 'b': [2, 3, 4]}) .join_apply(take_a_if_even, 'a_if_even') ) ``` :param df: A pandas DataFrame :param func: A function that is applied elementwise across all rows of the DataFrame. :param new_column_name: New column name. :returns: A pandas DataFrame with new column appended. \"\"\" df = df.copy().join(df.apply(func, axis=1).rename(new_column_name)) return df","title":"join_apply()"},{"location":"api/functions/#janitor.functions.label_encode","text":"","title":"label_encode"},{"location":"api/functions/#janitor.functions.label_encode.label_encode","text":"Convert labels into numerical data. This method will create a new column with the string _enc appended after the original column's name. Consider this to be syntactic sugar. This method behaves differently from encode_categorical . This method creates a new column of numeric data. encode_categorical replaces the dtype of the original column with a categorical dtype. This method mutates the original DataFrame. Functional usage syntax: df = label_encode(df, column_names=\"my_categorical_column\") # one way Method chaining syntax: import pandas as pd import janitor categorical_cols = ['col1', 'col2', 'col4'] df = pd.DataFrame(...).label_encode(column_names=categorical_cols) Parameters: Name Type Description Default df DataFrame The pandas DataFrame object. required column_names Union[str, Iterable[str], Hashable] A column name or an iterable (list or tuple) of column names. required Returns: Type Description DataFrame A pandas DataFrame. Source code in janitor/functions/label_encode.py @pf.register_dataframe_method @deprecated_alias(columns=\"column_names\") def label_encode( df: pd.DataFrame, column_names: Union[str, Iterable[str], Hashable] ) -> pd.DataFrame: \"\"\" Convert labels into numerical data. This method will create a new column with the string `_enc` appended after the original column's name. Consider this to be syntactic sugar. This method behaves differently from `encode_categorical`. This method creates a new column of numeric data. `encode_categorical` replaces the dtype of the original column with a *categorical* dtype. This method mutates the original DataFrame. Functional usage syntax: ```python df = label_encode(df, column_names=\"my_categorical_column\") # one way ``` Method chaining syntax: ```python import pandas as pd import janitor categorical_cols = ['col1', 'col2', 'col4'] df = pd.DataFrame(...).label_encode(column_names=categorical_cols) ``` :param df: The pandas DataFrame object. :param column_names: A column name or an iterable (list or tuple) of column names. :returns: A pandas DataFrame. \"\"\" warnings.warn( \"label_encode will be deprecated in a 1.x release. \\ Please use factorize_columns instead\" ) df = _factorize(df, column_names, \"_enc\") return df","title":"label_encode()"},{"location":"api/functions/#janitor.functions.limit_column_characters","text":"","title":"limit_column_characters"},{"location":"api/functions/#janitor.functions.limit_column_characters.limit_column_characters","text":"Truncate column sizes to a specific length. This method mutates the original DataFrame. Method chaining will truncate all columns to a given length and append a given separator character with the index of duplicate columns, except for the first distinct column name. Parameters: Name Type Description Default df DataFrame A pandas dataframe. required column_length int Character length for which to truncate all columns. The column separator value and number for duplicate column name does not contribute. Therefore, if all columns are truncated to 10 characters, the first distinct column will be 10 characters and the remaining will be 12 characters (assuming a column separator of one character). required col_separator str The separator to use for counting distinct column values. I think an underscore looks nicest, however a period is a common option as well. Supply an empty string (i.e. '') to remove the separator. '_' Returns: Type Description DataFrame A pandas DataFrame with truncated column lengths. Source code in janitor/functions/limit_column_characters.py @pf.register_dataframe_method def limit_column_characters( df: pd.DataFrame, column_length: int, col_separator: str = \"_\" ) -> pd.DataFrame: \"\"\"Truncate column sizes to a specific length. This method mutates the original DataFrame. Method chaining will truncate all columns to a given length and append a given separator character with the index of duplicate columns, except for the first distinct column name. :param df: A pandas dataframe. :param column_length: Character length for which to truncate all columns. The column separator value and number for duplicate column name does not contribute. Therefore, if all columns are truncated to 10 characters, the first distinct column will be 10 characters and the remaining will be 12 characters (assuming a column separator of one character). :param col_separator: The separator to use for counting distinct column values. I think an underscore looks nicest, however a period is a common option as well. Supply an empty string (i.e. '') to remove the separator. :returns: A pandas DataFrame with truncated column lengths. \"\"\" # :Example Setup: # ```python # import pandas as pd # import janitor # data_dict = { # \"really_long_name_for_a_column\": range(10), # \"another_really_long_name_for_a_column\": \\ # [2 * item for item in range(10)], # \"another_really_longer_name_for_a_column\": list(\"lllongname\"), # \"this_is_getting_out_of_hand\": list(\"longername\"), # } # :Example: Standard truncation: # ```python # example_dataframe = pd.DataFrame(data_dict) # example_dataframe.limit_column_characters(7) # :Output: # ```python # really_ another another_1 this_is # 0 0 0 l l # 1 1 2 l o # 2 2 4 l n # 3 3 6 o g # 4 4 8 n e # 5 5 10 g r # 6 6 12 n n # 7 7 14 a a # 8 8 16 m m # 9 9 18 e e # :Example: Standard truncation with different separator character: # ```python # example_dataframe2 = pd.DataFrame(data_dict) # example_dataframe2.limit_column_characters(7, \".\") # ```python # really_ another another.1 this_is # 0 0 0 l l # 1 1 2 l o # 2 2 4 l n # 3 3 6 o g # 4 4 8 n e # 5 5 10 g r # 6 6 12 n n # 7 7 14 a a # 8 8 16 m m # 9 9 18 e e check(\"column_length\", column_length, [int]) check(\"col_separator\", col_separator, [str]) col_names = df.columns col_names = [col_name[:column_length] for col_name in col_names] col_name_set = set(col_names) col_name_count = {} # If no columns are duplicates, we can skip the loops below. if len(col_name_set) == len(col_names): df.columns = col_names return df for col_name_to_check in col_name_set: count = 0 for idx, col_name in enumerate(col_names): if col_name_to_check == col_name: col_name_count[idx] = count count += 1 final_col_names = [] for idx, col_name in enumerate(col_names): if col_name_count[idx] > 0: col_name_to_append = ( col_name + col_separator + str(col_name_count[idx]) ) final_col_names.append(col_name_to_append) else: final_col_names.append(col_name) df.columns = final_col_names return df","title":"limit_column_characters()"},{"location":"api/functions/#janitor.functions.min_max_scale","text":"","title":"min_max_scale"},{"location":"api/functions/#janitor.functions.min_max_scale.min_max_scale","text":"Scales data to between a minimum and maximum value. This method mutates the original DataFrame. If minimum and maximum are provided, the true min/max of the DataFrame or column is ignored in the scaling process and replaced with these values, instead. One can optionally set a new target minimum and maximum value using the new_min and new_max keyword arguments. This will result in the transformed data being bounded between new_min and new_max . If a particular column name is specified, then only that column of data are scaled. Otherwise, the entire dataframe is scaled. Method chaining syntax: df = pd.DataFrame(...).min_max_scale(column_name=\"a\") Setting custom minimum and maximum: df = ( pd.DataFrame(...) .min_max_scale( column_name=\"a\", new_min=2, new_max=10 ) ) Setting a min and max that is not based on the data, while applying to entire dataframe: df = ( pd.DataFrame(...) .min_max_scale( old_min=0, old_max=14, new_min=0, new_max=1, ) ) The aforementioned example might be applied to something like scaling the isoelectric points of amino acids. While technically they range from approx 3-10, we can also think of them on the pH scale which ranges from 1 to 14. Hence, 3 gets scaled not to 0 but approx. 0.15 instead, while 10 gets scaled to approx. 0.69 instead. Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required old_min (optional) Overrides for the current minimum value of the data to be transformed. None old_max (optional) Overrides for the current maximum value of the data to be transformed. None new_min (optional) The minimum value of the data after it has been scaled. 0 new_max (optional) The maximum value of the data after it has been scaled. 1 column_name (optional) The column on which to perform scaling. None Returns: Type Description DataFrame A pandas DataFrame with scaled data. Exceptions: Type Description ValueError if old_max is not greater than `old_min``. ValueError if new_max is not greater than `new_min``. Source code in janitor/functions/min_max_scale.py @pf.register_dataframe_method @deprecated_alias(col_name=\"column_name\") def min_max_scale( df: pd.DataFrame, old_min=None, old_max=None, column_name=None, new_min=0, new_max=1, ) -> pd.DataFrame: \"\"\" Scales data to between a minimum and maximum value. This method mutates the original DataFrame. If `minimum` and `maximum` are provided, the true min/max of the `DataFrame` or column is ignored in the scaling process and replaced with these values, instead. One can optionally set a new target minimum and maximum value using the `new_min` and `new_max` keyword arguments. This will result in the transformed data being bounded between `new_min` and `new_max`. If a particular column name is specified, then only that column of data are scaled. Otherwise, the entire dataframe is scaled. Method chaining syntax: ```python df = pd.DataFrame(...).min_max_scale(column_name=\"a\") ``` Setting custom minimum and maximum: ```python df = ( pd.DataFrame(...) .min_max_scale( column_name=\"a\", new_min=2, new_max=10 ) ) ``` Setting a min and max that is not based on the data, while applying to entire dataframe: ```python df = ( pd.DataFrame(...) .min_max_scale( old_min=0, old_max=14, new_min=0, new_max=1, ) ) ``` The aforementioned example might be applied to something like scaling the isoelectric points of amino acids. While technically they range from approx 3-10, we can also think of them on the pH scale which ranges from 1 to 14. Hence, 3 gets scaled not to 0 but approx. 0.15 instead, while 10 gets scaled to approx. 0.69 instead. :param df: A pandas DataFrame. :param old_min: (optional) Overrides for the current minimum value of the data to be transformed. :param old_max: (optional) Overrides for the current maximum value of the data to be transformed. :param new_min: (optional) The minimum value of the data after it has been scaled. :param new_max: (optional) The maximum value of the data after it has been scaled. :param column_name: (optional) The column on which to perform scaling. :returns: A pandas DataFrame with scaled data. :raises ValueError: if `old_max` is not greater than `old_min``. :raises ValueError: if `new_max` is not greater than `new_min``. \"\"\" if ( (old_min is not None) and (old_max is not None) and (old_max <= old_min) ): raise ValueError(\"`old_max` should be greater than `old_min`\") if new_max <= new_min: raise ValueError(\"`new_max` should be greater than `new_min`\") new_range = new_max - new_min if column_name: if old_min is None: old_min = df[column_name].min() if old_max is None: old_max = df[column_name].max() old_range = old_max - old_min df[column_name] = ( df[column_name] - old_min ) * new_range / old_range + new_min else: if old_min is None: old_min = df.min().min() if old_max is None: old_max = df.max().max() old_range = old_max - old_min df = (df - old_min) * new_range / old_range + new_min return df","title":"min_max_scale()"},{"location":"api/functions/#janitor.functions.move","text":"","title":"move"},{"location":"api/functions/#janitor.functions.move.move","text":"Move column or row to a position adjacent to another column or row in dataframe. Must have unique column names or indices. This operation does not reset the index of the dataframe. User must explicitly do so. Does not apply to multilevel dataframes. Functional usage syntax: df = move(df, source=3, target=15, position='after', axis=0) Method chaining syntax: import pandas as pd import janitor df = ( pd.DataFrame(...) .move(source=3, target=15, position='after', axis=0) ) Parameters: Name Type Description Default df DataFrame The pandas Dataframe object. required source Union[int, str] column or row to move required target Union[int, str] column or row to move adjacent to required position str Specifies whether the Series is moved to before or after the adjacent Series. Values can be either before or after ; defaults to before . 'before' axis int Axis along which the function is applied. 0 to move a row, 1 to move a column. 0 Returns: Type Description DataFrame The dataframe with the Series moved. Exceptions: Type Description ValueError if axis is not 0 or `1``. ValueError if position is not before or `after``. ValueError if source row or column is not in dataframe. ValueError if target row or column is not in dataframe. Source code in janitor/functions/move.py @pf.register_dataframe_method def move( df: pd.DataFrame, source: Union[int, str], target: Union[int, str], position: str = \"before\", axis: int = 0, ) -> pd.DataFrame: \"\"\" Move column or row to a position adjacent to another column or row in dataframe. Must have unique column names or indices. This operation does not reset the index of the dataframe. User must explicitly do so. Does not apply to multilevel dataframes. Functional usage syntax: ```python df = move(df, source=3, target=15, position='after', axis=0) ``` Method chaining syntax: ```python import pandas as pd import janitor df = ( pd.DataFrame(...) .move(source=3, target=15, position='after', axis=0) ) ``` :param df: The pandas Dataframe object. :param source: column or row to move :param target: column or row to move adjacent to :param position: Specifies whether the Series is moved to before or after the adjacent Series. Values can be either `before` or `after`; defaults to `before`. :param axis: Axis along which the function is applied. 0 to move a row, 1 to move a column. :returns: The dataframe with the Series moved. :raises ValueError: if `axis` is not `0` or `1``. :raises ValueError: if `position` is not `before` or `after``. :raises ValueError: if `source` row or column is not in dataframe. :raises ValueError: if `target` row or column is not in dataframe. \"\"\" df = df.copy() if axis not in [0, 1]: raise ValueError(f\"Invalid axis '{axis}'. Can only be 0 or 1.\") if position not in [\"before\", \"after\"]: raise ValueError( f\"Invalid position '{position}'. Can only be 'before' or 'after'.\" ) if axis == 0: names = list(df.index) if source not in names: raise ValueError(f\"Source row '{source}' not in dataframe.\") if target not in names: raise ValueError(f\"Target row '{target}' not in dataframe.\") names.remove(source) pos = names.index(target) if position == \"after\": pos += 1 names.insert(pos, source) df = df.loc[names, :] else: names = list(df.columns) if source not in names: raise ValueError(f\"Source column '{source}' not in dataframe.\") if target not in names: raise ValueError(f\"Target column '{target}' not in dataframe.\") names.remove(source) pos = names.index(target) if position == \"after\": pos += 1 names.insert(pos, source) df = df.loc[:, names] return df","title":"move()"},{"location":"api/functions/#janitor.functions.pivot","text":"","title":"pivot"},{"location":"api/functions/#janitor.functions.pivot.pivot_longer","text":"Unpivots a DataFrame from wide to long format. This method does not mutate the original DataFrame. It is a wrapper around pd.melt and is meant to serve as a single point for transformations that require pd.melt or pd.wide_to_long . It is modeled after the pivot_longer function in R's tidyr package, and offers more functionality and flexibility than pd.wide_to_long . This function is useful to massage a DataFrame into a format where one or more columns are considered measured variables, and all other columns are considered as identifier variables. All measured variables are unpivoted (and typically duplicated) along the row axis. Example 1: The following DataFrame contains heartrate data for patients treated with two different drugs, a and b . name a b 0 Wilbur 67 56 1 Petunia 80 90 2 Gregory 64 50 The column names a and b are actually the names of a measured variable (i.e. the name of a drug), but the values are a different measured variable (heartrate). We would like to unpivot these a and b columns into a drug column and a heartrate column. df.pivot_longer( column_names = ['a', 'b'], names_to = 'drug', values_to = 'heartrate', sort_by_appearance = True ) name drug heartrate 0 Wilbur a 67 1 Wilbur b 56 2 Petunia a 80 3 Petunia b 90 4 Gregory a 64 5 Gregory b 50 Note how the data is stacked in order of first appearance. If, however, you do not care for order of appearance, and want to wring out some more performance, you can set sort_by_appearance to False (the default is False ). df.pivot_longer( column_names = ['a', 'b'], names_to = 'drug', values_to = 'heartrate', sort_by_appearance = False ) name drug heartrate 0 Wilbur a 67 1 Petunia a 80 2 Gregory a 64 3 Wilbur b 56 4 Petunia b 90 5 Gregory b 50 You can set ignore_index to False , if you wish to reuse the index from the source DataFrame (the index will be repeated as many times as necessary): df.pivot_longer( column_names = ['a', 'b'], names_to = 'drug', values_to = 'heartrate', sort_by_appearance = False, ignore_index = False ) name drug heartrate 0 Wilbur a 67 1 Petunia a 80 2 Gregory a 64 0 Wilbur b 56 1 Petunia b 90 2 Gregory b 50 MultiIndex DataFrames are unpivoted in the same form that you would expect from pandas' melt : A B C D E F 0 a 1 2 1 b 3 4 2 c 5 6 df.pivot_longer( index = [(\"A\", \"D\")], names_to = [\"first\", \"second\"] ) (A, D) first second value 0 a B E 1 1 b B E 3 2 c B E 5 3 a C F 2 4 b C F 4 5 c C F 6 You can also unpivot on a specific level: df.pivot_longer( index = \"A\", names_to = \"first\", column_level = 0 ) A first value 0 a B 1 1 b B 3 2 c B 5 Example 2: The DataFrame below has year and month variables embedded within the column names. col1 2019-12 2020-01 2020-02 0 a -1.085631 -1.506295 -2.426679 1 b 0.997345 -0.578600 -0.428913 2 c 0.282978 1.651437 1.265936 pivot_longer can conveniently reshape the DataFrame into long format, with new columns for the year and month. You simply pass in the new column names to names_to , and pass the hyphen - to the names_sep argument. df.pivot_longer( index = 'col1', names_to = ('year', 'month'), names_sep = '-', sort_by_appearance = True ) col1 year month value 0 a 2019 12 -1.085631 1 a 2020 01 -1.506295 2 a 2020 02 -2.426679 3 b 2019 12 0.997345 4 b 2020 01 -0.578600 5 b 2020 02 -0.428913 6 c 2019 12 0.282978 7 c 2020 01 1.651437 8 c 2020 02 1.265936 Example 3: The DataFrame below has names embedded in it (measure1, measure2) that we would love to reuse as column names. treat1-measure1 treat1-measure2 treat2-measure1 treat2-measure2 0 1 4 2 5 1 2 5 3 4 For this, we use the .value variable, which signals to pivot_longer to treat the part of the column names corresponding to .value as new column names. The .value variable is similar to stubnames in pandas' wide_to_long function, but with more flexibility. df.pivot_longer( names_to = (\"group\", '.value'), names_sep = '-', sort_by_appearance = True ) group measure1 measure2 0 treat1 1 4 1 treat2 2 5 2 treat1 2 5 3 treat2 3 4 Let's break down the .value idea. When .value is used, pivot_longer creates a pairing. In the example above, we get a pairing {\"group\":[\"treat1\", \"treat2\"], \".value\":[\"measure1\", \"measure2\"]} . All the values associated with .value become new column names, while those not associated with .value ( treat1 and treat2 ) become values in a new column group . values_to is overridden during this process. Note The values not associated with .value (in the example above, this is the group column) are returned as object dtypes. You can change it to your preferred dtype using pandas' astype method. Example 4: You can also unpivot from wide to long using regular expressions n_1 n_2 n_3 pct_1 pct_2 pct_3 0 10 20 30 0.1 0.2 0.3 df.pivot_longer( names_to = (\".value\", \"name\"), names_pattern = \"(.*)_(.)\" ) name n pct 0 1 10.0 0.1 1 2 20.0 0.2 2 3 30.0 0.3 The same idea of .value works here as well. Based on the capturing groups in the regex in names_pattern , we have two pairings --> {\".value\":[\"n\", \"pct\"], \"name\":[1,2,3]} . Just like in the previous example, the values associated with .value become new column names, while those not associated with .value become values in the new column name . Note There are no limits to the pairing; however, you can only have one .value in names_to . Example 5: You can also pass a list/tuple of regular expressions that match specific patterns to names_pattern , along with a list/tuple of new names to names_to ; this can come in handy if .value falls short: GameID Date Visitor Score_V Home Score_H 0 1 9/10/2020 Houston Texans 20 Kansas City Chiefs 34 1 2 9/13/2020 Seattle Seahawks 38 Atlanta Falcons 25 df.pivot_longer( index = ['GameID','Date'], names_to = (\"Team\",\"Score\"), names_pattern = (\"^Visitor|Home\", \"^Score\") ) GameID Date Team Score 0 1 9/10/2020 Houston Texans 20 1 2 9/13/2020 Seattle Seahawks 38 2 1 9/10/2020 Kansas City Chiefs 34 3 2 9/13/2020 Atlanta Falcons 25 Note that in the code above, the number of entries in both names_to and names_pattern must match. Essentially, what the code does is look for columns that start with Visitor or Home (using the regex supplied) and puts all the values associated with these columns under a new column name Team . It then looks for columns that start with Score and collate all the values associated with these columns to a single column named Score . You can also take advantage of janitor.patterns function, or the select_columns syntax, which allows selection of columns via a regular expression; this can come in handy if you have a lot of column names to pass to the index or column_names parameters, and you do not wish to manually type them all. name wk1 wk2 wk3 wk4 0 Alice 5 9 20 22 1 Bob 7 11 17 33 2 Carla 6 13 39 40 df.pivot_longer(index = janitor.patterns(\"^(?!wk)\")) name variable value 0 Alice wk1 5 1 Bob wk1 7 2 Carla wk1 6 3 Alice wk2 9 4 Bob wk2 11 5 Carla wk2 13 6 Alice wk3 20 7 Bob wk3 17 8 Carla wk3 39 9 Alice wk4 22 10 Bob wk4 33 11 Carla wk4 40 Note Unpivoting a DataFrame with MultiIndex columns, when either names_sep or names_pattern is provided is not supported. Functional usage syntax: import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.pivot_longer( df = df, index = [column1, column2, ...], column_names = [column3, column4, ...], names_to = new_column_name, names_sep = string/regular expression, names_pattern = string/regular expression, values_to= new_column_name, column_level = None/int/str, sort_by_appearance = True/False, ignore_index = True/False, ) Method chaining syntax: df = ( pd.DataFrame(...) .pivot_longer( index = [column1, column2, ...], column_names = [column3, column4, ...], names_to = new_column_name, names_sep = string/regular expression, names_pattern = string/regular expression, values_to = new_column_name, column_level = None/int/str, sort_by_appearance = True/False, ignore_index = True/False, ) ) Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required index Union[List, Tuple, str, Pattern] Name(s) of columns to use as identifier variables. Should be either a single column name, or a list/tuple of column names. The janitor.select_columns syntax is supported here, allowing for flexible and dynamic column selection. Index should be a list of tuples if the columns are a MultiIndex. None column_names Union[List, Tuple, str, Pattern] Name(s) of columns to unpivot. Should be either a single column name or a list/tuple of column names. The janitor.select_columns syntax is supported here, allowing for flexible and dynamic column selection. Column_names should be a list of tuples if the columns are a MultiIndex. None names_to Union[List, Tuple, str] Name of new column as a string that will contain what were previously the column names in column_names . The default is variable if no value is provided. It can also be a list/tuple of strings that will serve as new column names, if name_sep or names_pattern is provided. If .value is in names_to , new column names will be extracted from part of the existing column names and overrides values_to . None names_sep Union[str, Pattern] Determines how the column name is broken up, if names_to contains multiple values. It takes the same specification as pandas' str.split method, and can be a string or regular expression. names_sep does not work with MultiIndex columns. None names_pattern Union[List, Tuple, str, Pattern] Determines how the column name is broken up. It can be a regular expression containing matching groups (it takes the same specification as pandas' str.extract method), or a list/tuple of regular expressions. If it is a single regex, the number of groups must match the length of names_to ( if the length of names_to is 3, then the number of groups must be 3. If names_to is a string, then there should be only one group in names_pattern``). For a list/tuple of regular expressions, names_to must also be a list/tuple and the lengths of both arguments must match(if the length of names_to is 4, then the length of names_pattern must also be 4). The entries in both arguments must also match positionally, i.e if names_to = (\"name1\", \"name2\", \"name3\") , then `names_pattern should be (\"regex1\", \"regex2\", \"regex3\"), with \"name1\" pairing \"regex1\", \"name2\" pairing \"regex2\", and \"name3\" pairing \"regex3\". names_pattern does not work with MultiIndex columns. None values_to Optional[str] Name of new column as a string that will contain what were previously the values of the columns in column_names . 'value' column_level Union[int, str] If columns are a MultiIndex, then use this level to unpivot the DataFrame. Provided for compatibility with pandas' melt, and applies only if neither names_sep nor names_pattern is provided. None sort_by_appearance Optional[bool] Default False . Boolean value that determines the final look of the DataFrame. If True , the unpivoted DataFrame will be stacked in order of first appearance. See examples for more details. pivot_longer is usually more performant if sort_by_appearance is `False``. False ignore_index Optional[bool] Default True . If True, original index is ignored. If False, the original index is retained and the Index labels will be repeated as necessary. True Returns: Type Description DataFrame A pandas DataFrame that has been unpivoted from wide to long format. Source code in janitor/functions/pivot.py @pf.register_dataframe_method def pivot_longer( df: pd.DataFrame, index: Optional[Union[List, Tuple, str, Pattern]] = None, column_names: Optional[Union[List, Tuple, str, Pattern]] = None, names_to: Optional[Union[List, Tuple, str]] = None, values_to: Optional[str] = \"value\", column_level: Optional[Union[int, str]] = None, names_sep: Optional[Union[str, Pattern]] = None, names_pattern: Optional[Union[List, Tuple, str, Pattern]] = None, sort_by_appearance: Optional[bool] = False, ignore_index: Optional[bool] = True, ) -> pd.DataFrame: \"\"\" Unpivots a DataFrame from *wide* to *long* format. This method does not mutate the original DataFrame. It is a wrapper around `pd.melt` and is meant to serve as a single point for transformations that require `pd.melt` or `pd.wide_to_long`. It is modeled after the `pivot_longer` function in R's tidyr package, and offers more functionality and flexibility than `pd.wide_to_long`. This function is useful to massage a DataFrame into a format where one or more columns are considered measured variables, and all other columns are considered as identifier variables. All measured variables are *unpivoted* (and typically duplicated) along the row axis. Example 1: The following DataFrame contains heartrate data for patients treated with two different drugs, `a` and `b`. ```python name a b 0 Wilbur 67 56 1 Petunia 80 90 2 Gregory 64 50 ``` The column names `a` and `b` are actually the names of a measured variable (i.e. the name of a drug), but the values are a different measured variable (heartrate). We would like to unpivot these `a` and `b` columns into a `drug` column and a `heartrate` column. ```python df.pivot_longer( column_names = ['a', 'b'], names_to = 'drug', values_to = 'heartrate', sort_by_appearance = True ) name drug heartrate 0 Wilbur a 67 1 Wilbur b 56 2 Petunia a 80 3 Petunia b 90 4 Gregory a 64 5 Gregory b 50 ``` Note how the data is stacked in order of first appearance. If, however, you do not care for order of appearance, and want to wring out some more performance, you can set `sort_by_appearance` to `False` (the default is `False`). ```python df.pivot_longer( column_names = ['a', 'b'], names_to = 'drug', values_to = 'heartrate', sort_by_appearance = False ) name drug heartrate 0 Wilbur a 67 1 Petunia a 80 2 Gregory a 64 3 Wilbur b 56 4 Petunia b 90 5 Gregory b 50 ``` You can set `ignore_index` to `False`, if you wish to reuse the index from the source DataFrame (the index will be repeated as many times as necessary): ```python df.pivot_longer( column_names = ['a', 'b'], names_to = 'drug', values_to = 'heartrate', sort_by_appearance = False, ignore_index = False ) name drug heartrate 0 Wilbur a 67 1 Petunia a 80 2 Gregory a 64 0 Wilbur b 56 1 Petunia b 90 2 Gregory b 50 ``` MultiIndex DataFrames are unpivoted in the same form that you would expect from pandas' `melt`: ```python A B C D E F 0 a 1 2 1 b 3 4 2 c 5 6 df.pivot_longer( index = [(\"A\", \"D\")], names_to = [\"first\", \"second\"] ) (A, D) first second value 0 a B E 1 1 b B E 3 2 c B E 5 3 a C F 2 4 b C F 4 5 c C F 6 ``` You can also unpivot on a specific level: ```python df.pivot_longer( index = \"A\", names_to = \"first\", column_level = 0 ) A first value 0 a B 1 1 b B 3 2 c B 5 ``` Example 2: The DataFrame below has year and month variables embedded within the column names. ```python col1 2019-12 2020-01 2020-02 0 a -1.085631 -1.506295 -2.426679 1 b 0.997345 -0.578600 -0.428913 2 c 0.282978 1.651437 1.265936 ``` `pivot_longer` can conveniently reshape the DataFrame into long format, with new columns for the year and month. You simply pass in the new column names to `names_to`, and pass the hyphen `-` to the `names_sep` argument. ```python df.pivot_longer( index = 'col1', names_to = ('year', 'month'), names_sep = '-', sort_by_appearance = True ) col1 year month value 0 a 2019 12 -1.085631 1 a 2020 01 -1.506295 2 a 2020 02 -2.426679 3 b 2019 12 0.997345 4 b 2020 01 -0.578600 5 b 2020 02 -0.428913 6 c 2019 12 0.282978 7 c 2020 01 1.651437 8 c 2020 02 1.265936 ``` Example 3: The DataFrame below has names embedded in it `(measure1, measure2)` that we would love to reuse as column names. ```python treat1-measure1 treat1-measure2 treat2-measure1 treat2-measure2 0 1 4 2 5 1 2 5 3 4 ``` For this, we use the `.value` variable, which signals to `pivot_longer` to treat the part of the column names corresponding to `.value` as new column names. The `.value` variable is similar to `stubnames` in pandas' `wide_to_long` function, but with more flexibility. ```python df.pivot_longer( names_to = (\"group\", '.value'), names_sep = '-', sort_by_appearance = True ) group measure1 measure2 0 treat1 1 4 1 treat2 2 5 2 treat1 2 5 3 treat2 3 4 ``` Let's break down the `.value` idea. When `.value` is used, `pivot_longer` creates a pairing. In the example above, we get a pairing `{\"group\":[\"treat1\", \"treat2\"], \".value\":[\"measure1\", \"measure2\"]}`. All the values associated with `.value` become new column names, while those not associated with `.value`(`treat1` and `treat2`) become values in a new column `group`. `values_to` is overridden during this process. !!! note The values not associated with `.value` (in the example above, this is the `group` column) are returned as object dtypes. You can change it to your preferred dtype using pandas' `astype` method. Example 4: You can also unpivot from wide to long using regular expressions ```python n_1 n_2 n_3 pct_1 pct_2 pct_3 0 10 20 30 0.1 0.2 0.3 df.pivot_longer( names_to = (\".value\", \"name\"), names_pattern = \"(.*)_(.)\" ) name n pct 0 1 10.0 0.1 1 2 20.0 0.2 2 3 30.0 0.3 ``` The same idea of `.value` works here as well. Based on the capturing groups in the regex in `names_pattern`, we have two pairings --> `{\".value\":[\"n\", \"pct\"], \"name\":[1,2,3]}`. Just like in the previous example, the values associated with `.value` become new column names, while those not associated with `.value` become values in the new column `name`. !!!note There are no limits to the pairing; however, you can only have one `.value` in `names_to`. Example 5: You can also pass a list/tuple of regular expressions that match specific patterns to `names_pattern`, along with a list/tuple of new names to `names_to`; this can come in handy if `.value` falls short: ```python GameID Date Visitor Score_V Home Score_H 0 1 9/10/2020 Houston Texans 20 Kansas City Chiefs 34 1 2 9/13/2020 Seattle Seahawks 38 Atlanta Falcons 25 df.pivot_longer( index = ['GameID','Date'], names_to = (\"Team\",\"Score\"), names_pattern = (\"^Visitor|Home\", \"^Score\") ) GameID Date Team Score 0 1 9/10/2020 Houston Texans 20 1 2 9/13/2020 Seattle Seahawks 38 2 1 9/10/2020 Kansas City Chiefs 34 3 2 9/13/2020 Atlanta Falcons 25 ``` Note that in the code above, the number of entries in both `names_to` and `names_pattern` must match. Essentially, what the code does is look for columns that start with `Visitor` or `Home` (using the regex supplied) and puts all the values associated with these columns under a new column name `Team`. It then looks for columns that start with `Score` and collate all the values associated with these columns to a single column named `Score`. You can also take advantage of `janitor.patterns` function, or the `select_columns` syntax, which allows selection of columns via a regular expression; this can come in handy if you have a lot of column names to pass to the `index` or `column_names` parameters, and you do not wish to manually type them all. ```python name wk1 wk2 wk3 wk4 0 Alice 5 9 20 22 1 Bob 7 11 17 33 2 Carla 6 13 39 40 df.pivot_longer(index = janitor.patterns(\"^(?!wk)\")) name variable value 0 Alice wk1 5 1 Bob wk1 7 2 Carla wk1 6 3 Alice wk2 9 4 Bob wk2 11 5 Carla wk2 13 6 Alice wk3 20 7 Bob wk3 17 8 Carla wk3 39 9 Alice wk4 22 10 Bob wk4 33 11 Carla wk4 40 ``` !!!note Unpivoting a DataFrame with MultiIndex columns, when either `names_sep` or `names_pattern` is provided is not supported. Functional usage syntax: ```python import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.pivot_longer( df = df, index = [column1, column2, ...], column_names = [column3, column4, ...], names_to = new_column_name, names_sep = string/regular expression, names_pattern = string/regular expression, values_to= new_column_name, column_level = None/int/str, sort_by_appearance = True/False, ignore_index = True/False, ) ``` Method chaining syntax: ```python df = ( pd.DataFrame(...) .pivot_longer( index = [column1, column2, ...], column_names = [column3, column4, ...], names_to = new_column_name, names_sep = string/regular expression, names_pattern = string/regular expression, values_to = new_column_name, column_level = None/int/str, sort_by_appearance = True/False, ignore_index = True/False, ) ) ``` :param df: A pandas DataFrame. :param index: Name(s) of columns to use as identifier variables. Should be either a single column name, or a list/tuple of column names. The `janitor.select_columns` syntax is supported here, allowing for flexible and dynamic column selection. Index should be a list of tuples if the columns are a MultiIndex. :param column_names: Name(s) of columns to unpivot. Should be either a single column name or a list/tuple of column names. The `janitor.select_columns` syntax is supported here, allowing for flexible and dynamic column selection. Column_names should be a list of tuples if the columns are a MultiIndex. :param names_to: Name of new column as a string that will contain what were previously the column names in `column_names`. The default is `variable` if no value is provided. It can also be a list/tuple of strings that will serve as new column names, if `name_sep` or `names_pattern` is provided. If `.value` is in `names_to`, new column names will be extracted from part of the existing column names and overrides`values_to`. :param names_sep: Determines how the column name is broken up, if `names_to` contains multiple values. It takes the same specification as pandas' `str.split` method, and can be a string or regular expression. `names_sep` does not work with MultiIndex columns. :param names_pattern: Determines how the column name is broken up. It can be a regular expression containing matching groups (it takes the same specification as pandas' `str.extract` method), or a list/tuple of regular expressions. If it is a single regex, the number of groups must match the length of `names_to` ( if the length of `names_to` is 3, then the number of groups must be 3. If `names_to` is a string, then there should be only one group in `names_pattern``). For a list/tuple of regular expressions, `names_to` must also be a list/tuple and the lengths of both arguments must match(if the length of `names_to` is 4, then the length of `names_pattern` must also be 4). The entries in both arguments must also match positionally, i.e if `names_to = (\"name1\", \"name2\", \"name3\")``, then `names_pattern`` should be (\"regex1\", \"regex2\", \"regex3\"), with \"name1\" pairing \"regex1\", \"name2\" pairing \"regex2\", and \"name3\" pairing \"regex3\". `names_pattern` does not work with MultiIndex columns. :param values_to: Name of new column as a string that will contain what were previously the values of the columns in `column_names`. :param column_level: If columns are a MultiIndex, then use this level to unpivot the DataFrame. Provided for compatibility with pandas' melt, and applies only if neither `names_sep` nor `names_pattern` is provided. :param sort_by_appearance: Default `False`. Boolean value that determines the final look of the DataFrame. If `True`, the unpivoted DataFrame will be stacked in order of first appearance. See examples for more details. `pivot_longer` is usually more performant if `sort_by_appearance` is `False``. :param ignore_index: Default `True`. If True, original index is ignored. If False, the original index is retained and the Index labels will be repeated as necessary. :returns: A pandas DataFrame that has been unpivoted from wide to long format. \"\"\" # this code builds on the wonderful work of @benjaminjack\u2019s PR # https://github.com/benjaminjack/pyjanitor/commit/e3df817903c20dd21634461c8a92aec137963ed0 df = df.copy() ( df, index, column_names, names_to, values_to, column_level, names_sep, names_pattern, sort_by_appearance, ignore_index, ) = _data_checks_pivot_longer( df, index, column_names, names_to, values_to, column_level, names_sep, names_pattern, sort_by_appearance, ignore_index, ) return _computations_pivot_longer( df, index, column_names, names_to, values_to, column_level, names_sep, names_pattern, sort_by_appearance, ignore_index, )","title":"pivot_longer()"},{"location":"api/functions/#janitor.functions.pivot.pivot_wider","text":"Reshapes data from 'long' to 'wide' form. The number of columns are increased, while decreasing the number of rows. It is the inverse of the pivot_longer method, and is a wrapper around pd.DataFrame.pivot method. This method does not mutate the original DataFrame. .. note:: Column selection in index , names_from and values_from is possible using the janitor.select_columns syntax. .. note:: A ValueError is raised if the combination of the index and names_from is not unique. .. note:: By default, values from values_from are always at the top level if the columns are not flattened. If flattened, the values from values_from are usually at the start of each label in the columns. Functional usage syntax: import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.pivot_wider( df = df, index = [column1, column2, ...], names_from = [column3, column4, ...], value_from = [column5, column6, ...], levels_order = None/list, flatten_levels = True/False, names_sep='_', names_glue= Callable ) Method chaining syntax: df = ( pd.DataFrame(...) .pivot_wider( index = [column1, column2, ...], names_from = [column3, column4, ...], value_from = [column5, column6, ...], levels_order = None/list, flatten_levels = True/False, names_sep='_', names_glue= Callable ) ) Parameters: Name Type Description Default df DataFrame A pandas dataframe. required index Union[List, str] Name(s) of columns to use as identifier variables. Should be either a single column name, or a list of column names. The janitor.select_columns syntax is supported here, allowing for flexible and dynamic column selection. If index is not provided, the dataframe's index is used. None names_from Union[List, str] Name(s) of column(s) to use to make the new dataframe's columns. Should be either a single column name, or a list of column names. The janitor.select_columns syntax is supported here, allowing for flexible and dynamic column selection. A label or labels must be provided for names_from . None values_from Union[List, str] Name(s) of column(s) that will be used for populating the new dataframe's values. The janitor.select_columns syntax is supported here, allowing for flexible and dynamic column selection. If values_from is not specified, all remaining columns will be used. Note that values from values_from are usually at the top level, the dataframe's columns is not flattened, or the start of each label in the columns, if flattened. None levels_order Optional[list] Applicable if there are multiple names_from and/or values_from . Reorders the levels. Accepts a list of strings. If there are multiple values_from , pass a None to represent that level. None flatten_levels Optional[bool] Default is True . If False , the dataframe stays as a MultiIndex. True names_sep If names_from or values_from contain multiple variables, this will be used to join their values into a single string to use as a column name. Default is _ . Applicable only if flatten_levels is True . '_' names_glue Callable A callable to control the output of the flattened columns. Applicable only if flatten_levels is True. Function should be acceptable to pandas\u2019 map function. None Returns: Type Description DataFrame A pandas DataFrame that has been unpivoted from long to wide form. Source code in janitor/functions/pivot.py @pf.register_dataframe_method def pivot_wider( df: pd.DataFrame, index: Optional[Union[List, str]] = None, names_from: Optional[Union[List, str]] = None, values_from: Optional[Union[List, str]] = None, levels_order: Optional[list] = None, flatten_levels: Optional[bool] = True, names_sep=\"_\", names_glue: Callable = None, ) -> pd.DataFrame: \"\"\" Reshapes data from 'long' to 'wide' form. The number of columns are increased, while decreasing the number of rows. It is the inverse of the `pivot_longer` method, and is a wrapper around `pd.DataFrame.pivot` method. This method does not mutate the original DataFrame. .. note:: Column selection in `index`, `names_from` and `values_from` is possible using the `janitor.select_columns` syntax. .. note:: A ValueError is raised if the combination of the `index` and `names_from` is not unique. .. note:: By default, values from `values_from` are always at the top level if the columns are not flattened. If flattened, the values from `values_from` are usually at the start of each label in the columns. Functional usage syntax: ```python import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.pivot_wider( df = df, index = [column1, column2, ...], names_from = [column3, column4, ...], value_from = [column5, column6, ...], levels_order = None/list, flatten_levels = True/False, names_sep='_', names_glue= Callable ) ``` Method chaining syntax: ```python df = ( pd.DataFrame(...) .pivot_wider( index = [column1, column2, ...], names_from = [column3, column4, ...], value_from = [column5, column6, ...], levels_order = None/list, flatten_levels = True/False, names_sep='_', names_glue= Callable ) ) ``` :param df: A pandas dataframe. :param index: Name(s) of columns to use as identifier variables. Should be either a single column name, or a list of column names. The `janitor.select_columns` syntax is supported here, allowing for flexible and dynamic column selection. If `index` is not provided, the dataframe's index is used. :param names_from: Name(s) of column(s) to use to make the new dataframe's columns. Should be either a single column name, or a list of column names. The `janitor.select_columns` syntax is supported here, allowing for flexible and dynamic column selection. A label or labels must be provided for `names_from`. :param values_from: Name(s) of column(s) that will be used for populating the new dataframe's values. The `janitor.select_columns` syntax is supported here, allowing for flexible and dynamic column selection. If ``values_from`` is not specified, all remaining columns will be used. Note that values from `values_from` are usually at the top level, the dataframe's columns is not flattened, or the start of each label in the columns, if flattened. :param levels_order: Applicable if there are multiple `names_from` and/or `values_from`. Reorders the levels. Accepts a list of strings. If there are multiple `values_from`, pass a None to represent that level. :param flatten_levels: Default is `True`. If `False`, the dataframe stays as a MultiIndex. :param names_sep: If `names_from` or `values_from` contain multiple variables, this will be used to join their values into a single string to use as a column name. Default is `_`. Applicable only if `flatten_levels` is `True`. :param names_glue: A callable to control the output of the flattened columns. Applicable only if `flatten_levels` is True. Function should be acceptable to pandas\u2019 `map` function. :returns: A pandas DataFrame that has been unpivoted from long to wide form. \"\"\" df = df.copy() return _computations_pivot_wider( df, index, names_from, values_from, levels_order, flatten_levels, names_sep, names_glue, )","title":"pivot_wider()"},{"location":"api/functions/#janitor.functions.process_text","text":"","title":"process_text"},{"location":"api/functions/#janitor.functions.process_text.process_text","text":"Apply a Pandas string method to an existing column and return a dataframe. This function aims to make string cleaning easy, while chaining, by simply passing the string method name to the process_text function. This modifies an existing column; it does not create a new column. New columns can be created via pyjanitor's transform_columns . A list of all the string methods in Pandas can be accessed here <https://pandas.pydata.org/docs/user_guide/text.html#method-summary> __. Example: import pandas as pd import janitor as jn text code 0 Ragnar 1 1 sammywemmy 2 2 ginger 3 df.process_text(column_name = \"text\", string_function = \"lower\") text code 0 ragnar 1 1 sammywemmy 2 2 ginger 3 For string methods with parameters, simply pass the keyword arguments:: df.process_text( column_name = \"text\", string_function = \"extract\", pat = r\"(ag)\", expand = False, flags = re.IGNORECASE ) text code 0 ag 1 1 NaN 2 2 NaN 3 Functional usage syntax: import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.process_text( df = df, column_name, string_function = \"string_func_name_here\", kwargs ) Method-chaining usage syntax: import pandas as pd import janitor as jn df = ( pd.DataFrame(...) .process_text( column_name, string_function = \"string_func_name_here\", kwargs ) ) Parameters: Name Type Description Default df DataFrame A pandas dataframe. required column_name str String column to be operated on. required string_function str Pandas string method to be applied. required kwargs str Keyword arguments for parameters of the string_function . {} Returns: Type Description DataFrame A pandas dataframe with modified column(s). Exceptions: Type Description KeyError if string_function is not a Pandas string method. TypeError if the wrong kwarg is supplied. ValueError if column_name not found in dataframe. .. # noqa: DAR402 Source code in janitor/functions/process_text.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def process_text( df: pd.DataFrame, column_name: str, string_function: str, **kwargs: str, ) -> pd.DataFrame: \"\"\" Apply a Pandas string method to an existing column and return a dataframe. This function aims to make string cleaning easy, while chaining, by simply passing the string method name to the ``process_text`` function. This modifies an existing column; it does not create a new column. New columns can be created via pyjanitor's `transform_columns`. A list of all the string methods in Pandas can be accessed `here <https://pandas.pydata.org/docs/user_guide/text.html#method-summary>`__. Example: import pandas as pd import janitor as jn text code 0 Ragnar 1 1 sammywemmy 2 2 ginger 3 df.process_text(column_name = \"text\", string_function = \"lower\") text code 0 ragnar 1 1 sammywemmy 2 2 ginger 3 For string methods with parameters, simply pass the keyword arguments:: df.process_text( column_name = \"text\", string_function = \"extract\", pat = r\"(ag)\", expand = False, flags = re.IGNORECASE ) text code 0 ag 1 1 NaN 2 2 NaN 3 Functional usage syntax: import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.process_text( df = df, column_name, string_function = \"string_func_name_here\", kwargs ) Method-chaining usage syntax: import pandas as pd import janitor as jn df = ( pd.DataFrame(...) .process_text( column_name, string_function = \"string_func_name_here\", kwargs ) ) :param df: A pandas dataframe. :param column_name: String column to be operated on. :param string_function: Pandas string method to be applied. :param kwargs: Keyword arguments for parameters of the `string_function`. :returns: A pandas dataframe with modified column(s). :raises KeyError: if ``string_function`` is not a Pandas string method. :raises TypeError: if the wrong ``kwarg`` is supplied. :raises ValueError: if `column_name` not found in dataframe. .. # noqa: DAR402 \"\"\" check(\"column_name\", column_name, [str]) check(\"string_function\", string_function, [str]) check_column(df, [column_name]) pandas_string_methods = [ func.__name__ for _, func in inspect.getmembers(pd.Series.str, inspect.isfunction) if not func.__name__.startswith(\"_\") ] if string_function not in pandas_string_methods: raise KeyError(f\"{string_function} is not a Pandas string method.\") result = getattr(df[column_name].str, string_function)(**kwargs) if isinstance(result, pd.DataFrame): raise ValueError( \"\"\" The outcome of the processed text is a DataFrame, which is not supported in `process_text`. \"\"\" ) return df.assign(**{column_name: result})","title":"process_text()"},{"location":"api/functions/#janitor.functions.remove_columns","text":"","title":"remove_columns"},{"location":"api/functions/#janitor.functions.remove_columns.remove_columns","text":"Remove the set of columns specified in column_names . This method does not mutate the original DataFrame. Intended to be the method-chaining alternative to del df[col] . Method chaining syntax: df = pd.DataFrame(...).remove_columns(column_names=['col1', 'col2']) Parameters: Name Type Description Default df DataFrame A pandas DataFrame required column_names Union[str, Iterable[str], Hashable] The columns to remove. required Returns: Type Description DataFrame A pandas DataFrame. Source code in janitor/functions/remove_columns.py @pf.register_dataframe_method @deprecated_alias(columns=\"column_names\") def remove_columns( df: pd.DataFrame, column_names: Union[str, Iterable[str], Hashable] ) -> pd.DataFrame: \"\"\"Remove the set of columns specified in `column_names`. This method does not mutate the original DataFrame. Intended to be the method-chaining alternative to `del df[col]`. Method chaining syntax: df = pd.DataFrame(...).remove_columns(column_names=['col1', 'col2']) :param df: A pandas DataFrame :param column_names: The columns to remove. :returns: A pandas DataFrame. \"\"\" return df.drop(columns=column_names)","title":"remove_columns()"},{"location":"api/functions/#janitor.functions.remove_empty","text":"","title":"remove_empty"},{"location":"api/functions/#janitor.functions.remove_empty.remove_empty","text":"Drop all rows and columns that are completely null. This method also resets the index(by default) since it doesn't make sense to preserve the index of a completely empty row. This method mutates the original DataFrame. Implementation is inspired from StackOverflow . Functional usage syntax: df = remove_empty(df) Method chaining syntax: import pandas as pd import janitor df = pd.DataFrame(...).remove_empty() Parameters: Name Type Description Default df DataFrame The pandas DataFrame object. required Returns: Type Description DataFrame A pandas DataFrame. Source code in janitor/functions/remove_empty.py @pf.register_dataframe_method def remove_empty(df: pd.DataFrame) -> pd.DataFrame: \"\"\"Drop all rows and columns that are completely null. This method also resets the index(by default) since it doesn't make sense to preserve the index of a completely empty row. This method mutates the original DataFrame. Implementation is inspired from [StackOverflow][so]. [so]: https://stackoverflow.com/questions/38884538/python-pandas-find-all-rows-where-all-values-are-nan Functional usage syntax: ```python df = remove_empty(df) ``` Method chaining syntax: ```python import pandas as pd import janitor df = pd.DataFrame(...).remove_empty() ``` :param df: The pandas DataFrame object. :returns: A pandas DataFrame. \"\"\" # noqa: E501 nanrows = df.index[df.isna().all(axis=1)] df = df.drop(index=nanrows).reset_index(drop=True) nancols = df.columns[df.isna().all(axis=0)] df = df.drop(columns=nancols) return df","title":"remove_empty()"},{"location":"api/functions/#janitor.functions.rename_columns","text":"","title":"rename_columns"},{"location":"api/functions/#janitor.functions.rename_columns.rename_column","text":"Rename a column in place. This method does not mutate the original DataFrame. Functional usage syntax: df = rename_column(df, \"old_column_name\", \"new_column_name\") Method chaining syntax: import pandas as pd import janitor df = ( pd.DataFrame(...) .rename_column(\"old_column_name\", \"new_column_name\") ) This is just syntactic sugar/a convenience function for renaming one column at a time. If you are convinced that there are multiple columns in need of changing, then use the pandas.DataFrame.rename method. Parameters: Name Type Description Default df DataFrame The pandas DataFrame object. required old_column_name str The old column name. required new_column_name str The new column name. required Returns: Type Description DataFrame A pandas DataFrame with renamed columns. Source code in janitor/functions/rename_columns.py @pf.register_dataframe_method @deprecated_alias(old=\"old_column_name\", new=\"new_column_name\") def rename_column( df: pd.DataFrame, old_column_name: str, new_column_name: str ) -> pd.DataFrame: \"\"\"Rename a column in place. This method does not mutate the original DataFrame. Functional usage syntax: ```python df = rename_column(df, \"old_column_name\", \"new_column_name\") ``` Method chaining syntax: ```python import pandas as pd import janitor df = ( pd.DataFrame(...) .rename_column(\"old_column_name\", \"new_column_name\") ) ``` This is just syntactic sugar/a convenience function for renaming one column at a time. If you are convinced that there are multiple columns in need of changing, then use the `pandas.DataFrame.rename` method. :param df: The pandas DataFrame object. :param old_column_name: The old column name. :param new_column_name: The new column name. :returns: A pandas DataFrame with renamed columns. \"\"\" # noqa: E501 check_column(df, [old_column_name]) return df.rename(columns={old_column_name: new_column_name})","title":"rename_column()"},{"location":"api/functions/#janitor.functions.rename_columns.rename_columns","text":"Rename columns. Functional usage syntax: df = rename_columns(df, {\"old_column_name\": \"new_column_name\"}) df = rename_columns(df, function = str.upper) df = rename_columns( df, function = lambda x : x.lower() if x.startswith(\"double\") else x ) Method chaining syntax: import pandas as pd import janitor df = pd.DataFrame(...).rename_columns({\"old_column_name\": \"new_column_name\"}) This is just syntactic sugar/a convenience function for renaming multiple columns at a time. If you need to rename single column, then use the rename_column method. One of the new_column_names or function are a required parameter. If both are provided then new_column_names takes priority and function is never executed. Parameters: Name Type Description Default df DataFrame The pandas DataFrame object. required new_column_names Optional[Dict] A dictionary of old and new column names. None function Callable A function which should be applied to all the columns None Returns: Type Description DataFrame A pandas DataFrame with renamed columns. Exceptions: Type Description ValueError if both new_column_names and function are None Source code in janitor/functions/rename_columns.py @pf.register_dataframe_method def rename_columns( df: pd.DataFrame, new_column_names: Union[Dict, None] = None, function: Callable = None, ) -> pd.DataFrame: \"\"\"Rename columns. Functional usage syntax: ```python df = rename_columns(df, {\"old_column_name\": \"new_column_name\"}) df = rename_columns(df, function = str.upper) df = rename_columns( df, function = lambda x : x.lower() if x.startswith(\"double\") else x ) ``` Method chaining syntax: ```python import pandas as pd import janitor df = pd.DataFrame(...).rename_columns({\"old_column_name\": \"new_column_name\"}) ``` This is just syntactic sugar/a convenience function for renaming multiple columns at a time. If you need to rename single column, then use the `rename_column` method. One of the new_column_names or function are a required parameter. If both are provided then new_column_names takes priority and function is never executed. :param df: The pandas DataFrame object. :param new_column_names: A dictionary of old and new column names. :param function: A function which should be applied to all the columns :returns: A pandas DataFrame with renamed columns. :raises ValueError: if both new_column_names and function are None \"\"\" # noqa: E501 if new_column_names is None and function is None: raise ValueError( \"One of new_column_names or function must be provided\" ) if new_column_names is not None: check_column(df, new_column_names) return df.rename(columns=new_column_names) return df.rename(mapper=function, axis=\"columns\")","title":"rename_columns()"},{"location":"api/functions/#janitor.functions.reorder_columns","text":"","title":"reorder_columns"},{"location":"api/functions/#janitor.functions.reorder_columns.reorder_columns","text":"Reorder DataFrame columns by specifying desired order as list of col names. Columns not specified retain their order and follow after specified cols. Validates column_order to ensure columns are all present in DataFrame. This method does not mutate the original DataFrame. Functional usage syntax: Given DataFrame with column names col1 , col2 , col3 : df = reorder_columns(df, ['col2', 'col3']) Method chaining syntax: import pandas as pd import janitor df = pd.DataFrame(...).reorder_columns(['col2', 'col3']) The column order of df is now col2 , col3 , col1 . Internally, this function uses DataFrame.reindex with copy=False to avoid unnecessary data duplication. Parameters: Name Type Description Default df DataFrame DataFrame to reorder required column_order Union[Iterable[str], pandas.core.indexes.base.Index, Hashable] A list of column names or Pandas Index specifying their order in the returned DataFrame . required Returns: Type Description DataFrame A pandas DataFrame with reordered columns. Exceptions: Type Description IndexError if a column within column_order is not found within the DataFrame. Source code in janitor/functions/reorder_columns.py @pf.register_dataframe_method def reorder_columns( df: pd.DataFrame, column_order: Union[Iterable[str], pd.Index, Hashable] ) -> pd.DataFrame: \"\"\"Reorder DataFrame columns by specifying desired order as list of col names. Columns not specified retain their order and follow after specified cols. Validates column_order to ensure columns are all present in DataFrame. This method does not mutate the original DataFrame. Functional usage syntax: Given `DataFrame` with column names `col1`, `col2`, `col3`: ```python df = reorder_columns(df, ['col2', 'col3']) ``` Method chaining syntax: ```python import pandas as pd import janitor df = pd.DataFrame(...).reorder_columns(['col2', 'col3']) ``` The column order of `df` is now `col2`, `col3`, `col1`. Internally, this function uses `DataFrame.reindex` with `copy=False` to avoid unnecessary data duplication. :param df: `DataFrame` to reorder :param column_order: A list of column names or Pandas `Index` specifying their order in the returned `DataFrame`. :returns: A pandas DataFrame with reordered columns. :raises IndexError: if a column within `column_order` is not found within the DataFrame. \"\"\" check(\"column_order\", column_order, [list, tuple, pd.Index]) if any(col not in df.columns for col in column_order): raise IndexError( \"A column in `column_order` was not found in the DataFrame.\" ) # if column_order is a Pandas index, needs conversion to list: column_order = list(column_order) return df.reindex( columns=( column_order + [col for col in df.columns if col not in column_order] ), copy=False, )","title":"reorder_columns()"},{"location":"api/functions/#janitor.functions.round_to_fraction","text":"","title":"round_to_fraction"},{"location":"api/functions/#janitor.functions.round_to_fraction.round_to_fraction","text":"Round all values in a column to a fraction. This method mutates the original DataFrame. Taken from the R package . Also, optionally round to a specified number of digits. Method-chaining usage: # Round to two decimal places df = pd.DataFrame(...).round_to_fraction('a', 2) Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable Name of column to round to fraction. None denominator float The denominator of the fraction for rounding None digits float The number of digits for rounding after rounding to the fraction. Default is np.inf (i.e. no subsequent rounding) inf Returns: Type Description DataFrame A pandas DataFrame with a column's values rounded. Source code in janitor/functions/round_to_fraction.py @pf.register_dataframe_method @deprecated_alias(col_name=\"column_name\") def round_to_fraction( df: pd.DataFrame, column_name: Hashable = None, denominator: float = None, digits: float = np.inf, ) -> pd.DataFrame: \"\"\" Round all values in a column to a fraction. This method mutates the original DataFrame. Taken from [the R package](https://github.com/sfirke/janitor/issues/235). Also, optionally round to a specified number of digits. Method-chaining usage: ```python # Round to two decimal places df = pd.DataFrame(...).round_to_fraction('a', 2) ``` :param df: A pandas DataFrame. :param column_name: Name of column to round to fraction. :param denominator: The denominator of the fraction for rounding :param digits: The number of digits for rounding after rounding to the fraction. Default is np.inf (i.e. no subsequent rounding) :returns: A pandas DataFrame with a column's values rounded. \"\"\" if denominator: check(\"denominator\", denominator, [float, int]) if digits: check(\"digits\", digits, [float, int]) df[column_name] = round(df[column_name] * denominator, 0) / denominator if not np.isinf(digits): df[column_name] = round(df[column_name], digits) return df","title":"round_to_fraction()"},{"location":"api/functions/#janitor.functions.row_to_names","text":"","title":"row_to_names"},{"location":"api/functions/#janitor.functions.row_to_names.row_to_names","text":"Elevates a row to be the column names of a DataFrame. This method mutates the original DataFrame. Contains options to remove the elevated row from the DataFrame along with removing the rows above the selected row. Method chaining usage: df = ( pd.DataFrame(...) .row_to_names( row_number=0, remove_row=False, remove_rows_above=False, reset_index=False, ) ) Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required row_number int The row containing the variable names None remove_row bool Whether the row should be removed from the DataFrame. Defaults to False. False remove_rows_above bool Whether the rows above the selected row should be removed from the DataFrame. Defaults to False. False reset_index bool Whether the index should be reset on the returning DataFrame. Defaults to False. False Returns: Type Description DataFrame A pandas DataFrame with set column names. Source code in janitor/functions/row_to_names.py @pf.register_dataframe_method def row_to_names( df: pd.DataFrame, row_number: int = None, remove_row: bool = False, remove_rows_above: bool = False, reset_index: bool = False, ) -> pd.DataFrame: \"\"\"Elevates a row to be the column names of a DataFrame. This method mutates the original DataFrame. Contains options to remove the elevated row from the DataFrame along with removing the rows above the selected row. Method chaining usage: df = ( pd.DataFrame(...) .row_to_names( row_number=0, remove_row=False, remove_rows_above=False, reset_index=False, ) ) :param df: A pandas DataFrame. :param row_number: The row containing the variable names :param remove_row: Whether the row should be removed from the DataFrame. Defaults to False. :param remove_rows_above: Whether the rows above the selected row should be removed from the DataFrame. Defaults to False. :param reset_index: Whether the index should be reset on the returning DataFrame. Defaults to False. :returns: A pandas DataFrame with set column names. \"\"\" # :Setup: # ```python # import pandas as pd # import janitor # data_dict = { # \"a\": [1, 2, 3] * 3, # \"Bell__Chart\": [1, 2, 3] * 3, # \"decorated-elephant\": [1, 2, 3] * 3, # \"animals\": [\"rabbit\", \"leopard\", \"lion\"] * 3, # \"cities\": [\"Cambridge\", \"Shanghai\", \"Basel\"] * 3 # } # :Example: Move first row to column names: # ```python # example_dataframe = pd.DataFrame(data_dict) # example_dataframe.row_to_names(0) # :Output: # ```python # 1 1 1 rabbit Cambridge # 0 1 1 1 rabbit Cambridge # 1 2 2 2 leopard Shanghai # 2 3 3 3 lion Basel # 3 1 1 1 rabbit Cambridge # 4 2 2 2 leopard Shanghai # 5 3 3 3 lion Basel # 6 1 1 1 rabbit Cambridge # 7 2 2 2 leopard Shanghai # :Example: Move first row to column names and # remove row while resetting the index: # ```python # example_dataframe = pd.DataFrame(data_dict) # example_dataframe.row_to_names(0, remove_row=True,\\ # reset_index=True) # :Output: # ```python # 1 1 1 rabbit Cambridge # 0 2 2 2 leopard Shanghai # 1 3 3 3 lion Basel # 2 1 1 1 rabbit Cambridge # 3 2 2 2 leopard Shanghai # 4 3 3 3 lion Basel # 5 1 1 1 rabbit Cambridge # 6 2 2 2 leopard Shanghai # 7 3 3 3 lion Basel # :Example: Move first row to column names and remove # row without resetting the index: # ```python # example_dataframe = pd.DataFrame(data_dict) # example_dataframe.row_to_names(0, remove_row=True) # :Output: # ```python # 1 1 1 rabbit Cambridge # 1 2 2 2 leopard Shanghai # 2 3 3 3 lion Basel # 3 1 1 1 rabbit Cambridge # 4 2 2 2 leopard Shanghai # 5 3 3 3 lion Basel # 6 1 1 1 rabbit Cambridge # 7 2 2 2 leopard Shanghai # 8 3 3 3 lion Basel # :Example: Move first row to column names, remove row # and remove rows above selected row without resetting # index: # ```python # example_dataframe = pd.DataFrame(data_dict) # example_dataframe.row_to_names(2, remove_row=True, \\ # remove_rows_above=True, reset_index= True) # :Output: # ```python # 3 3 3 lion Basel # 0 1 1 1 rabbit Cambridge # 1 2 2 2 leopard Shanghai # 2 3 3 3 lion Basel # 3 1 1 1 rabbit Cambridge # 4 2 2 2 leopard Shanghai # 5 3 3 3 lion Basel # :Example: Move first row to column names, remove row, # and remove rows above selected row without resetting # index: # ```python # example_dataframe = pd.DataFrame(data_dict) # example_dataframe.row_to_names(2, remove_row=True, \\ # remove_rows_above=True) # :Output: # ```python # 3 3 3 lion Basel # 3 1 1 1 rabbit Cambridge # 4 2 2 2 leopard Shanghai # 5 3 3 3 lion Basel # 6 1 1 1 rabbit Cambridge # 7 2 2 2 leopard Shanghai # 8 3 3 3 lion Basel check(\"row_number\", row_number, [int]) warnings.warn( \"The function row_to_names will, in the official 1.0 release, \" \"change its behaviour to reset the dataframe's index by default. \" \"You can prepare for this change right now by explicitly setting \" \"`reset_index=True` when calling on `row_to_names`.\" ) df.columns = df.iloc[row_number, :] df.columns.name = None if remove_row: df = df.drop(df.index[row_number]) if remove_rows_above: df = df.drop(df.index[range(row_number)]) if reset_index: df = df.reset_index(drop=[\"index\"]) return df","title":"row_to_names()"},{"location":"api/functions/#janitor.functions.select_columns","text":"","title":"select_columns"},{"location":"api/functions/#janitor.functions.select_columns.select_columns","text":"Method-chainable selection of columns. Not applicable to MultiIndex columns. It accepts a string, shell-like glob strings (*string*) , regex, slice, array-like object, or a list of the previous options. This method does not mutate the original DataFrame. Optional ability to invert selection of columns available as well. import pandas as pd import janitor import numpy as np import datetime import re from janitor import patterns from pandas.api.types import is_datetime64_dtype df = pd.DataFrame( { \"id\": [0, 1], \"Name\": [\"ABC\", \"XYZ\"], \"code\": [1, 2], \"code1\": [4, np.nan], \"code2\": [\"8\", 5], \"type\": [\"S\", \"R\"], \"type1\": [\"E\", np.nan], \"type2\": [\"T\", \"U\"], \"code3\": pd.Series([\"a\", \"b\"], dtype=\"category\"), \"type3\": pd.to_datetime([np.datetime64(\"2018-01-01\"), datetime.datetime(2018, 1, 1)]), } ) df id Name code code1 code2 type type1 type2 code3 type3 0 0 ABC 1 4.0 8 S E T a 2018-01-01 1 1 XYZ 2 NaN 5 R NaN U b 2018-01-01 Select by string: df.select_columns(\"id\") id 0 0 1 1 Select via shell-like glob strings ( * ) is possible: df.select_columns(\"type*\") type type1 type2 type3 0 S E T 2018-01-01 1 R NaN U 2018-01-01 Select by slice: df.select_columns(slice(\"code1\", \"type1\")) code1 code2 type type1 0 4.0 8 S E 1 NaN 5 R NaN Select by Callable (the callable is applied to every column and should return a single True or False per column): df.select_columns(is_datetime64_dtype) type3 0 2018-01-01 1 2018-01-01 df.select_columns(lambda x: x.name.startswith(\"code\") or x.name.endswith(\"1\")) code code1 code2 type1 code3 0 1 4.0 8 E a 1 2 NaN 5 NaN b df.select_columns(lambda x: x.isna().any()) code1 type1 0 4.0 E 1 NaN NaN Select by regular expression: df.select_columns(re.compile(\"\\d+\")) code1 code2 type1 type2 code3 type3 0 4.0 8 E T a 2018-01-01 1 NaN 5 NaN U b 2018-01-01 # same as above, with janitor.patterns # simply a wrapper around re.compile df.select_columns(patterns(\"\\d+\")) code1 code2 type1 type2 code3 type3 0 4.0 8 E T a 2018-01-01 1 NaN 5 NaN U b 2018-01-01 Select a combination of the above (you can combine any of the previous options): df.select_columns(\"id\", \"code*\", slice(\"code\", \"code2\")) id code code1 code2 code3 0 0 1 4.0 8 a 1 1 2 NaN 5 b You can also pass a sequence of booleans: df.select_columns([True, False, True, True, True, False, False, False, True, False]) id code code1 code2 code3 0 0 1 4.0 8 a 1 1 2 NaN 5 b Setting invert to True returns the complement of the columns provided: df.select_columns(\"id\", \"code*\", slice(\"code\", \"code2\"), invert = True) Name type type1 type2 type3 0 ABC S E T 2018-01-01 1 XYZ R NaN U 2018-01-01 Functional usage example: import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.select_columns('a', 'b', 'col_*', invert=True) Method-chaining example: df = (pd.DataFrame(...) .select_columns('a', 'b', 'col_*', invert=True)) Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required args Valid inputs include: - an exact column name to look for - a shell-style glob string (e.g., *_thing_* ) - a regular expression - a callable which is applicable to each Series in the dataframe - variable arguments of all the aforementioned. - a sequence of booleans. () invert bool Whether or not to invert the selection. This will result in the selection of the complement of the columns provided. False Returns: Type Description DataFrame A pandas DataFrame with the specified columns selected. Source code in janitor/functions/select_columns.py @pf.register_dataframe_method @deprecated_alias(search_cols=\"search_column_names\") def select_columns( df: pd.DataFrame, *args, invert: bool = False, ) -> pd.DataFrame: \"\"\" Method-chainable selection of columns. Not applicable to MultiIndex columns. It accepts a string, shell-like glob strings `(*string*)`, regex, slice, array-like object, or a list of the previous options. This method does not mutate the original DataFrame. Optional ability to invert selection of columns available as well. ```python import pandas as pd import janitor import numpy as np import datetime import re from janitor import patterns from pandas.api.types import is_datetime64_dtype df = pd.DataFrame( { \"id\": [0, 1], \"Name\": [\"ABC\", \"XYZ\"], \"code\": [1, 2], \"code1\": [4, np.nan], \"code2\": [\"8\", 5], \"type\": [\"S\", \"R\"], \"type1\": [\"E\", np.nan], \"type2\": [\"T\", \"U\"], \"code3\": pd.Series([\"a\", \"b\"], dtype=\"category\"), \"type3\": pd.to_datetime([np.datetime64(\"2018-01-01\"), datetime.datetime(2018, 1, 1)]), } ) df id Name code code1 code2 type type1 type2 code3 type3 0 0 ABC 1 4.0 8 S E T a 2018-01-01 1 1 XYZ 2 NaN 5 R NaN U b 2018-01-01 ``` - Select by string: ``` df.select_columns(\"id\") id 0 0 1 1 ``` - Select via shell-like glob strings (`*`) is possible: ```python df.select_columns(\"type*\") type type1 type2 type3 0 S E T 2018-01-01 1 R NaN U 2018-01-01 ``` - Select by slice: ```python df.select_columns(slice(\"code1\", \"type1\")) code1 code2 type type1 0 4.0 8 S E 1 NaN 5 R NaN ``` - Select by `Callable` (the callable is applied to every column and should return a single `True` or `False` per column): ```python df.select_columns(is_datetime64_dtype) type3 0 2018-01-01 1 2018-01-01 df.select_columns(lambda x: x.name.startswith(\"code\") or x.name.endswith(\"1\")) code code1 code2 type1 code3 0 1 4.0 8 E a 1 2 NaN 5 NaN b df.select_columns(lambda x: x.isna().any()) code1 type1 0 4.0 E 1 NaN NaN ``` - Select by regular expression: ```python df.select_columns(re.compile(\"\\\\d+\")) code1 code2 type1 type2 code3 type3 0 4.0 8 E T a 2018-01-01 1 NaN 5 NaN U b 2018-01-01 # same as above, with janitor.patterns # simply a wrapper around re.compile df.select_columns(patterns(\"\\\\d+\")) code1 code2 type1 type2 code3 type3 0 4.0 8 E T a 2018-01-01 1 NaN 5 NaN U b 2018-01-01 ``` - Select a combination of the above (you can combine any of the previous options): ```python df.select_columns(\"id\", \"code*\", slice(\"code\", \"code2\")) id code code1 code2 code3 0 0 1 4.0 8 a 1 1 2 NaN 5 b ``` - You can also pass a sequence of booleans: ```python df.select_columns([True, False, True, True, True, False, False, False, True, False]) id code code1 code2 code3 0 0 1 4.0 8 a 1 1 2 NaN 5 b ``` - Setting `invert` to `True` returns the complement of the columns provided: ```python df.select_columns(\"id\", \"code*\", slice(\"code\", \"code2\"), invert = True) Name type type1 type2 type3 0 ABC S E T 2018-01-01 1 XYZ R NaN U 2018-01-01 ``` Functional usage example: ```python import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.select_columns('a', 'b', 'col_*', invert=True) ``` Method-chaining example: ```python df = (pd.DataFrame(...) .select_columns('a', 'b', 'col_*', invert=True)) ``` :param df: A pandas DataFrame. :param args: Valid inputs include: - an exact column name to look for - a shell-style glob string (e.g., `*_thing_*`) - a regular expression - a callable which is applicable to each Series in the dataframe - variable arguments of all the aforementioned. - a sequence of booleans. :param invert: Whether or not to invert the selection. This will result in the selection of the complement of the columns provided. :returns: A pandas DataFrame with the specified columns selected. \"\"\" # applicable for any # list-like object (ndarray, Series, pd.Index, ...) # excluding tuples, which are returned as is search_column_names = [] for arg in args: if is_list_like(arg) and (not isinstance(arg, tuple)): search_column_names.extend([*arg]) else: search_column_names.append(arg) if len(search_column_names) == 1: search_column_names = search_column_names[0] full_column_list = _select_column_names(search_column_names, df) if invert: return df.drop(columns=full_column_list) return df.loc[:, full_column_list]","title":"select_columns()"},{"location":"api/functions/#janitor.functions.shuffle","text":"","title":"shuffle"},{"location":"api/functions/#janitor.functions.shuffle.shuffle","text":"Shuffle the rows of the DataFrame. This method does not mutate the original DataFrame. Super-sugary syntax! Underneath the hood, we use df.sample(frac=1) , with the option to set the random state. Example usage: df = pd.DataFrame(...).shuffle() Parameters: Name Type Description Default df DataFrame A pandas DataFrame required random_state (optional) A seed for the random number generator. None reset_index bool (optional) Resets index to default integers True Returns: Type Description DataFrame A shuffled pandas DataFrame. Source code in janitor/functions/shuffle.py @pf.register_dataframe_method def shuffle( df: pd.DataFrame, random_state=None, reset_index: bool = True ) -> pd.DataFrame: \"\"\"Shuffle the rows of the DataFrame. This method does not mutate the original DataFrame. Super-sugary syntax! Underneath the hood, we use `df.sample(frac=1)`, with the option to set the random state. Example usage: ```python df = pd.DataFrame(...).shuffle() ``` :param df: A pandas DataFrame :param random_state: (optional) A seed for the random number generator. :param reset_index: (optional) Resets index to default integers :returns: A shuffled pandas DataFrame. \"\"\" result = df.sample(frac=1, random_state=random_state) if reset_index: result = result.reset_index(drop=True) return result","title":"shuffle()"},{"location":"api/functions/#janitor.functions.sort_column_value_order","text":"","title":"sort_column_value_order"},{"location":"api/functions/#janitor.functions.sort_column_value_order.sort_column_value_order","text":"This function adds precedence to certain values in a specified column, then sorts based on that column and any other specified columns. Example: SalesMonth Company2 Company3 Company1 150.0 Jan 180.0 400.0 200.0 Feb 250.0 500.0 200.0 Feb 250.0 500.0 300.0 Mar NaN 600.0 400.0 April 500.0 675.0 Given the current DataFrame, we want to order the sales month in desc order. To achieve this we would assign the later months with smaller values with the latest month, such as April with the precedence of 0. df = sort_column_value_order( df, 'SalesMonth', {'April':1,'Mar':2,'Feb':3,'Jan':4} ) The returned DataFrame will look as follows. SalesMonth Company2 Company3 Company1 400.0 April 500.0 675.0 300.0 Mar NaN 600.0 200.0 Feb 250.0 500.0 200.0 Feb 250.0 500.0 150.0 Jan 180.0 400.0 Parameters: Name Type Description Default df DataFrame This is our DataFrame that we are manipulating required column str This is a column name as a string we are using to specify which column to sort by required column_value_order dict This is a dictionary of values that will represent precedence of the values in the specified column required columns This is a list of additional columns that we can sort by None Returns: Type Description DataFrame This function returns a Pandas DataFrame Exceptions: Type Description ValueError raises error if chosen Column Name is not in Dataframe, or if column_value_order dictionary is empty. Source code in janitor/functions/sort_column_value_order.py @pf.register_dataframe_method def sort_column_value_order( df: pd.DataFrame, column: str, column_value_order: dict, columns=None ) -> pd.DataFrame: \"\"\" This function adds precedence to certain values in a specified column, then sorts based on that column and any other specified columns. Example: SalesMonth Company2 Company3 Company1 150.0 Jan 180.0 400.0 200.0 Feb 250.0 500.0 200.0 Feb 250.0 500.0 300.0 Mar NaN 600.0 400.0 April 500.0 675.0 Given the current DataFrame, we want to order the sales month in desc order. To achieve this we would assign the later months with smaller values with the latest month, such as April with the precedence of 0. df = sort_column_value_order( df, 'SalesMonth', {'April':1,'Mar':2,'Feb':3,'Jan':4} ) The returned DataFrame will look as follows. SalesMonth Company2 Company3 Company1 400.0 April 500.0 675.0 300.0 Mar NaN 600.0 200.0 Feb 250.0 500.0 200.0 Feb 250.0 500.0 150.0 Jan 180.0 400.0 :param df: This is our DataFrame that we are manipulating :param column: This is a column name as a string we are using to specify which column to sort by :param column_value_order: This is a dictionary of values that will represent precedence of the values in the specified column :param columns: This is a list of additional columns that we can sort by :raises ValueError: raises error if chosen Column Name is not in Dataframe, or if column_value_order dictionary is empty. :return: This function returns a Pandas DataFrame \"\"\" if len(column_value_order) > 0: if column in df.columns: df[\"cond_order\"] = df[column].replace(column_value_order) if columns is None: new_df = df.sort_values(\"cond_order\") del new_df[\"cond_order\"] else: new_df = df.sort_values(columns + [\"cond_order\"]) del new_df[\"cond_order\"] return new_df else: raise ValueError(\"Column Name not in DataFrame\") else: raise ValueError(\"column_value_order dictionary cannot be empty\")","title":"sort_column_value_order()"},{"location":"api/functions/#janitor.functions.sort_naturally","text":"","title":"sort_naturally"},{"location":"api/functions/#janitor.functions.sort_naturally.sort_naturally","text":"Sort a DataFrame by a column using natural sorting. Natural sorting is distinct from the default lexiographical sorting provided by pandas . For example, given the following list of items: [\"A1\", \"A11\", \"A3\", \"A2\", \"A10\"] lexicographical sorting would give us: [\"A1\", \"A10\", \"A11\", \"A2\", \"A3\"] By contrast, \"natural\" sorting would give us: [\"A1\", \"A2\", \"A3\", \"A10\", \"A11\"] This function thus provides natural sorting on a single column of a dataframe. To accomplish this, we do a natural sort on the unique values that are present in the dataframe. Then, we reconstitute the entire dataframe in the naturally sorted order. Natural sorting is provided by the Python package natsort All keyword arguments to natsort should be provided after the column name to sort by is provided. They are passed through to the natsorted function. Functional usage syntax: import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.sort_naturally( df=df, column_name='alphanumeric_column', ) Method chaining usage syntax: import pandas as pd import janitor df = pd.DataFrame(...) df = df.sort_naturally( column_name='alphanumeric_column', ) Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name str The column on which natural sorting should take place. required natsorted_kwargs Keyword arguments to be passed to natsort's natsorted function. {} Returns: Type Description DataFrame A sorted pandas DataFrame. Source code in janitor/functions/sort_naturally.py @pf.register_dataframe_method def sort_naturally( df: pd.DataFrame, column_name: str, **natsorted_kwargs ) -> pd.DataFrame: \"\"\"Sort a DataFrame by a column using *natural* sorting. Natural sorting is distinct from the default lexiographical sorting provided by `pandas`. For example, given the following list of items: [\"A1\", \"A11\", \"A3\", \"A2\", \"A10\"] lexicographical sorting would give us: [\"A1\", \"A10\", \"A11\", \"A2\", \"A3\"] By contrast, \"natural\" sorting would give us: [\"A1\", \"A2\", \"A3\", \"A10\", \"A11\"] This function thus provides *natural* sorting on a single column of a dataframe. To accomplish this, we do a natural sort on the unique values that are present in the dataframe. Then, we reconstitute the entire dataframe in the naturally sorted order. Natural sorting is provided by the Python package [natsort](https://natsort.readthedocs.io/en/master/index.html) All keyword arguments to `natsort` should be provided after the column name to sort by is provided. They are passed through to the `natsorted` function. Functional usage syntax: ```python import pandas as pd import janitor as jn df = pd.DataFrame(...) df = jn.sort_naturally( df=df, column_name='alphanumeric_column', ) ``` Method chaining usage syntax: ```python import pandas as pd import janitor df = pd.DataFrame(...) df = df.sort_naturally( column_name='alphanumeric_column', ) ``` :param df: A pandas DataFrame. :param column_name: The column on which natural sorting should take place. :param natsorted_kwargs: Keyword arguments to be passed to natsort's `natsorted` function. :returns: A sorted pandas DataFrame. \"\"\" new_order = index_natsorted(df[column_name], **natsorted_kwargs) return df.iloc[new_order, :]","title":"sort_naturally()"},{"location":"api/functions/#janitor.functions.take_first","text":"","title":"take_first"},{"location":"api/functions/#janitor.functions.take_first.take_first","text":"Take the first row within each group specified by subset . This method does not mutate the original DataFrame. import pandas as pd import janitor data = { \"a\": [\"x\", \"x\", \"y\", \"y\"], \"b\": [0, 1, 2, 3] } df = pd.DataFrame(data) df.take_first(subset=\"a\", by=\"b\") Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required subset Union[Hashable, Iterable[Hashable]] Column(s) defining the group. required by Hashable Column to sort by. required ascending bool Whether or not to sort in ascending order, bool . True Returns: Type Description DataFrame A pandas DataFrame. Source code in janitor/functions/take_first.py @pf.register_dataframe_method def take_first( df: pd.DataFrame, subset: Union[Hashable, Iterable[Hashable]], by: Hashable, ascending: bool = True, ) -> pd.DataFrame: \"\"\" Take the first row within each group specified by `subset`. This method does not mutate the original DataFrame. ```python import pandas as pd import janitor data = { \"a\": [\"x\", \"x\", \"y\", \"y\"], \"b\": [0, 1, 2, 3] } df = pd.DataFrame(data) df.take_first(subset=\"a\", by=\"b\") ``` :param df: A pandas DataFrame. :param subset: Column(s) defining the group. :param by: Column to sort by. :param ascending: Whether or not to sort in ascending order, `bool`. :returns: A pandas DataFrame. \"\"\" result = df.sort_values(by=by, ascending=ascending).drop_duplicates( subset=subset, keep=\"first\" ) return result","title":"take_first()"},{"location":"api/functions/#janitor.functions.then","text":"","title":"then"},{"location":"api/functions/#janitor.functions.then.then","text":"Add an arbitrary function to run in the pyjanitor method chain. This method does not mutate the original DataFrame. Parameters: Name Type Description Default df DataFrame A pandas dataframe. required func Callable A function you would like to run in the method chain. It should take one parameter and return one parameter, each being the DataFrame object. After that, do whatever you want in the middle. Go crazy. required Returns: Type Description DataFrame A pandas DataFrame. Source code in janitor/functions/then.py @pf.register_dataframe_method def then(df: pd.DataFrame, func: Callable) -> pd.DataFrame: \"\"\" Add an arbitrary function to run in the `pyjanitor` method chain. This method does not mutate the original DataFrame. :param df: A pandas dataframe. :param func: A function you would like to run in the method chain. It should take one parameter and return one parameter, each being the DataFrame object. After that, do whatever you want in the middle. Go crazy. :returns: A pandas DataFrame. \"\"\" df = func(df) return df","title":"then()"},{"location":"api/functions/#janitor.functions.to_datetime","text":"","title":"to_datetime"},{"location":"api/functions/#janitor.functions.to_datetime.to_datetime","text":"Method-chainable pd.to_datetime . This method mutates the original DataFrame. Functional usage syntax: df = to_datetime(df, 'col1', format='%Y%m%d') Method chaining syntax: import pandas as pd import janitor df = pd.DataFrame(...).to_datetime('col1', format='%Y%m%d') Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable Column name. required kwargs provide any kwargs that pd.to_datetime can take. {} Returns: Type Description DataFrame A pandas DataFrame with updated datetime data. Source code in janitor/functions/to_datetime.py @pf.register_dataframe_method @deprecated_alias(column=\"column_name\") def to_datetime( df: pd.DataFrame, column_name: Hashable, **kwargs ) -> pd.DataFrame: \"\"\" Method-chainable `pd.to_datetime`. This method mutates the original DataFrame. Functional usage syntax: ```python df = to_datetime(df, 'col1', format='%Y%m%d') ``` Method chaining syntax: ```python import pandas as pd import janitor df = pd.DataFrame(...).to_datetime('col1', format='%Y%m%d') ``` :param df: A pandas DataFrame. :param column_name: Column name. :param kwargs: provide any kwargs that `pd.to_datetime` can take. :returns: A pandas DataFrame with updated datetime data. \"\"\" df[column_name] = pd.to_datetime(df[column_name], **kwargs) return df","title":"to_datetime()"},{"location":"api/functions/#janitor.functions.toset","text":"","title":"toset"},{"location":"api/functions/#janitor.functions.toset.toset","text":"Return a set of the values. These are each a scalar type, which is a Python scalar (for str, int, float) or a pandas scalar (for Timestamp/Timedelta/Interval/Period) Functional usage syntax: import pandas as pd import janitor as jn series = pd.Series(...) s = jn.functions.toset(series=series) Method chaining usage example: import pandas as pd import janitor series = pd.Series(...) s = series.toset() Parameters: Name Type Description Default series Series A pandas series. required Returns: Type Description Set A set of values. Source code in janitor/functions/toset.py @pf.register_series_method def toset(series: pd.Series) -> Set: \"\"\"Return a set of the values. These are each a scalar type, which is a Python scalar (for str, int, float) or a pandas scalar (for Timestamp/Timedelta/Interval/Period) Functional usage syntax: import pandas as pd import janitor as jn series = pd.Series(...) s = jn.functions.toset(series=series) Method chaining usage example: import pandas as pd import janitor series = pd.Series(...) s = series.toset() :param series: A pandas series. :returns: A set of values. \"\"\" return set(series.tolist())","title":"toset()"},{"location":"api/functions/#janitor.functions.transform_columns","text":"","title":"transform_columns"},{"location":"api/functions/#janitor.functions.transform_columns.transform_column","text":"Transform the given column in-place using the provided function. Functions can be applied one of two ways: Element-wise (default; `elementwise=True``) Column-wise (alternative; `elementwise=False``) If the function is applied \"elementwise\", then the first argument of the function signature should be the individual element of each function. This is the default behaviour of `transform_column``, because it is easy to understand. For example: def elemwise_func(x): modified_x = ... # do stuff here return modified_x df.transform_column(column_name=\"my_column\", function=elementwise_func) On the other hand, columnwise application of a function behaves as if the function takes in a pandas Series and emits back a sequence that is of identical length to the original. One place where this is desirable is to gain access to pandas native string methods, which are super fast! def columnwise_func(s: pd.Series) -> pd.Series: return s.str[0:5] df.transform_column( column_name=\"my_column\", lambda s: s.str[0:5], elementwise=False ) This method does not mutate the original DataFrame. Let's say we wanted to apply a log10 transform a column of data. Originally one would write code like this: # YOU NO LONGER NEED TO WRITE THIS! df[column_name] = df[column_name].apply(np.log10) With the method chaining syntax, we can do the following instead: df = ( pd.DataFrame(...) .transform_column(column_name, np.log10) ) With the functional syntax: df = pd.DataFrame(...) df = transform_column(df, column_name, np.log10) Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_name Hashable The column to transform. required function Callable A function to apply on the column. required dest_column_name Optional[str] The column name to store the transformation result in. Defaults to None, which will result in the original column name being overwritten. If a name is provided here, then a new column with the transformed values will be created. None elementwise bool Whether to apply the function elementwise or not. If elementwise is True, then the function's first argument should be the data type of each datum in the column of data, and should return a transformed datum. If elementwise is False, then the function's should expect a pandas Series passed into it, and return a pandas Series. True Returns: Type Description DataFrame A pandas DataFrame with a transformed column. Source code in janitor/functions/transform_columns.py @pf.register_dataframe_method @deprecated_alias(col_name=\"column_name\", dest_col_name=\"dest_column_name\") def transform_column( df: pd.DataFrame, column_name: Hashable, function: Callable, dest_column_name: Optional[str] = None, elementwise: bool = True, ) -> pd.DataFrame: \"\"\"Transform the given column in-place using the provided function. Functions can be applied one of two ways: - Element-wise (default; `elementwise=True``) - Column-wise (alternative; `elementwise=False``) If the function is applied \"elementwise\", then the first argument of the function signature should be the individual element of each function. This is the default behaviour of `transform_column``, because it is easy to understand. For example: def elemwise_func(x): modified_x = ... # do stuff here return modified_x df.transform_column(column_name=\"my_column\", function=elementwise_func) On the other hand, columnwise application of a function behaves as if the function takes in a pandas Series and emits back a sequence that is of identical length to the original. One place where this is desirable is to gain access to `pandas` native string methods, which are super fast! def columnwise_func(s: pd.Series) -> pd.Series: return s.str[0:5] df.transform_column( column_name=\"my_column\", lambda s: s.str[0:5], elementwise=False ) This method does not mutate the original DataFrame. Let's say we wanted to apply a log10 transform a column of data. Originally one would write code like this: # YOU NO LONGER NEED TO WRITE THIS! df[column_name] = df[column_name].apply(np.log10) With the method chaining syntax, we can do the following instead: df = ( pd.DataFrame(...) .transform_column(column_name, np.log10) ) With the functional syntax: df = pd.DataFrame(...) df = transform_column(df, column_name, np.log10) :param df: A pandas DataFrame. :param column_name: The column to transform. :param function: A function to apply on the column. :param dest_column_name: The column name to store the transformation result in. Defaults to None, which will result in the original column name being overwritten. If a name is provided here, then a new column with the transformed values will be created. :param elementwise: Whether to apply the function elementwise or not. If elementwise is True, then the function's first argument should be the data type of each datum in the column of data, and should return a transformed datum. If elementwise is False, then the function's should expect a pandas Series passed into it, and return a pandas Series. :returns: A pandas DataFrame with a transformed column. \"\"\" if dest_column_name is None: dest_column_name = column_name if elementwise: result = df[column_name].apply(function) else: result = function(df[column_name]) df = df.assign(**{dest_column_name: result}) return df","title":"transform_column()"},{"location":"api/functions/#janitor.functions.transform_columns.transform_columns","text":"Transform multiple columns through the same transformation. This method mutates the original DataFrame. Super syntactic sugar! Basically wraps transform_column and calls it repeatedly over all column names provided. User can optionally supply either a suffix to create a new set of columns with the specified suffix, or provide a dictionary mapping each original column name to its corresponding new column name. Note that all column names must be strings. A few examples below. Firstly, to just log10 transform a list of columns without creating new columns to hold the transformed values: df = ( pd.DataFrame(...) .transform_columns(['col1', 'col2', 'col3'], np.log10) ) Secondly, to add a '_log' suffix when creating a new column, which we think is going to be the most common use case: df = ( pd.DataFrame(...) .transform_columns( ['col1', 'col2', 'col3'], np.log10, suffix=\"_log\" ) ) Finally, to provide new names explicitly: df = ( pd.DataFrame(...) .transform_column( ['col1', 'col2', 'col3'], np.log10, new_column_names={ 'col1': 'transform1', 'col2': 'transform2', 'col3': 'transform3', } ) ) Parameters: Name Type Description Default df DataFrame A pandas DataFrame. required column_names Union[List[str], Tuple[str]] An iterable of columns to transform. required function Callable A function to apply on each column. required suffix Optional[str] (optional) Suffix to use when creating new columns to hold the transformed values. None elementwise bool Passed on to transform_column ; whether or not to apply the transformation function elementwise (True) or columnwise (False). True new_column_names Optional[Dict[str, str]] (optional) An explicit mapping of old column names to new column names. None Returns: Type Description DataFrame A pandas DataFrame with transformed columns. Exceptions: Type Description ValueError if both suffix and new_column_names are specified Source code in janitor/functions/transform_columns.py @pf.register_dataframe_method @deprecated_alias(columns=\"column_names\", new_names=\"new_column_names\") def transform_columns( df: pd.DataFrame, column_names: Union[List[str], Tuple[str]], function: Callable, suffix: Optional[str] = None, elementwise: bool = True, new_column_names: Optional[Dict[str, str]] = None, ) -> pd.DataFrame: \"\"\"Transform multiple columns through the same transformation. This method mutates the original DataFrame. Super syntactic sugar! Basically wraps `transform_column` and calls it repeatedly over all column names provided. User can optionally supply either a suffix to create a new set of columns with the specified suffix, or provide a dictionary mapping each original column name to its corresponding new column name. Note that all column names must be strings. A few examples below. Firstly, to just log10 transform a list of columns without creating new columns to hold the transformed values: df = ( pd.DataFrame(...) .transform_columns(['col1', 'col2', 'col3'], np.log10) ) Secondly, to add a '_log' suffix when creating a new column, which we think is going to be the most common use case: df = ( pd.DataFrame(...) .transform_columns( ['col1', 'col2', 'col3'], np.log10, suffix=\"_log\" ) ) Finally, to provide new names explicitly: df = ( pd.DataFrame(...) .transform_column( ['col1', 'col2', 'col3'], np.log10, new_column_names={ 'col1': 'transform1', 'col2': 'transform2', 'col3': 'transform3', } ) ) :param df: A pandas DataFrame. :param column_names: An iterable of columns to transform. :param function: A function to apply on each column. :param suffix: (optional) Suffix to use when creating new columns to hold the transformed values. :param elementwise: Passed on to `transform_column`; whether or not to apply the transformation function elementwise (True) or columnwise (False). :param new_column_names: (optional) An explicit mapping of old column names to new column names. :returns: A pandas DataFrame with transformed columns. :raises ValueError: if both `suffix` and `new_column_names` are specified \"\"\" dest_column_names = dict(zip(column_names, column_names)) check(\"column_names\", column_names, [list, tuple]) if suffix is not None and new_column_names is not None: raise ValueError( \"only one of suffix or new_column_names should be specified\" ) if suffix: # If suffix is specified... check(\"suffix\", suffix, [str]) for col in column_names: dest_column_names[col] = col + suffix if new_column_names: # If new_column_names is specified... check(\"new_column_names\", new_column_names, [dict]) dest_column_names = new_column_names # Now, transform columns. for old_col, new_col in dest_column_names.items(): df = transform_column( df, old_col, function, new_col, elementwise=elementwise ) return df","title":"transform_columns()"},{"location":"api/functions/#janitor.functions.truncate_datetime","text":"","title":"truncate_datetime"},{"location":"api/functions/#janitor.functions.truncate_datetime.truncate_datetime_dataframe","text":"Truncate times down to a user-specified precision of year, month, day, hour, minute, or second. Call on datetime object to truncate it. Calling on existing df will not alter the contents of said df. Note: Truncating down to a Month or Day will yields 0s, as there is no 0 month or 0 day in most datetime systems. Parameters: Name Type Description Default df DataFrame The dataframe on which to truncate datetime. required datepart str Truncation precision, YEAR, MONTH, DAY, HOUR, MINUTE, SECOND. (String is automagically capitalized) required Returns: Type Description DataFrame a truncated datetime object to the precision specified by datepart. Source code in janitor/functions/truncate_datetime.py @pf.register_dataframe_method def truncate_datetime_dataframe( df: pd.DataFrame, datepart: str ) -> pd.DataFrame: \"\"\" Truncate times down to a user-specified precision of year, month, day, hour, minute, or second. Call on datetime object to truncate it. Calling on existing df will not alter the contents of said df. Note: Truncating down to a Month or Day will yields 0s, as there is no 0 month or 0 day in most datetime systems. :param df: The dataframe on which to truncate datetime. :param datepart: Truncation precision, YEAR, MONTH, DAY, HOUR, MINUTE, SECOND. (String is automagically capitalized) :returns: a truncated datetime object to the precision specified by datepart. \"\"\" for i in df.columns: for j in df.index: try: df[i][j] = _truncate_datetime(datepart, df[i][j]) except KeyError: pass except TypeError: pass except AttributeError: pass return df","title":"truncate_datetime_dataframe()"},{"location":"api/functions/#janitor.functions.update_where","text":"","title":"update_where"},{"location":"api/functions/#janitor.functions.update_where.update_where","text":"Add multiple conditions to update a column in the dataframe. This method does not mutate the original DataFrame. Example usage: data = { \"a\": [1, 2, 3, 4], \"b\": [5, 6, 7, 8], \"c\": [0, 0, 0, 0] } df = pd.DataFrame(data) a b c 0 1 5 0 1 2 6 0 2 3 7 0 3 4 8 0 df.update_where(conditions = (df.a > 2) & (df.b < 8), target_column_name = 'c', target_val = 10) a b c 0 1 5 0 1 2 6 0 2 3 7 10 3 4 8 0 update_where also supports pandas query style string expressions: df.update_where(conditions = \"a > 2 and b < 8\", target_column_name = 'c', target_val = 10) a b c 0 1 5 0 1 2 6 0 2 3 7 10 3 4 8 0 Parameters: Name Type Description Default df DataFrame The pandas DataFrame object. required conditions Any Conditions used to update a target column and target value. required target_column_name Hashable Column to be updated. If column does not exist in DataFrame, a new column will be created; note that entries that do not get set in the new column will be null. required target_val Any Value to be updated required Returns: Type Description DataFrame A pandas DataFrame. Exceptions: Type Description ValueError if conditions does not return a boolean array-like data structure. .. # noqa: DAR402 Source code in janitor/functions/update_where.py @pf.register_dataframe_method @deprecated_alias(target_col=\"target_column_name\") def update_where( df: pd.DataFrame, conditions: Any, target_column_name: Hashable, target_val: Any, ) -> pd.DataFrame: \"\"\" Add multiple conditions to update a column in the dataframe. This method does not mutate the original DataFrame. Example usage: ```python data = { \"a\": [1, 2, 3, 4], \"b\": [5, 6, 7, 8], \"c\": [0, 0, 0, 0] } df = pd.DataFrame(data) a b c 0 1 5 0 1 2 6 0 2 3 7 0 3 4 8 0 df.update_where(conditions = (df.a > 2) & (df.b < 8), target_column_name = 'c', target_val = 10) a b c 0 1 5 0 1 2 6 0 2 3 7 10 3 4 8 0 ``` `update_where` also supports pandas *query* style string expressions: ```python df.update_where(conditions = \"a > 2 and b < 8\", target_column_name = 'c', target_val = 10) a b c 0 1 5 0 1 2 6 0 2 3 7 10 3 4 8 0 ``` :param df: The pandas DataFrame object. :param conditions: Conditions used to update a target column and target value. :param target_column_name: Column to be updated. If column does not exist in DataFrame, a new column will be created; note that entries that do not get set in the new column will be null. :param target_val: Value to be updated :returns: A pandas DataFrame. :raises ValueError: if `conditions` does not return a boolean array-like data structure. .. # noqa: DAR402 \"\"\" df = df.copy() # use query mode if a string expression is passed if isinstance(conditions, str): conditions = df.eval(conditions) if not is_bool_dtype(conditions): raise ValueError( \"\"\" Kindly ensure that `conditions` passed evaluates to a Boolean dtype. \"\"\" ) df.loc[conditions, target_column_name] = target_val return df","title":"update_where()"},{"location":"api/functions/#janitor.functions.utils","text":"Utility functions for all of the functions submodule.","title":"utils"},{"location":"api/functions/#janitor.functions.utils.patterns","text":"This function converts a string into a compiled regular expression; it can be used to select columns in the index or columns_names arguments of pivot_longer function. Parameters: Name Type Description Default regex_pattern Union[str, Pattern] string to be converted to compiled regular expression. required Returns: Type Description Pattern A compile regular expression from provided regex_pattern . Source code in janitor/functions/utils.py def patterns(regex_pattern: Union[str, Pattern]) -> Pattern: \"\"\" This function converts a string into a compiled regular expression; it can be used to select columns in the index or columns_names arguments of `pivot_longer` function. :param regex_pattern: string to be converted to compiled regular expression. :returns: A compile regular expression from provided `regex_pattern`. \"\"\" check(\"regular expression\", regex_pattern, [str, Pattern]) return re.compile(regex_pattern)","title":"patterns()"},{"location":"api/functions/#janitor.functions.utils.unionize_dataframe_categories","text":"Given a group of dataframes which contain some categorical columns, for each categorical column present, find all the possible categories across all the dataframes which have that column. Update each dataframes' corresponding column with a new categorical object that contains the original data but has labels for all the possible categories from all dataframes. This is useful when concatenating a list of dataframes which all have the same categorical columns into one dataframe. If, for a given categorical column, all input dataframes do not have at least one instance of all the possible categories, Pandas will change the output dtype of that column from category to object , losing out on dramatic speed gains you get from the former format. Usage example for concatenation of categorical column-containing dataframes: Instead of: concatenated_df = pd.concat([df1, df2, df3], ignore_index=True) which in your case has resulted in category -> object conversion, use: unionized_dataframes = unionize_dataframe_categories(df1, df2, df2) concatenated_df = pd.concat(unionized_dataframes, ignore_index=True) Parameters: Name Type Description Default dataframes The dataframes you wish to unionize the categorical objects for. () column_names Optional[Iterable[pandas.core.dtypes.dtypes.CategoricalDtype]] If supplied, only unionize this subset of columns. None Returns: Type Description List[pandas.core.frame.DataFrame] A list of the category-unioned dataframes in the same order they were provided. Exceptions: Type Description TypeError If any of the inputs are not pandas DataFrames. Source code in janitor/functions/utils.py def unionize_dataframe_categories( *dataframes, column_names: Optional[Iterable[pd.CategoricalDtype]] = None ) -> List[pd.DataFrame]: \"\"\" Given a group of dataframes which contain some categorical columns, for each categorical column present, find all the possible categories across all the dataframes which have that column. Update each dataframes' corresponding column with a new categorical object that contains the original data but has labels for all the possible categories from all dataframes. This is useful when concatenating a list of dataframes which all have the same categorical columns into one dataframe. If, for a given categorical column, all input dataframes do not have at least one instance of all the possible categories, Pandas will change the output dtype of that column from `category` to `object`, losing out on dramatic speed gains you get from the former format. Usage example for concatenation of categorical column-containing dataframes: Instead of: ```python concatenated_df = pd.concat([df1, df2, df3], ignore_index=True) ``` which in your case has resulted in `category` -> `object` conversion, use: ```python unionized_dataframes = unionize_dataframe_categories(df1, df2, df2) concatenated_df = pd.concat(unionized_dataframes, ignore_index=True) ``` :param dataframes: The dataframes you wish to unionize the categorical objects for. :param column_names: If supplied, only unionize this subset of columns. :returns: A list of the category-unioned dataframes in the same order they were provided. :raises TypeError: If any of the inputs are not pandas DataFrames. \"\"\" if any(not isinstance(df, pd.DataFrame) for df in dataframes): raise TypeError(\"Inputs must all be dataframes.\") if column_names is None: # Find all columns across all dataframes that are categorical column_names = set() for dataframe in dataframes: column_names = column_names.union( [ column_name for column_name in dataframe.columns if isinstance( dataframe[column_name].dtype, pd.CategoricalDtype ) ] ) else: column_names = [column_names] # For each categorical column, find all possible values across the DFs category_unions = { column_name: union_categoricals( [df[column_name] for df in dataframes if column_name in df.columns] ) for column_name in column_names } # Make a shallow copy of all DFs and modify the categorical columns # such that they can encode the union of all possible categories for each. refactored_dfs = [] for df in dataframes: df = df.copy(deep=False) for column_name, categorical in category_unions.items(): if column_name in df.columns: df[column_name] = pd.Categorical( df[column_name], categories=categorical.categories ) refactored_dfs.append(df) return refactored_dfs","title":"unionize_dataframe_categories()"},{"location":"api/io/","text":"Input/Output (io) read_csvs(files_path, separate_df=False, **kwargs) Read multiple CSV files and return a dictionary of DataFrames, or one concatenated DataFrame. Parameters: Name Type Description Default files_path Union[str, Iterable[str]] The filepath pattern matching the CSV files. Accepts regular expressions, with or without .csv extension. Also accepts iterable of file paths. required separate_df bool If False (default), returns a single Dataframe with the concatenation of the csv files. If True , returns a dictionary of separate DataFrames for each CSV file. False kwargs Keyword arguments to pass into the original pandas read_csv . {} Returns: Type Description Union[pandas.core.frame.DataFrame, dict] DataFrame of concatenated DataFrames or dictionary of DataFrames. Exceptions: Type Description JanitorError if None provided for files_path . JanitorError if length of files_path is 0 . ValueError if no CSV files exist in files_path . ValueError if columns in input CSV files do not match. Source code in janitor/io.py @deprecated_alias(seperate_df=\"separate_df\", filespath=\"files_path\") def read_csvs( files_path: Union[str, Iterable[str]], separate_df: bool = False, **kwargs ) -> Union[pd.DataFrame, dict]: \"\"\" Read multiple CSV files and return a dictionary of DataFrames, or one concatenated DataFrame. :param files_path: The filepath pattern matching the CSV files. Accepts regular expressions, with or without `.csv` extension. Also accepts iterable of file paths. :param separate_df: If `False` (default), returns a single Dataframe with the concatenation of the csv files. If `True`, returns a dictionary of separate DataFrames for each CSV file. :param kwargs: Keyword arguments to pass into the original pandas `read_csv`. :returns: DataFrame of concatenated DataFrames or dictionary of DataFrames. :raises JanitorError: if `None` provided for `files_path`. :raises JanitorError: if length of `files_path` is `0`. :raises ValueError: if no CSV files exist in `files_path`. :raises ValueError: if columns in input CSV files do not match. \"\"\" # Sanitize input if files_path is None: raise JanitorError(\"`None` provided for `files_path`\") if len(files_path) == 0: raise JanitorError(\"0 length `files_path` provided\") # Read the csv files # String to file/folder or file pattern provided if isinstance(files_path, str): dfs_dict = { os.path.basename(f): pd.read_csv(f, **kwargs) for f in glob(files_path) } # Iterable of file paths provided else: dfs_dict = { os.path.basename(f): pd.read_csv(f, **kwargs) for f in files_path } # Check if dataframes have been read if len(dfs_dict) == 0: raise ValueError(\"No CSV files to read with the given `files_path`\") # Concatenate the dataframes if requested (default) col_names = list(dfs_dict.values())[0].columns # noqa: PD011 if not separate_df: # If columns do not match raise an error for df in dfs_dict.values(): # noqa: PD011 if not all(df.columns == col_names): raise ValueError( \"Columns in input CSV files do not match.\" \"Files cannot be concatenated\" ) return pd.concat( list(dfs_dict.values()), # noqa: PD011 ignore_index=True, sort=False, ) else: return dfs_dict","title":"Input/Output (io)"},{"location":"api/io/#inputoutput-io","text":"","title":"Input/Output (io)"},{"location":"api/io/#janitor.io.read_csvs","text":"Read multiple CSV files and return a dictionary of DataFrames, or one concatenated DataFrame. Parameters: Name Type Description Default files_path Union[str, Iterable[str]] The filepath pattern matching the CSV files. Accepts regular expressions, with or without .csv extension. Also accepts iterable of file paths. required separate_df bool If False (default), returns a single Dataframe with the concatenation of the csv files. If True , returns a dictionary of separate DataFrames for each CSV file. False kwargs Keyword arguments to pass into the original pandas read_csv . {} Returns: Type Description Union[pandas.core.frame.DataFrame, dict] DataFrame of concatenated DataFrames or dictionary of DataFrames. Exceptions: Type Description JanitorError if None provided for files_path . JanitorError if length of files_path is 0 . ValueError if no CSV files exist in files_path . ValueError if columns in input CSV files do not match. Source code in janitor/io.py @deprecated_alias(seperate_df=\"separate_df\", filespath=\"files_path\") def read_csvs( files_path: Union[str, Iterable[str]], separate_df: bool = False, **kwargs ) -> Union[pd.DataFrame, dict]: \"\"\" Read multiple CSV files and return a dictionary of DataFrames, or one concatenated DataFrame. :param files_path: The filepath pattern matching the CSV files. Accepts regular expressions, with or without `.csv` extension. Also accepts iterable of file paths. :param separate_df: If `False` (default), returns a single Dataframe with the concatenation of the csv files. If `True`, returns a dictionary of separate DataFrames for each CSV file. :param kwargs: Keyword arguments to pass into the original pandas `read_csv`. :returns: DataFrame of concatenated DataFrames or dictionary of DataFrames. :raises JanitorError: if `None` provided for `files_path`. :raises JanitorError: if length of `files_path` is `0`. :raises ValueError: if no CSV files exist in `files_path`. :raises ValueError: if columns in input CSV files do not match. \"\"\" # Sanitize input if files_path is None: raise JanitorError(\"`None` provided for `files_path`\") if len(files_path) == 0: raise JanitorError(\"0 length `files_path` provided\") # Read the csv files # String to file/folder or file pattern provided if isinstance(files_path, str): dfs_dict = { os.path.basename(f): pd.read_csv(f, **kwargs) for f in glob(files_path) } # Iterable of file paths provided else: dfs_dict = { os.path.basename(f): pd.read_csv(f, **kwargs) for f in files_path } # Check if dataframes have been read if len(dfs_dict) == 0: raise ValueError(\"No CSV files to read with the given `files_path`\") # Concatenate the dataframes if requested (default) col_names = list(dfs_dict.values())[0].columns # noqa: PD011 if not separate_df: # If columns do not match raise an error for df in dfs_dict.values(): # noqa: PD011 if not all(df.columns == col_names): raise ValueError( \"Columns in input CSV files do not match.\" \"Files cannot be concatenated\" ) return pd.concat( list(dfs_dict.values()), # noqa: PD011 ignore_index=True, sort=False, ) else: return dfs_dict","title":"read_csvs()"},{"location":"api/math/","text":"Math Miscellaneous mathematical operators. ecdf(s) Return cumulative distribution of values in a series. Intended to be used with the following pattern: df = pd.DataFrame(...) # Obtain ECDF values to be plotted x, y = df[\"column_name\"].ecdf() # Plot ECDF values plt.scatter(x, y) Null values must be dropped from the series, otherwise a ValueError is raised. Also, if the dtype of the series is not numeric, a TypeError is raised. Parameters: Name Type Description Default s Series A pandas series. dtype should be numeric. required Returns: Type Description Tuple[numpy.ndarray, numpy.ndarray] (x, y) . x : sorted array of values. y : cumulative fraction of data points with value x or lower. Exceptions: Type Description TypeError if series is not numeric. ValueError if series contains nulls. Source code in janitor/math.py @pf.register_series_method def ecdf(s: pd.Series) -> Tuple[np.ndarray, np.ndarray]: \"\"\" Return cumulative distribution of values in a series. Intended to be used with the following pattern: ```python df = pd.DataFrame(...) # Obtain ECDF values to be plotted x, y = df[\"column_name\"].ecdf() # Plot ECDF values plt.scatter(x, y) ``` Null values must be dropped from the series, otherwise a `ValueError` is raised. Also, if the `dtype` of the series is not numeric, a `TypeError` is raised. :param s: A pandas series. `dtype` should be numeric. :returns: `(x, y)`. `x`: sorted array of values. `y`: cumulative fraction of data points with value `x` or lower. :raises TypeError: if series is not numeric. :raises ValueError: if series contains nulls. \"\"\" if not is_numeric_dtype(s): raise TypeError(f\"series {s.name} must be numeric!\") if not s.isna().sum() == 0: raise ValueError(f\"series {s.name} contains nulls. Please drop them.\") n = len(s) x = np.sort(s) y = np.arange(1, n + 1) / n return x, y exp(s) Take the exponential transform of the series. Parameters: Name Type Description Default s Series Input Series. required Returns: Type Description Series Transformed Series. Source code in janitor/math.py @pf.register_series_method def exp(s: pd.Series) -> pd.Series: \"\"\" Take the exponential transform of the series. :param s: Input Series. :return: Transformed Series. \"\"\" return np.exp(s) log(s, error='warn') Take natural logarithm of the Series. Parameters: Name Type Description Default s Series Input Series. required error str Determines behavior when taking the log of nonpositive entries. If 'warn' then a RuntimeWarning is thrown. If 'raise' , then a RuntimeError is thrown. Otherwise, nothing is thrown and log of nonpositive values is np.nan ; defaults to 'warn' . 'warn' Returns: Type Description Series Transformed Series. Exceptions: Type Description RuntimeError Raised when there are nonpositive values in the Series and error='raise' . Source code in janitor/math.py @pf.register_series_method def log(s: pd.Series, error: str = \"warn\") -> pd.Series: \"\"\" Take natural logarithm of the Series. :param s: Input Series. :param error: Determines behavior when taking the log of nonpositive entries. If `'warn'` then a `RuntimeWarning` is thrown. If `'raise'`, then a `RuntimeError` is thrown. Otherwise, nothing is thrown and log of nonpositive values is `np.nan`; defaults to `'warn'`. :raises RuntimeError: Raised when there are nonpositive values in the Series and `error='raise'`. :return: Transformed Series. \"\"\" s = s.copy() nonpositive = s <= 0 if (nonpositive).any(): msg = f\"Log taken on {nonpositive.sum()} nonpositive value(s)\" if error.lower() == \"warn\": warnings.warn(msg, RuntimeWarning) if error.lower() == \"raise\": raise RuntimeError(msg) else: pass s[nonpositive] = np.nan return np.log(s) logit(s, error='warn') Take logit transform of the Series where: logit(p) = log(p/(1-p)) Parameters: Name Type Description Default s Series Input Series. required error str Determines behavior when s / (1-s) is outside of (0, 1) . If 'warn' then a RuntimeWarning is thrown. If 'raise' , then a RuntimeError is thrown. Otherwise, nothing is thrown and np.nan is returned for the problematic entries; defaults to 'warn' . 'warn' Returns: Type Description Series Transformed Series. Exceptions: Type Description RuntimeError if error is set to 'raise' . Source code in janitor/math.py @pf.register_series_method def logit(s: pd.Series, error: str = \"warn\") -> pd.Series: \"\"\" Take logit transform of the Series where: ```python logit(p) = log(p/(1-p)) ``` :param s: Input Series. :param error: Determines behavior when `s / (1-s)` is outside of `(0, 1)`. If `'warn'` then a `RuntimeWarning` is thrown. If `'raise'`, then a `RuntimeError` is thrown. Otherwise, nothing is thrown and `np.nan` is returned for the problematic entries; defaults to `'warn'`. :return: Transformed Series. :raises RuntimeError: if `error` is set to `'raise'`. \"\"\" s = s.copy() odds_ratio = s / (1 - s) outside_support = (odds_ratio <= 0) | (odds_ratio >= 1) if (outside_support).any(): msg = f\"Odds ratio for {outside_support.sum()} value(s) \\ are outside of (0, 1)\" if error.lower() == \"warn\": warnings.warn(msg, RuntimeWarning) if error.lower() == \"raise\": raise RuntimeError(msg) else: pass odds_ratio[outside_support] = np.nan return odds_ratio.log(error=\"ignore\") normal_cdf(s) Transforms the Series via the CDF of the Normal distribution. Parameters: Name Type Description Default s Series Input Series. required Returns: Type Description Series Transformed Series. Source code in janitor/math.py @pf.register_series_method def normal_cdf(s: pd.Series) -> pd.Series: \"\"\" Transforms the Series via the CDF of the Normal distribution. :param s: Input Series. :return: Transformed Series. \"\"\" return pd.Series(norm.cdf(s), index=s.index) probit(s, error='warn') Transforms the Series via the inverse CDF of the Normal distribution. Parameters: Name Type Description Default s Series Input Series. required error str Determines behavior when s is outside of (0, 1) . If 'warn' then a RuntimeWarning is thrown. If 'raise' , then a RuntimeError is thrown. Otherwise, nothing is thrown and np.nan is returned for the problematic entries; defaults to 'warn' . 'warn' Returns: Type Description Series Transformed Series Exceptions: Type Description RuntimeError Raised when there are problematic values in the Series and error='raise' . Source code in janitor/math.py @pf.register_series_method def probit(s: pd.Series, error: str = \"warn\") -> pd.Series: \"\"\" Transforms the Series via the inverse CDF of the Normal distribution. :param s: Input Series. :param error: Determines behavior when `s` is outside of `(0, 1)`. If `'warn'` then a `RuntimeWarning` is thrown. If `'raise'`, then a `RuntimeError` is thrown. Otherwise, nothing is thrown and `np.nan` is returned for the problematic entries; defaults to `'warn'`. :raises RuntimeError: Raised when there are problematic values in the Series and `error='raise'`. :return: Transformed Series \"\"\" s = s.copy() outside_support = (s <= 0) | (s >= 1) if (outside_support).any(): msg = f\"{outside_support.sum()} value(s) are outside of (0, 1)\" if error.lower() == \"warn\": warnings.warn(msg, RuntimeWarning) if error.lower() == \"raise\": raise RuntimeError(msg) else: pass s[outside_support] = np.nan with np.errstate(all=\"ignore\"): out = pd.Series(norm.ppf(s), index=s.index) return out sigmoid(s) Take the sigmoid transform of the series where: sigmoid(x) = 1 / (1 + exp(-x)) Parameters: Name Type Description Default s Series Input Series. required Returns: Type Description Series Transformed Series. Source code in janitor/math.py @pf.register_series_method def sigmoid(s: pd.Series) -> pd.Series: \"\"\" Take the sigmoid transform of the series where: ```python sigmoid(x) = 1 / (1 + exp(-x)) ``` :param s: Input Series. :return: Transformed Series. \"\"\" return expit(s) softmax(s) Take the softmax transform of the series. The softmax function transforms each element of a collection by computing the exponential of each element divided by the sum of the exponentials of all the elements. That is, if x is a one-dimensional numpy array or pandas Series: softmax(x) = exp(x)/sum(exp(x)) Parameters: Name Type Description Default s Series Input Series. required Returns: Type Description Series Transformed Series. Source code in janitor/math.py @pf.register_series_method def softmax(s: pd.Series) -> pd.Series: \"\"\" Take the softmax transform of the series. The softmax function transforms each element of a collection by computing the exponential of each element divided by the sum of the exponentials of all the elements. That is, if x is a one-dimensional numpy array or pandas Series: ```python softmax(x) = exp(x)/sum(exp(x)) ``` :param s: Input Series. :return: Transformed Series. \"\"\" return scipy_softmax(s) z_score(s, moments_dict=None, keys=('mean', 'std')) Transforms the Series into z-scores where: z = (s - s.mean()) / s.std() Parameters: Name Type Description Default s Series Input Series. required moments_dict dict If not None , then the mean and standard deviation used to compute the z-score transformation is saved as entries in moments_dict with keys determined by the keys argument; defaults to None . None keys Tuple[str, str] Determines the keys saved in moments_dict if moments are saved; defaults to ( 'mean' , 'std' ). ('mean', 'std') Returns: Type Description Series Transformed Series. Source code in janitor/math.py @pf.register_series_method def z_score( s: pd.Series, moments_dict: dict = None, keys: Tuple[str, str] = (\"mean\", \"std\"), ) -> pd.Series: \"\"\" Transforms the Series into z-scores where: ```python z = (s - s.mean()) / s.std() ``` :param s: Input Series. :param moments_dict: If not `None`, then the mean and standard deviation used to compute the z-score transformation is saved as entries in `moments_dict` with keys determined by the `keys` argument; defaults to `None`. :param keys: Determines the keys saved in `moments_dict` if moments are saved; defaults to (`'mean'`, `'std'`). :return: Transformed Series. \"\"\" mean = s.mean() std = s.std() if std == 0: return 0 if moments_dict is not None: moments_dict[keys[0]] = mean moments_dict[keys[1]] = std return (s - mean) / std","title":"Math"},{"location":"api/math/#math","text":"Miscellaneous mathematical operators.","title":"Math"},{"location":"api/math/#janitor.math.ecdf","text":"Return cumulative distribution of values in a series. Intended to be used with the following pattern: df = pd.DataFrame(...) # Obtain ECDF values to be plotted x, y = df[\"column_name\"].ecdf() # Plot ECDF values plt.scatter(x, y) Null values must be dropped from the series, otherwise a ValueError is raised. Also, if the dtype of the series is not numeric, a TypeError is raised. Parameters: Name Type Description Default s Series A pandas series. dtype should be numeric. required Returns: Type Description Tuple[numpy.ndarray, numpy.ndarray] (x, y) . x : sorted array of values. y : cumulative fraction of data points with value x or lower. Exceptions: Type Description TypeError if series is not numeric. ValueError if series contains nulls. Source code in janitor/math.py @pf.register_series_method def ecdf(s: pd.Series) -> Tuple[np.ndarray, np.ndarray]: \"\"\" Return cumulative distribution of values in a series. Intended to be used with the following pattern: ```python df = pd.DataFrame(...) # Obtain ECDF values to be plotted x, y = df[\"column_name\"].ecdf() # Plot ECDF values plt.scatter(x, y) ``` Null values must be dropped from the series, otherwise a `ValueError` is raised. Also, if the `dtype` of the series is not numeric, a `TypeError` is raised. :param s: A pandas series. `dtype` should be numeric. :returns: `(x, y)`. `x`: sorted array of values. `y`: cumulative fraction of data points with value `x` or lower. :raises TypeError: if series is not numeric. :raises ValueError: if series contains nulls. \"\"\" if not is_numeric_dtype(s): raise TypeError(f\"series {s.name} must be numeric!\") if not s.isna().sum() == 0: raise ValueError(f\"series {s.name} contains nulls. Please drop them.\") n = len(s) x = np.sort(s) y = np.arange(1, n + 1) / n return x, y","title":"ecdf()"},{"location":"api/math/#janitor.math.exp","text":"Take the exponential transform of the series. Parameters: Name Type Description Default s Series Input Series. required Returns: Type Description Series Transformed Series. Source code in janitor/math.py @pf.register_series_method def exp(s: pd.Series) -> pd.Series: \"\"\" Take the exponential transform of the series. :param s: Input Series. :return: Transformed Series. \"\"\" return np.exp(s)","title":"exp()"},{"location":"api/math/#janitor.math.log","text":"Take natural logarithm of the Series. Parameters: Name Type Description Default s Series Input Series. required error str Determines behavior when taking the log of nonpositive entries. If 'warn' then a RuntimeWarning is thrown. If 'raise' , then a RuntimeError is thrown. Otherwise, nothing is thrown and log of nonpositive values is np.nan ; defaults to 'warn' . 'warn' Returns: Type Description Series Transformed Series. Exceptions: Type Description RuntimeError Raised when there are nonpositive values in the Series and error='raise' . Source code in janitor/math.py @pf.register_series_method def log(s: pd.Series, error: str = \"warn\") -> pd.Series: \"\"\" Take natural logarithm of the Series. :param s: Input Series. :param error: Determines behavior when taking the log of nonpositive entries. If `'warn'` then a `RuntimeWarning` is thrown. If `'raise'`, then a `RuntimeError` is thrown. Otherwise, nothing is thrown and log of nonpositive values is `np.nan`; defaults to `'warn'`. :raises RuntimeError: Raised when there are nonpositive values in the Series and `error='raise'`. :return: Transformed Series. \"\"\" s = s.copy() nonpositive = s <= 0 if (nonpositive).any(): msg = f\"Log taken on {nonpositive.sum()} nonpositive value(s)\" if error.lower() == \"warn\": warnings.warn(msg, RuntimeWarning) if error.lower() == \"raise\": raise RuntimeError(msg) else: pass s[nonpositive] = np.nan return np.log(s)","title":"log()"},{"location":"api/math/#janitor.math.logit","text":"Take logit transform of the Series where: logit(p) = log(p/(1-p)) Parameters: Name Type Description Default s Series Input Series. required error str Determines behavior when s / (1-s) is outside of (0, 1) . If 'warn' then a RuntimeWarning is thrown. If 'raise' , then a RuntimeError is thrown. Otherwise, nothing is thrown and np.nan is returned for the problematic entries; defaults to 'warn' . 'warn' Returns: Type Description Series Transformed Series. Exceptions: Type Description RuntimeError if error is set to 'raise' . Source code in janitor/math.py @pf.register_series_method def logit(s: pd.Series, error: str = \"warn\") -> pd.Series: \"\"\" Take logit transform of the Series where: ```python logit(p) = log(p/(1-p)) ``` :param s: Input Series. :param error: Determines behavior when `s / (1-s)` is outside of `(0, 1)`. If `'warn'` then a `RuntimeWarning` is thrown. If `'raise'`, then a `RuntimeError` is thrown. Otherwise, nothing is thrown and `np.nan` is returned for the problematic entries; defaults to `'warn'`. :return: Transformed Series. :raises RuntimeError: if `error` is set to `'raise'`. \"\"\" s = s.copy() odds_ratio = s / (1 - s) outside_support = (odds_ratio <= 0) | (odds_ratio >= 1) if (outside_support).any(): msg = f\"Odds ratio for {outside_support.sum()} value(s) \\ are outside of (0, 1)\" if error.lower() == \"warn\": warnings.warn(msg, RuntimeWarning) if error.lower() == \"raise\": raise RuntimeError(msg) else: pass odds_ratio[outside_support] = np.nan return odds_ratio.log(error=\"ignore\")","title":"logit()"},{"location":"api/math/#janitor.math.normal_cdf","text":"Transforms the Series via the CDF of the Normal distribution. Parameters: Name Type Description Default s Series Input Series. required Returns: Type Description Series Transformed Series. Source code in janitor/math.py @pf.register_series_method def normal_cdf(s: pd.Series) -> pd.Series: \"\"\" Transforms the Series via the CDF of the Normal distribution. :param s: Input Series. :return: Transformed Series. \"\"\" return pd.Series(norm.cdf(s), index=s.index)","title":"normal_cdf()"},{"location":"api/math/#janitor.math.probit","text":"Transforms the Series via the inverse CDF of the Normal distribution. Parameters: Name Type Description Default s Series Input Series. required error str Determines behavior when s is outside of (0, 1) . If 'warn' then a RuntimeWarning is thrown. If 'raise' , then a RuntimeError is thrown. Otherwise, nothing is thrown and np.nan is returned for the problematic entries; defaults to 'warn' . 'warn' Returns: Type Description Series Transformed Series Exceptions: Type Description RuntimeError Raised when there are problematic values in the Series and error='raise' . Source code in janitor/math.py @pf.register_series_method def probit(s: pd.Series, error: str = \"warn\") -> pd.Series: \"\"\" Transforms the Series via the inverse CDF of the Normal distribution. :param s: Input Series. :param error: Determines behavior when `s` is outside of `(0, 1)`. If `'warn'` then a `RuntimeWarning` is thrown. If `'raise'`, then a `RuntimeError` is thrown. Otherwise, nothing is thrown and `np.nan` is returned for the problematic entries; defaults to `'warn'`. :raises RuntimeError: Raised when there are problematic values in the Series and `error='raise'`. :return: Transformed Series \"\"\" s = s.copy() outside_support = (s <= 0) | (s >= 1) if (outside_support).any(): msg = f\"{outside_support.sum()} value(s) are outside of (0, 1)\" if error.lower() == \"warn\": warnings.warn(msg, RuntimeWarning) if error.lower() == \"raise\": raise RuntimeError(msg) else: pass s[outside_support] = np.nan with np.errstate(all=\"ignore\"): out = pd.Series(norm.ppf(s), index=s.index) return out","title":"probit()"},{"location":"api/math/#janitor.math.sigmoid","text":"Take the sigmoid transform of the series where: sigmoid(x) = 1 / (1 + exp(-x)) Parameters: Name Type Description Default s Series Input Series. required Returns: Type Description Series Transformed Series. Source code in janitor/math.py @pf.register_series_method def sigmoid(s: pd.Series) -> pd.Series: \"\"\" Take the sigmoid transform of the series where: ```python sigmoid(x) = 1 / (1 + exp(-x)) ``` :param s: Input Series. :return: Transformed Series. \"\"\" return expit(s)","title":"sigmoid()"},{"location":"api/math/#janitor.math.softmax","text":"Take the softmax transform of the series. The softmax function transforms each element of a collection by computing the exponential of each element divided by the sum of the exponentials of all the elements. That is, if x is a one-dimensional numpy array or pandas Series: softmax(x) = exp(x)/sum(exp(x)) Parameters: Name Type Description Default s Series Input Series. required Returns: Type Description Series Transformed Series. Source code in janitor/math.py @pf.register_series_method def softmax(s: pd.Series) -> pd.Series: \"\"\" Take the softmax transform of the series. The softmax function transforms each element of a collection by computing the exponential of each element divided by the sum of the exponentials of all the elements. That is, if x is a one-dimensional numpy array or pandas Series: ```python softmax(x) = exp(x)/sum(exp(x)) ``` :param s: Input Series. :return: Transformed Series. \"\"\" return scipy_softmax(s)","title":"softmax()"},{"location":"api/math/#janitor.math.z_score","text":"Transforms the Series into z-scores where: z = (s - s.mean()) / s.std() Parameters: Name Type Description Default s Series Input Series. required moments_dict dict If not None , then the mean and standard deviation used to compute the z-score transformation is saved as entries in moments_dict with keys determined by the keys argument; defaults to None . None keys Tuple[str, str] Determines the keys saved in moments_dict if moments are saved; defaults to ( 'mean' , 'std' ). ('mean', 'std') Returns: Type Description Series Transformed Series. Source code in janitor/math.py @pf.register_series_method def z_score( s: pd.Series, moments_dict: dict = None, keys: Tuple[str, str] = (\"mean\", \"std\"), ) -> pd.Series: \"\"\" Transforms the Series into z-scores where: ```python z = (s - s.mean()) / s.std() ``` :param s: Input Series. :param moments_dict: If not `None`, then the mean and standard deviation used to compute the z-score transformation is saved as entries in `moments_dict` with keys determined by the `keys` argument; defaults to `None`. :param keys: Determines the keys saved in `moments_dict` if moments are saved; defaults to (`'mean'`, `'std'`). :return: Transformed Series. \"\"\" mean = s.mean() std = s.std() if std == 0: return 0 if moments_dict is not None: moments_dict[keys[0]] = mean moments_dict[keys[1]] = std return (s - mean) / std","title":"z_score()"},{"location":"api/ml/","text":"Machine Learning Machine learning specific functions. get_features_targets(df, target_column_names, feature_column_names=None) Get the features and targets as separate DataFrames/Series. This method does not mutate the original DataFrame. The behaviour is as such: target_column_names is mandatory. If feature_column_names is present, then we will respect the column names inside there. If feature_column_names is not passed in, then we will assume that the rest of the columns are feature columns, and return them. Functional usage example: X, y = get_features_targets(df, target_column_names=\"measurement\") Method chaining example: import pandas as pd import janitor.ml df = pd.DataFrame(...) target_cols = ['output1', 'output2'] X, y = df.get_features_targets(target_column_names=target_cols) Parameters: Name Type Description Default df DataFrame The pandas DataFrame object. required target_column_names Union[str, List, Tuple, Hashable] Either a column name or an iterable (list or tuple) of column names that are the target(s) to be predicted. required feature_column_names Union[str, Iterable[str], Hashable] (optional) The column name or iterable of column names that are the features (a.k.a. predictors) used to predict the targets. None Returns: Type Description Tuple[pandas.core.frame.DataFrame, pandas.core.frame.DataFrame] (X, Y) the feature matrix ( X ) and the target matrix ( y ). Both are pandas DataFrames. Source code in janitor/ml.py @pf.register_dataframe_method @deprecated_alias( target_columns=\"target_column_names\", feature_columns=\"feature_column_names\", ) def get_features_targets( df: pd.DataFrame, target_column_names: Union[str, Union[List, Tuple], Hashable], feature_column_names: Optional[Union[str, Iterable[str], Hashable]] = None, ) -> Tuple[pd.DataFrame, pd.DataFrame]: \"\"\" Get the features and targets as separate DataFrames/Series. This method does not mutate the original DataFrame. The behaviour is as such: - `target_column_names` is mandatory. - If `feature_column_names` is present, then we will respect the column names inside there. - If `feature_column_names` is not passed in, then we will assume that the rest of the columns are feature columns, and return them. Functional usage example: ```python X, y = get_features_targets(df, target_column_names=\"measurement\") ``` Method chaining example: ```python import pandas as pd import janitor.ml df = pd.DataFrame(...) target_cols = ['output1', 'output2'] X, y = df.get_features_targets(target_column_names=target_cols) ``` :param df: The pandas DataFrame object. :param target_column_names: Either a column name or an iterable (list or tuple) of column names that are the target(s) to be predicted. :param feature_column_names: (optional) The column name or iterable of column names that are the features (a.k.a. predictors) used to predict the targets. :returns: `(X, Y)` the feature matrix (`X`) and the target matrix (`y`). Both are pandas DataFrames. \"\"\" Y = df[target_column_names] if feature_column_names: X = df[feature_column_names] else: if isinstance(target_column_names, (list, tuple)): # noqa: W503 xcols = [c for c in df.columns if c not in target_column_names] else: xcols = [c for c in df.columns if target_column_names != c] X = df[xcols] return X, Y","title":"Machine Learning"},{"location":"api/ml/#machine-learning","text":"Machine learning specific functions.","title":"Machine Learning"},{"location":"api/ml/#janitor.ml.get_features_targets","text":"Get the features and targets as separate DataFrames/Series. This method does not mutate the original DataFrame. The behaviour is as such: target_column_names is mandatory. If feature_column_names is present, then we will respect the column names inside there. If feature_column_names is not passed in, then we will assume that the rest of the columns are feature columns, and return them. Functional usage example: X, y = get_features_targets(df, target_column_names=\"measurement\") Method chaining example: import pandas as pd import janitor.ml df = pd.DataFrame(...) target_cols = ['output1', 'output2'] X, y = df.get_features_targets(target_column_names=target_cols) Parameters: Name Type Description Default df DataFrame The pandas DataFrame object. required target_column_names Union[str, List, Tuple, Hashable] Either a column name or an iterable (list or tuple) of column names that are the target(s) to be predicted. required feature_column_names Union[str, Iterable[str], Hashable] (optional) The column name or iterable of column names that are the features (a.k.a. predictors) used to predict the targets. None Returns: Type Description Tuple[pandas.core.frame.DataFrame, pandas.core.frame.DataFrame] (X, Y) the feature matrix ( X ) and the target matrix ( y ). Both are pandas DataFrames. Source code in janitor/ml.py @pf.register_dataframe_method @deprecated_alias( target_columns=\"target_column_names\", feature_columns=\"feature_column_names\", ) def get_features_targets( df: pd.DataFrame, target_column_names: Union[str, Union[List, Tuple], Hashable], feature_column_names: Optional[Union[str, Iterable[str], Hashable]] = None, ) -> Tuple[pd.DataFrame, pd.DataFrame]: \"\"\" Get the features and targets as separate DataFrames/Series. This method does not mutate the original DataFrame. The behaviour is as such: - `target_column_names` is mandatory. - If `feature_column_names` is present, then we will respect the column names inside there. - If `feature_column_names` is not passed in, then we will assume that the rest of the columns are feature columns, and return them. Functional usage example: ```python X, y = get_features_targets(df, target_column_names=\"measurement\") ``` Method chaining example: ```python import pandas as pd import janitor.ml df = pd.DataFrame(...) target_cols = ['output1', 'output2'] X, y = df.get_features_targets(target_column_names=target_cols) ``` :param df: The pandas DataFrame object. :param target_column_names: Either a column name or an iterable (list or tuple) of column names that are the target(s) to be predicted. :param feature_column_names: (optional) The column name or iterable of column names that are the features (a.k.a. predictors) used to predict the targets. :returns: `(X, Y)` the feature matrix (`X`) and the target matrix (`y`). Both are pandas DataFrames. \"\"\" Y = df[target_column_names] if feature_column_names: X = df[feature_column_names] else: if isinstance(target_column_names, (list, tuple)): # noqa: W503 xcols = [c for c in df.columns if c not in target_column_names] else: xcols = [c for c in df.columns if target_column_names != c] X = df[xcols] return X, Y","title":"get_features_targets()"},{"location":"api/pyspark/","text":"PySpark backend Backend functions for pyspark. CachedAccessor Custom property-like object (descriptor) for caching accessors. Parameters name : str The namespace this will be accessed under, e.g. df.foo accessor : cls The class with the extension methods. NOTE Modified based on pandas.core.accessor. register_dataframe_accessor(name) NOTE Modified based on pandas.core.accessor. .. # noqa: DAR101 name .. # noqa: DAR201 Source code in janitor/spark/backend.py def register_dataframe_accessor(name): \"\"\" NOTE ---- Modified based on pandas.core.accessor. .. # noqa: DAR101 name .. # noqa: DAR201 \"\"\" try: from pyspark.sql import DataFrame except ImportError: import_message( submodule=\"spark\", package=\"pyspark\", conda_channel=\"conda-forge\", pip_install=True, ) return _register_accessor(name, DataFrame) register_dataframe_method(method) Register a function as a method attached to the Pyspark DataFrame. NOTE Modified based on pandas_flavor.register. .. # noqa: DAR101 method .. # noqa: DAR201 Source code in janitor/spark/backend.py def register_dataframe_method(method): \"\"\"Register a function as a method attached to the Pyspark DataFrame. NOTE ---- Modified based on pandas_flavor.register. .. # noqa: DAR101 method .. # noqa: DAR201 \"\"\" def inner(*args, **kwargs): class AccessorMethod: def __init__(self, pyspark_obj): self._obj = pyspark_obj @wraps(method) def __call__(self, *args, **kwargs): return method(self._obj, *args, **kwargs) register_dataframe_accessor(method.__name__)(AccessorMethod) return method return inner() functions General purpose data cleaning functions for pyspark. clean_names(df, case_type='lower', remove_special=False, strip_underscores=None) Clean column names for pyspark dataframe. Takes all column names, converts them to lowercase, then replaces all spaces with underscores. This method does not mutate the original DataFrame. Functional usage example: .. code-block:: python df = clean_names(df) Method chaining example: .. code-block:: python from pyspark.sql import DataFrame import janitor.spark df = DataFrame(...).clean_names() :Example of transformation: .. code-block:: python Columns before: First Name, Last Name, Employee Status, Subject Columns after: first_name, last_name, employee_status, subject Parameters: Name Type Description Default df DataFrame Spark DataFrame object. required strip_underscores str (optional) Removes the outer underscores from all column names. Default None keeps outer underscores. Values can be either 'left', 'right' or 'both' or the respective shorthand 'l', 'r' and True. None case_type str (optional) Whether to make columns lower or uppercase. Current case may be preserved with 'preserve', while snake case conversion (from CamelCase or camelCase only) can be turned on using \"snake\". Default 'lower' makes all characters lowercase. 'lower' remove_special bool (optional) Remove special characters from columns. Only letters, numbers and underscores are preserved. False Returns: Type Description DataFrame A Spark DataFrame. Source code in janitor/spark/functions.py @backend.register_dataframe_method def clean_names( df: DataFrame, case_type: str = \"lower\", remove_special: bool = False, strip_underscores: str = None, ) -> DataFrame: \"\"\" Clean column names for pyspark dataframe. Takes all column names, converts them to lowercase, then replaces all spaces with underscores. This method does not mutate the original DataFrame. Functional usage example: .. code-block:: python df = clean_names(df) Method chaining example: .. code-block:: python from pyspark.sql import DataFrame import janitor.spark df = DataFrame(...).clean_names() :Example of transformation: .. code-block:: python Columns before: First Name, Last Name, Employee Status, Subject Columns after: first_name, last_name, employee_status, subject :param df: Spark DataFrame object. :param strip_underscores: (optional) Removes the outer underscores from all column names. Default None keeps outer underscores. Values can be either 'left', 'right' or 'both' or the respective shorthand 'l', 'r' and True. :param case_type: (optional) Whether to make columns lower or uppercase. Current case may be preserved with 'preserve', while snake case conversion (from CamelCase or camelCase only) can be turned on using \"snake\". Default 'lower' makes all characters lowercase. :param remove_special: (optional) Remove special characters from columns. Only letters, numbers and underscores are preserved. :returns: A Spark DataFrame. \"\"\" cols = df.columns cols = [_change_case(col, case_type) for col in cols] cols = [_normalize_1(col) for col in cols] if remove_special: cols = [_remove_special(col) for col in cols] cols = [re.sub(\"_+\", \"_\", col) for col in cols] # noqa: PD005 cols = [_strip_underscores_func(col, strip_underscores) for col in cols] cols = [ f\"`{col}` AS `{new_col}`\" for col, new_col in zip(df.columns, cols) ] return df.selectExpr(*cols) update_where(df, conditions, target_column_name, target_val) Add multiple conditions to update a column in the dataframe. This method does not mutate the original DataFrame. Example usage: .. code-block:: python import pandas as pd from pyspark.sql import SparkSession import janitor.spark spark = SparkSession.builder.getOrCreate() data = { \"a\": [1, 2, 3, 4], \"b\": [5, 6, 7, 8], \"c\": [0, 0, 0, 0] } df = spark.createDataFrame(pd.DataFrame(data)) df = ( df .update_where( conditions=\"a > 2 AND b < 8\", target_column_name='c', target_val=10 ) ) df.show() # +---+---+---+ # | a| b| c| # +---+---+---+ # | 1| 5| 0| # | 2| 6| 0| # | 3| 7| 10| # | 4| 8| 0| # +---+---+---+ Parameters: Name Type Description Default df DataFrame Spark DataFrame object. required conditions str Spark SQL string condition used to update a target column and target value required target_column_name str Column to be updated. If column does not exist in dataframe, a new column will be created; note that entries that do not get set in the new column will be null. required target_val Union[str, int, float] Value to be updated required Returns: Type Description DataFrame An updated spark DataFrame. Source code in janitor/spark/functions.py @backend.register_dataframe_method def update_where( df: DataFrame, conditions: str, target_column_name: str, target_val: Union[str, int, float], ) -> DataFrame: \"\"\" Add multiple conditions to update a column in the dataframe. This method does not mutate the original DataFrame. Example usage: .. code-block:: python import pandas as pd from pyspark.sql import SparkSession import janitor.spark spark = SparkSession.builder.getOrCreate() data = { \"a\": [1, 2, 3, 4], \"b\": [5, 6, 7, 8], \"c\": [0, 0, 0, 0] } df = spark.createDataFrame(pd.DataFrame(data)) df = ( df .update_where( conditions=\"a > 2 AND b < 8\", target_column_name='c', target_val=10 ) ) df.show() # +---+---+---+ # | a| b| c| # +---+---+---+ # | 1| 5| 0| # | 2| 6| 0| # | 3| 7| 10| # | 4| 8| 0| # +---+---+---+ :param df: Spark DataFrame object. :param conditions: Spark SQL string condition used to update a target column and target value :param target_column_name: Column to be updated. If column does not exist in dataframe, a new column will be created; note that entries that do not get set in the new column will be null. :param target_val: Value to be updated :returns: An updated spark DataFrame. \"\"\" # String without quotes are treated as column name if isinstance(target_val, str): target_val = f\"'{target_val}'\" if target_column_name in df.columns: # `{col]` is to enable non-standard column name, # i.e. column name with special characters etc. select_stmts = [ f\"`{col}`\" for col in df.columns if col != target_column_name ] + [ f\"\"\" CASE WHEN {conditions} THEN {target_val} ELSE `{target_column_name}` END AS `{target_column_name}` \"\"\" ] # This is to retain the ordering col_order = df.columns else: select_stmts = [f\"`{col}`\" for col in df.columns] + [ f\"\"\" CASE WHEN {conditions} THEN {target_val} ELSE NULL END AS `{target_column_name}` \"\"\" ] col_order = df.columns + [target_column_name] return df.selectExpr(*select_stmts).select(*col_order)","title":"PySpark"},{"location":"api/pyspark/#pyspark","text":"","title":"PySpark"},{"location":"api/pyspark/#janitor.spark.backend","text":"Backend functions for pyspark.","title":"backend"},{"location":"api/pyspark/#janitor.spark.backend.CachedAccessor","text":"Custom property-like object (descriptor) for caching accessors.","title":"CachedAccessor"},{"location":"api/pyspark/#janitor.spark.backend.CachedAccessor--parameters","text":"name : str The namespace this will be accessed under, e.g. df.foo accessor : cls The class with the extension methods.","title":"Parameters"},{"location":"api/pyspark/#janitor.spark.backend.CachedAccessor--note","text":"Modified based on pandas.core.accessor.","title":"NOTE"},{"location":"api/pyspark/#janitor.spark.backend.register_dataframe_accessor","text":"","title":"register_dataframe_accessor()"},{"location":"api/pyspark/#janitor.spark.backend.register_dataframe_accessor--note","text":"Modified based on pandas.core.accessor. .. # noqa: DAR101 name .. # noqa: DAR201 Source code in janitor/spark/backend.py def register_dataframe_accessor(name): \"\"\" NOTE ---- Modified based on pandas.core.accessor. .. # noqa: DAR101 name .. # noqa: DAR201 \"\"\" try: from pyspark.sql import DataFrame except ImportError: import_message( submodule=\"spark\", package=\"pyspark\", conda_channel=\"conda-forge\", pip_install=True, ) return _register_accessor(name, DataFrame)","title":"NOTE"},{"location":"api/pyspark/#janitor.spark.backend.register_dataframe_method","text":"Register a function as a method attached to the Pyspark DataFrame.","title":"register_dataframe_method()"},{"location":"api/pyspark/#janitor.spark.backend.register_dataframe_method--note","text":"Modified based on pandas_flavor.register. .. # noqa: DAR101 method .. # noqa: DAR201 Source code in janitor/spark/backend.py def register_dataframe_method(method): \"\"\"Register a function as a method attached to the Pyspark DataFrame. NOTE ---- Modified based on pandas_flavor.register. .. # noqa: DAR101 method .. # noqa: DAR201 \"\"\" def inner(*args, **kwargs): class AccessorMethod: def __init__(self, pyspark_obj): self._obj = pyspark_obj @wraps(method) def __call__(self, *args, **kwargs): return method(self._obj, *args, **kwargs) register_dataframe_accessor(method.__name__)(AccessorMethod) return method return inner()","title":"NOTE"},{"location":"api/pyspark/#janitor.spark.functions","text":"General purpose data cleaning functions for pyspark.","title":"functions"},{"location":"api/pyspark/#janitor.spark.functions.clean_names","text":"Clean column names for pyspark dataframe. Takes all column names, converts them to lowercase, then replaces all spaces with underscores. This method does not mutate the original DataFrame. Functional usage example: .. code-block:: python df = clean_names(df) Method chaining example: .. code-block:: python from pyspark.sql import DataFrame import janitor.spark df = DataFrame(...).clean_names() :Example of transformation: .. code-block:: python Columns before: First Name, Last Name, Employee Status, Subject Columns after: first_name, last_name, employee_status, subject Parameters: Name Type Description Default df DataFrame Spark DataFrame object. required strip_underscores str (optional) Removes the outer underscores from all column names. Default None keeps outer underscores. Values can be either 'left', 'right' or 'both' or the respective shorthand 'l', 'r' and True. None case_type str (optional) Whether to make columns lower or uppercase. Current case may be preserved with 'preserve', while snake case conversion (from CamelCase or camelCase only) can be turned on using \"snake\". Default 'lower' makes all characters lowercase. 'lower' remove_special bool (optional) Remove special characters from columns. Only letters, numbers and underscores are preserved. False Returns: Type Description DataFrame A Spark DataFrame. Source code in janitor/spark/functions.py @backend.register_dataframe_method def clean_names( df: DataFrame, case_type: str = \"lower\", remove_special: bool = False, strip_underscores: str = None, ) -> DataFrame: \"\"\" Clean column names for pyspark dataframe. Takes all column names, converts them to lowercase, then replaces all spaces with underscores. This method does not mutate the original DataFrame. Functional usage example: .. code-block:: python df = clean_names(df) Method chaining example: .. code-block:: python from pyspark.sql import DataFrame import janitor.spark df = DataFrame(...).clean_names() :Example of transformation: .. code-block:: python Columns before: First Name, Last Name, Employee Status, Subject Columns after: first_name, last_name, employee_status, subject :param df: Spark DataFrame object. :param strip_underscores: (optional) Removes the outer underscores from all column names. Default None keeps outer underscores. Values can be either 'left', 'right' or 'both' or the respective shorthand 'l', 'r' and True. :param case_type: (optional) Whether to make columns lower or uppercase. Current case may be preserved with 'preserve', while snake case conversion (from CamelCase or camelCase only) can be turned on using \"snake\". Default 'lower' makes all characters lowercase. :param remove_special: (optional) Remove special characters from columns. Only letters, numbers and underscores are preserved. :returns: A Spark DataFrame. \"\"\" cols = df.columns cols = [_change_case(col, case_type) for col in cols] cols = [_normalize_1(col) for col in cols] if remove_special: cols = [_remove_special(col) for col in cols] cols = [re.sub(\"_+\", \"_\", col) for col in cols] # noqa: PD005 cols = [_strip_underscores_func(col, strip_underscores) for col in cols] cols = [ f\"`{col}` AS `{new_col}`\" for col, new_col in zip(df.columns, cols) ] return df.selectExpr(*cols)","title":"clean_names()"},{"location":"api/pyspark/#janitor.spark.functions.update_where","text":"Add multiple conditions to update a column in the dataframe. This method does not mutate the original DataFrame. Example usage: .. code-block:: python import pandas as pd from pyspark.sql import SparkSession import janitor.spark spark = SparkSession.builder.getOrCreate() data = { \"a\": [1, 2, 3, 4], \"b\": [5, 6, 7, 8], \"c\": [0, 0, 0, 0] } df = spark.createDataFrame(pd.DataFrame(data)) df = ( df .update_where( conditions=\"a > 2 AND b < 8\", target_column_name='c', target_val=10 ) ) df.show() # +---+---+---+ # | a| b| c| # +---+---+---+ # | 1| 5| 0| # | 2| 6| 0| # | 3| 7| 10| # | 4| 8| 0| # +---+---+---+ Parameters: Name Type Description Default df DataFrame Spark DataFrame object. required conditions str Spark SQL string condition used to update a target column and target value required target_column_name str Column to be updated. If column does not exist in dataframe, a new column will be created; note that entries that do not get set in the new column will be null. required target_val Union[str, int, float] Value to be updated required Returns: Type Description DataFrame An updated spark DataFrame. Source code in janitor/spark/functions.py @backend.register_dataframe_method def update_where( df: DataFrame, conditions: str, target_column_name: str, target_val: Union[str, int, float], ) -> DataFrame: \"\"\" Add multiple conditions to update a column in the dataframe. This method does not mutate the original DataFrame. Example usage: .. code-block:: python import pandas as pd from pyspark.sql import SparkSession import janitor.spark spark = SparkSession.builder.getOrCreate() data = { \"a\": [1, 2, 3, 4], \"b\": [5, 6, 7, 8], \"c\": [0, 0, 0, 0] } df = spark.createDataFrame(pd.DataFrame(data)) df = ( df .update_where( conditions=\"a > 2 AND b < 8\", target_column_name='c', target_val=10 ) ) df.show() # +---+---+---+ # | a| b| c| # +---+---+---+ # | 1| 5| 0| # | 2| 6| 0| # | 3| 7| 10| # | 4| 8| 0| # +---+---+---+ :param df: Spark DataFrame object. :param conditions: Spark SQL string condition used to update a target column and target value :param target_column_name: Column to be updated. If column does not exist in dataframe, a new column will be created; note that entries that do not get set in the new column will be null. :param target_val: Value to be updated :returns: An updated spark DataFrame. \"\"\" # String without quotes are treated as column name if isinstance(target_val, str): target_val = f\"'{target_val}'\" if target_column_name in df.columns: # `{col]` is to enable non-standard column name, # i.e. column name with special characters etc. select_stmts = [ f\"`{col}`\" for col in df.columns if col != target_column_name ] + [ f\"\"\" CASE WHEN {conditions} THEN {target_val} ELSE `{target_column_name}` END AS `{target_column_name}` \"\"\" ] # This is to retain the ordering col_order = df.columns else: select_stmts = [f\"`{col}`\" for col in df.columns] + [ f\"\"\" CASE WHEN {conditions} THEN {target_val} ELSE NULL END AS `{target_column_name}` \"\"\" ] col_order = df.columns + [target_column_name] return df.selectExpr(*select_stmts).select(*col_order)","title":"update_where()"},{"location":"api/timeseries/","text":"Timeseries Time series-specific data cleaning functions. fill_missing_timestamps(df, frequency, first_time_stamp=None, last_time_stamp=None) Fills a DataFrame with missing timestamps based on a defined frequency. If timestamps are missing, this function will re-index the DataFrame. If timestamps are not missing, then the function will return the DataFrame unmodified. Functional usage example: import pandas as pd import janitor.timeseries df = pd.DataFrame(...) df = janitor.timeseries.fill_missing_timestamps( df=df, frequency=\"1H\", ) Method chaining example: import pandas as pd import janitor.timeseries df = ( pd.DataFrame(...) .fill_missing_timestamps(frequency=\"1H\") ) Parameters: Name Type Description Default df DataFrame DataFrame which needs to be tested for missing timestamps required frequency str sampling frequency of the data. Acceptable frequency strings are available here . Check offset aliases under time series in user guide required first_time_stamp Timestamp timestamp expected to start from; defaults to None . If no input is provided, assumes the minimum value in time_series . None last_time_stamp Timestamp timestamp expected to end with; defaults to None . If no input is provided, assumes the maximum value in time_series . None Returns: Type Description DataFrame DataFrame that has a complete set of contiguous datetimes. Source code in janitor/timeseries.py @pf.register_dataframe_method def fill_missing_timestamps( df: pd.DataFrame, frequency: str, first_time_stamp: pd.Timestamp = None, last_time_stamp: pd.Timestamp = None, ) -> pd.DataFrame: \"\"\" Fills a DataFrame with missing timestamps based on a defined frequency. If timestamps are missing, this function will re-index the DataFrame. If timestamps are not missing, then the function will return the DataFrame unmodified. Functional usage example: ```python import pandas as pd import janitor.timeseries df = pd.DataFrame(...) df = janitor.timeseries.fill_missing_timestamps( df=df, frequency=\"1H\", ) ``` Method chaining example: ```python import pandas as pd import janitor.timeseries df = ( pd.DataFrame(...) .fill_missing_timestamps(frequency=\"1H\") ) ``` :param df: DataFrame which needs to be tested for missing timestamps :param frequency: sampling frequency of the data. Acceptable frequency strings are available [here](https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases). Check offset aliases under time series in user guide :param first_time_stamp: timestamp expected to start from; defaults to `None`. If no input is provided, assumes the minimum value in `time_series`. :param last_time_stamp: timestamp expected to end with; defaults to `None`. If no input is provided, assumes the maximum value in `time_series`. :returns: DataFrame that has a complete set of contiguous datetimes. \"\"\" # Check all the inputs are the correct data type check(\"frequency\", frequency, [str]) check(\"first_time_stamp\", first_time_stamp, [pd.Timestamp, type(None)]) check(\"last_time_stamp\", last_time_stamp, [pd.Timestamp, type(None)]) if first_time_stamp is None: first_time_stamp = df.index.min() if last_time_stamp is None: last_time_stamp = df.index.max() # Generate expected timestamps expected_timestamps = pd.date_range( start=first_time_stamp, end=last_time_stamp, freq=frequency ) return df.reindex(expected_timestamps) flag_jumps(df, scale='percentage', direction='any', threshold=0.0, strict=False) Create boolean column(s) that flag whether or not the change between consecutive rows exceeds a provided threshold. Functional usage example: import pandas as pd import janitor.timeseries df = pd.DataFrame(...) df = flag_jumps( df=df, scale=\"absolute\", direction=\"any\", threshold=2, ) Method chaining example: import pandas as pd import janitor.timeseries df = ( pd.DatFrame(...) .flag_jumps( scale=\"absolute\", direction=\"any\", threshold=2, ) ) Detailed chaining examples: # Applies specified criteria across all columns of the DataFrame # Appends a flag column for each column in the DataFrame df = ( pd.DataFrame(...) .flag_jumps( scale=\"absolute\", direction=\"any\", threshold=2 ) ) # Applies specific criteria to certain DataFrame columns # Applies default criteria to columns not specifically listed # Appends a flag column for each column in the DataFrame df = ( pd.DataFrame(...) .flag_jumps( scale=dict(col1=\"absolute\", col2=\"percentage\"), direction=dict(col1=\"increasing\", col2=\"any\"), threshold=dict(col1=1, col2=0.5), ) ) # Applies specific criteria to certain DataFrame columns # Applies default criteria to columns not specifically listed # Appends a flag column for each column in the DataFrame df = ( pd.DataFrame(...) .flag_jumps( scale=dict(col1=\"absolute\"), direction=dict(col2=\"increasing\"), ) ) # Applies specific criteria to certain DataFrame columns # Applies default criteria to columns not specifically listed # Appends a flag column for only those columns found in # specified criteria df = ( pd.DataFrame(...) .flag_jumps( scale=dict(col1=\"absolute\"), threshold=dict(col2=1), strict=True, ) ) Parameters: Name Type Description Default df DataFrame DataFrame which needs to be flagged for changes between consecutive rows above a certain threshold. required scale Union[str, Dict[str, str]] Type of scaling approach to use. Acceptable arguments are 'absolute' (consider the difference between rows) and 'percentage' (consider the percentage change between rows); defaults to 'percentage' . 'percentage' direction Union[str, Dict[str, str]] Type of method used to handle the sign change when comparing consecutive rows. Acceptable arguments are 'increasing' (only consider rows that are increasing in value), 'decreasing' (only consider rows that are decreasing in value), and 'any' (consider rows that are either increasing or decreasing; sign is ignored); defaults to 'any' . 'any' threshold Union[int, float, Dict[str, Union[int, float]]] The value to check if consecutive row comparisons exceed. Always uses a greater than comparison. Must be >= 0.0 ; defaults to 0.0 . 0.0 strict bool flag to enable/disable appending of a flag column for each column in the provided DataFrame. If set to True , will only append a flag column for those columns found in at least one of the input dictionaries. If set to False , will appen a flag column for each column found in the provided DataFrame. If criteria is not specified, the defaults for each criteria is used; defaults to False . False Returns: Type Description DataFrame DataFrame that has flag jump columns. Exceptions: Type Description JanitorError if strict=True and at least one of scale , direction , or threshold inputs is not a dictionary. JanitorError if scale is not one of (\"absolute\", \"percentage\") . JanitorError if direction is not one of (\"increasing\", \"decreasing\", \"any\") . JanitorError if threshold is less than 0.0 . Source code in janitor/timeseries.py @pf.register_dataframe_method def flag_jumps( df: pd.DataFrame, scale: Union[str, Dict[str, str]] = \"percentage\", direction: Union[str, Dict[str, str]] = \"any\", threshold: Union[int, float, Dict[str, Union[int, float]]] = 0.0, strict: bool = False, ) -> pd.DataFrame: \"\"\" Create boolean column(s) that flag whether or not the change between consecutive rows exceeds a provided threshold. Functional usage example: ```python import pandas as pd import janitor.timeseries df = pd.DataFrame(...) df = flag_jumps( df=df, scale=\"absolute\", direction=\"any\", threshold=2, ) ``` Method chaining example: ```python import pandas as pd import janitor.timeseries df = ( pd.DatFrame(...) .flag_jumps( scale=\"absolute\", direction=\"any\", threshold=2, ) ) ``` Detailed chaining examples: ```python # Applies specified criteria across all columns of the DataFrame # Appends a flag column for each column in the DataFrame df = ( pd.DataFrame(...) .flag_jumps( scale=\"absolute\", direction=\"any\", threshold=2 ) ) # Applies specific criteria to certain DataFrame columns # Applies default criteria to columns not specifically listed # Appends a flag column for each column in the DataFrame df = ( pd.DataFrame(...) .flag_jumps( scale=dict(col1=\"absolute\", col2=\"percentage\"), direction=dict(col1=\"increasing\", col2=\"any\"), threshold=dict(col1=1, col2=0.5), ) ) # Applies specific criteria to certain DataFrame columns # Applies default criteria to columns not specifically listed # Appends a flag column for each column in the DataFrame df = ( pd.DataFrame(...) .flag_jumps( scale=dict(col1=\"absolute\"), direction=dict(col2=\"increasing\"), ) ) # Applies specific criteria to certain DataFrame columns # Applies default criteria to columns not specifically listed # Appends a flag column for only those columns found in # specified criteria df = ( pd.DataFrame(...) .flag_jumps( scale=dict(col1=\"absolute\"), threshold=dict(col2=1), strict=True, ) ) ``` :param df: DataFrame which needs to be flagged for changes between consecutive rows above a certain threshold. :param scale: Type of scaling approach to use. Acceptable arguments are `'absolute'` (consider the difference between rows) and `'percentage'` (consider the percentage change between rows); defaults to `'percentage'`. :param direction: Type of method used to handle the sign change when comparing consecutive rows. Acceptable arguments are `'increasing'` (only consider rows that are increasing in value), `'decreasing'` (only consider rows that are decreasing in value), and `'any'` (consider rows that are either increasing or decreasing; sign is ignored); defaults to `'any'`. :param threshold: The value to check if consecutive row comparisons exceed. Always uses a greater than comparison. Must be `>= 0.0`; defaults to `0.0`. :param strict: flag to enable/disable appending of a flag column for each column in the provided DataFrame. If set to `True`, will only append a flag column for those columns found in at least one of the input dictionaries. If set to `False`, will appen a flag column for each column found in the provided DataFrame. If criteria is not specified, the defaults for each criteria is used; defaults to `False`. :returns: DataFrame that has `flag jump` columns. :raises JanitorError: if `strict=True` and at least one of `scale`, `direction`, or `threshold` inputs is not a dictionary. :raises JanitorError: if `scale` is not one of `(\"absolute\", \"percentage\")`. :raises JanitorError: if `direction` is not one of `(\"increasing\", \"decreasing\", \"any\")`. :raises JanitorError: if `threshold` is less than `0.0`. \"\"\" df = df.copy() if strict: if ( any(isinstance(arg, dict) for arg in (scale, direction, threshold)) is False ): raise JanitorError( \"When enacting 'strict=True', 'scale', 'direction', or \" + \"'threshold' must be a dictionary.\" ) # Only append a flag col for the cols that appear # in at least one of the input dicts arg_keys = [ arg.keys() for arg in (scale, direction, threshold) if isinstance(arg, dict) ] cols = set(itertools.chain.from_iterable(arg_keys)) else: # Append a flag col for each col in the DataFrame cols = df.columns columns_to_add = {} for col in sorted(cols): # Allow arguments to be a mix of dict and single instances s = scale.get(col, \"percentage\") if isinstance(scale, dict) else scale d = ( direction.get(col, \"any\") if isinstance(direction, dict) else direction ) t = ( threshold.get(col, 0.0) if isinstance(threshold, dict) else threshold ) columns_to_add[f\"{col}_jump_flag\"] = _flag_jumps_single_col( df, col, scale=s, direction=d, threshold=t ) df = df.assign(**columns_to_add) return df sort_timestamps_monotonically(df, direction='increasing', strict=False) Sort DataFrame such that index is monotonic. If timestamps are monotonic, this function will return the DataFrame unmodified. If timestamps are not monotonic, then the function will sort the DataFrame. Functional usage example: import pandas as pd import janitor.timeseries df = pd.DataFrame(...) df = janitor.timeseries.sort_timestamps_monotonically( direction=\"increasing\" ) Method chaining example: import pandas as pd import janitor.timeseries df = ( pd.DataFrame(...) .sort_timestamps_monotonically(direction=\"increasing\") ) Parameters: Name Type Description Default df DataFrame DataFrame which needs to be tested for monotonicity. required direction str type of monotonicity desired. Acceptable arguments are 'increasing' or 'decreasing' . 'increasing' strict bool flag to enable/disable strict monotonicity. If set to True , will remove duplicates in the index by retaining first occurrence of value in index. If set to False , will not test for duplicates in the index; defaults to False . False Returns: Type Description DataFrame DataFrame that has monotonically increasing (or decreasing) timestamps. Source code in janitor/timeseries.py @pf.register_dataframe_method def sort_timestamps_monotonically( df: pd.DataFrame, direction: str = \"increasing\", strict: bool = False ) -> pd.DataFrame: \"\"\" Sort DataFrame such that index is monotonic. If timestamps are monotonic, this function will return the DataFrame unmodified. If timestamps are not monotonic, then the function will sort the DataFrame. Functional usage example: ```python import pandas as pd import janitor.timeseries df = pd.DataFrame(...) df = janitor.timeseries.sort_timestamps_monotonically( direction=\"increasing\" ) ``` Method chaining example: ```python import pandas as pd import janitor.timeseries df = ( pd.DataFrame(...) .sort_timestamps_monotonically(direction=\"increasing\") ) ``` :param df: DataFrame which needs to be tested for monotonicity. :param direction: type of monotonicity desired. Acceptable arguments are `'increasing'` or `'decreasing'`. :param strict: flag to enable/disable strict monotonicity. If set to `True`, will remove duplicates in the index by retaining first occurrence of value in index. If set to `False`, will not test for duplicates in the index; defaults to `False`. :returns: DataFrame that has monotonically increasing (or decreasing) timestamps. \"\"\" # Check all the inputs are the correct data type check(\"df\", df, [pd.DataFrame]) check(\"direction\", direction, [str]) check(\"strict\", strict, [bool]) # Remove duplicates if requested if strict: df = df[~df.index.duplicated(keep=\"first\")] # Sort timestamps if direction == \"increasing\": df = df.sort_index() else: df = df.sort_index(ascending=False) # Return the DataFrame return df","title":"Timeseries"},{"location":"api/timeseries/#timeseries","text":"Time series-specific data cleaning functions.","title":"Timeseries"},{"location":"api/timeseries/#janitor.timeseries.fill_missing_timestamps","text":"Fills a DataFrame with missing timestamps based on a defined frequency. If timestamps are missing, this function will re-index the DataFrame. If timestamps are not missing, then the function will return the DataFrame unmodified. Functional usage example: import pandas as pd import janitor.timeseries df = pd.DataFrame(...) df = janitor.timeseries.fill_missing_timestamps( df=df, frequency=\"1H\", ) Method chaining example: import pandas as pd import janitor.timeseries df = ( pd.DataFrame(...) .fill_missing_timestamps(frequency=\"1H\") ) Parameters: Name Type Description Default df DataFrame DataFrame which needs to be tested for missing timestamps required frequency str sampling frequency of the data. Acceptable frequency strings are available here . Check offset aliases under time series in user guide required first_time_stamp Timestamp timestamp expected to start from; defaults to None . If no input is provided, assumes the minimum value in time_series . None last_time_stamp Timestamp timestamp expected to end with; defaults to None . If no input is provided, assumes the maximum value in time_series . None Returns: Type Description DataFrame DataFrame that has a complete set of contiguous datetimes. Source code in janitor/timeseries.py @pf.register_dataframe_method def fill_missing_timestamps( df: pd.DataFrame, frequency: str, first_time_stamp: pd.Timestamp = None, last_time_stamp: pd.Timestamp = None, ) -> pd.DataFrame: \"\"\" Fills a DataFrame with missing timestamps based on a defined frequency. If timestamps are missing, this function will re-index the DataFrame. If timestamps are not missing, then the function will return the DataFrame unmodified. Functional usage example: ```python import pandas as pd import janitor.timeseries df = pd.DataFrame(...) df = janitor.timeseries.fill_missing_timestamps( df=df, frequency=\"1H\", ) ``` Method chaining example: ```python import pandas as pd import janitor.timeseries df = ( pd.DataFrame(...) .fill_missing_timestamps(frequency=\"1H\") ) ``` :param df: DataFrame which needs to be tested for missing timestamps :param frequency: sampling frequency of the data. Acceptable frequency strings are available [here](https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases). Check offset aliases under time series in user guide :param first_time_stamp: timestamp expected to start from; defaults to `None`. If no input is provided, assumes the minimum value in `time_series`. :param last_time_stamp: timestamp expected to end with; defaults to `None`. If no input is provided, assumes the maximum value in `time_series`. :returns: DataFrame that has a complete set of contiguous datetimes. \"\"\" # Check all the inputs are the correct data type check(\"frequency\", frequency, [str]) check(\"first_time_stamp\", first_time_stamp, [pd.Timestamp, type(None)]) check(\"last_time_stamp\", last_time_stamp, [pd.Timestamp, type(None)]) if first_time_stamp is None: first_time_stamp = df.index.min() if last_time_stamp is None: last_time_stamp = df.index.max() # Generate expected timestamps expected_timestamps = pd.date_range( start=first_time_stamp, end=last_time_stamp, freq=frequency ) return df.reindex(expected_timestamps)","title":"fill_missing_timestamps()"},{"location":"api/timeseries/#janitor.timeseries.flag_jumps","text":"Create boolean column(s) that flag whether or not the change between consecutive rows exceeds a provided threshold. Functional usage example: import pandas as pd import janitor.timeseries df = pd.DataFrame(...) df = flag_jumps( df=df, scale=\"absolute\", direction=\"any\", threshold=2, ) Method chaining example: import pandas as pd import janitor.timeseries df = ( pd.DatFrame(...) .flag_jumps( scale=\"absolute\", direction=\"any\", threshold=2, ) ) Detailed chaining examples: # Applies specified criteria across all columns of the DataFrame # Appends a flag column for each column in the DataFrame df = ( pd.DataFrame(...) .flag_jumps( scale=\"absolute\", direction=\"any\", threshold=2 ) ) # Applies specific criteria to certain DataFrame columns # Applies default criteria to columns not specifically listed # Appends a flag column for each column in the DataFrame df = ( pd.DataFrame(...) .flag_jumps( scale=dict(col1=\"absolute\", col2=\"percentage\"), direction=dict(col1=\"increasing\", col2=\"any\"), threshold=dict(col1=1, col2=0.5), ) ) # Applies specific criteria to certain DataFrame columns # Applies default criteria to columns not specifically listed # Appends a flag column for each column in the DataFrame df = ( pd.DataFrame(...) .flag_jumps( scale=dict(col1=\"absolute\"), direction=dict(col2=\"increasing\"), ) ) # Applies specific criteria to certain DataFrame columns # Applies default criteria to columns not specifically listed # Appends a flag column for only those columns found in # specified criteria df = ( pd.DataFrame(...) .flag_jumps( scale=dict(col1=\"absolute\"), threshold=dict(col2=1), strict=True, ) ) Parameters: Name Type Description Default df DataFrame DataFrame which needs to be flagged for changes between consecutive rows above a certain threshold. required scale Union[str, Dict[str, str]] Type of scaling approach to use. Acceptable arguments are 'absolute' (consider the difference between rows) and 'percentage' (consider the percentage change between rows); defaults to 'percentage' . 'percentage' direction Union[str, Dict[str, str]] Type of method used to handle the sign change when comparing consecutive rows. Acceptable arguments are 'increasing' (only consider rows that are increasing in value), 'decreasing' (only consider rows that are decreasing in value), and 'any' (consider rows that are either increasing or decreasing; sign is ignored); defaults to 'any' . 'any' threshold Union[int, float, Dict[str, Union[int, float]]] The value to check if consecutive row comparisons exceed. Always uses a greater than comparison. Must be >= 0.0 ; defaults to 0.0 . 0.0 strict bool flag to enable/disable appending of a flag column for each column in the provided DataFrame. If set to True , will only append a flag column for those columns found in at least one of the input dictionaries. If set to False , will appen a flag column for each column found in the provided DataFrame. If criteria is not specified, the defaults for each criteria is used; defaults to False . False Returns: Type Description DataFrame DataFrame that has flag jump columns. Exceptions: Type Description JanitorError if strict=True and at least one of scale , direction , or threshold inputs is not a dictionary. JanitorError if scale is not one of (\"absolute\", \"percentage\") . JanitorError if direction is not one of (\"increasing\", \"decreasing\", \"any\") . JanitorError if threshold is less than 0.0 . Source code in janitor/timeseries.py @pf.register_dataframe_method def flag_jumps( df: pd.DataFrame, scale: Union[str, Dict[str, str]] = \"percentage\", direction: Union[str, Dict[str, str]] = \"any\", threshold: Union[int, float, Dict[str, Union[int, float]]] = 0.0, strict: bool = False, ) -> pd.DataFrame: \"\"\" Create boolean column(s) that flag whether or not the change between consecutive rows exceeds a provided threshold. Functional usage example: ```python import pandas as pd import janitor.timeseries df = pd.DataFrame(...) df = flag_jumps( df=df, scale=\"absolute\", direction=\"any\", threshold=2, ) ``` Method chaining example: ```python import pandas as pd import janitor.timeseries df = ( pd.DatFrame(...) .flag_jumps( scale=\"absolute\", direction=\"any\", threshold=2, ) ) ``` Detailed chaining examples: ```python # Applies specified criteria across all columns of the DataFrame # Appends a flag column for each column in the DataFrame df = ( pd.DataFrame(...) .flag_jumps( scale=\"absolute\", direction=\"any\", threshold=2 ) ) # Applies specific criteria to certain DataFrame columns # Applies default criteria to columns not specifically listed # Appends a flag column for each column in the DataFrame df = ( pd.DataFrame(...) .flag_jumps( scale=dict(col1=\"absolute\", col2=\"percentage\"), direction=dict(col1=\"increasing\", col2=\"any\"), threshold=dict(col1=1, col2=0.5), ) ) # Applies specific criteria to certain DataFrame columns # Applies default criteria to columns not specifically listed # Appends a flag column for each column in the DataFrame df = ( pd.DataFrame(...) .flag_jumps( scale=dict(col1=\"absolute\"), direction=dict(col2=\"increasing\"), ) ) # Applies specific criteria to certain DataFrame columns # Applies default criteria to columns not specifically listed # Appends a flag column for only those columns found in # specified criteria df = ( pd.DataFrame(...) .flag_jumps( scale=dict(col1=\"absolute\"), threshold=dict(col2=1), strict=True, ) ) ``` :param df: DataFrame which needs to be flagged for changes between consecutive rows above a certain threshold. :param scale: Type of scaling approach to use. Acceptable arguments are `'absolute'` (consider the difference between rows) and `'percentage'` (consider the percentage change between rows); defaults to `'percentage'`. :param direction: Type of method used to handle the sign change when comparing consecutive rows. Acceptable arguments are `'increasing'` (only consider rows that are increasing in value), `'decreasing'` (only consider rows that are decreasing in value), and `'any'` (consider rows that are either increasing or decreasing; sign is ignored); defaults to `'any'`. :param threshold: The value to check if consecutive row comparisons exceed. Always uses a greater than comparison. Must be `>= 0.0`; defaults to `0.0`. :param strict: flag to enable/disable appending of a flag column for each column in the provided DataFrame. If set to `True`, will only append a flag column for those columns found in at least one of the input dictionaries. If set to `False`, will appen a flag column for each column found in the provided DataFrame. If criteria is not specified, the defaults for each criteria is used; defaults to `False`. :returns: DataFrame that has `flag jump` columns. :raises JanitorError: if `strict=True` and at least one of `scale`, `direction`, or `threshold` inputs is not a dictionary. :raises JanitorError: if `scale` is not one of `(\"absolute\", \"percentage\")`. :raises JanitorError: if `direction` is not one of `(\"increasing\", \"decreasing\", \"any\")`. :raises JanitorError: if `threshold` is less than `0.0`. \"\"\" df = df.copy() if strict: if ( any(isinstance(arg, dict) for arg in (scale, direction, threshold)) is False ): raise JanitorError( \"When enacting 'strict=True', 'scale', 'direction', or \" + \"'threshold' must be a dictionary.\" ) # Only append a flag col for the cols that appear # in at least one of the input dicts arg_keys = [ arg.keys() for arg in (scale, direction, threshold) if isinstance(arg, dict) ] cols = set(itertools.chain.from_iterable(arg_keys)) else: # Append a flag col for each col in the DataFrame cols = df.columns columns_to_add = {} for col in sorted(cols): # Allow arguments to be a mix of dict and single instances s = scale.get(col, \"percentage\") if isinstance(scale, dict) else scale d = ( direction.get(col, \"any\") if isinstance(direction, dict) else direction ) t = ( threshold.get(col, 0.0) if isinstance(threshold, dict) else threshold ) columns_to_add[f\"{col}_jump_flag\"] = _flag_jumps_single_col( df, col, scale=s, direction=d, threshold=t ) df = df.assign(**columns_to_add) return df","title":"flag_jumps()"},{"location":"api/timeseries/#janitor.timeseries.sort_timestamps_monotonically","text":"Sort DataFrame such that index is monotonic. If timestamps are monotonic, this function will return the DataFrame unmodified. If timestamps are not monotonic, then the function will sort the DataFrame. Functional usage example: import pandas as pd import janitor.timeseries df = pd.DataFrame(...) df = janitor.timeseries.sort_timestamps_monotonically( direction=\"increasing\" ) Method chaining example: import pandas as pd import janitor.timeseries df = ( pd.DataFrame(...) .sort_timestamps_monotonically(direction=\"increasing\") ) Parameters: Name Type Description Default df DataFrame DataFrame which needs to be tested for monotonicity. required direction str type of monotonicity desired. Acceptable arguments are 'increasing' or 'decreasing' . 'increasing' strict bool flag to enable/disable strict monotonicity. If set to True , will remove duplicates in the index by retaining first occurrence of value in index. If set to False , will not test for duplicates in the index; defaults to False . False Returns: Type Description DataFrame DataFrame that has monotonically increasing (or decreasing) timestamps. Source code in janitor/timeseries.py @pf.register_dataframe_method def sort_timestamps_monotonically( df: pd.DataFrame, direction: str = \"increasing\", strict: bool = False ) -> pd.DataFrame: \"\"\" Sort DataFrame such that index is monotonic. If timestamps are monotonic, this function will return the DataFrame unmodified. If timestamps are not monotonic, then the function will sort the DataFrame. Functional usage example: ```python import pandas as pd import janitor.timeseries df = pd.DataFrame(...) df = janitor.timeseries.sort_timestamps_monotonically( direction=\"increasing\" ) ``` Method chaining example: ```python import pandas as pd import janitor.timeseries df = ( pd.DataFrame(...) .sort_timestamps_monotonically(direction=\"increasing\") ) ``` :param df: DataFrame which needs to be tested for monotonicity. :param direction: type of monotonicity desired. Acceptable arguments are `'increasing'` or `'decreasing'`. :param strict: flag to enable/disable strict monotonicity. If set to `True`, will remove duplicates in the index by retaining first occurrence of value in index. If set to `False`, will not test for duplicates in the index; defaults to `False`. :returns: DataFrame that has monotonically increasing (or decreasing) timestamps. \"\"\" # Check all the inputs are the correct data type check(\"df\", df, [pd.DataFrame]) check(\"direction\", direction, [str]) check(\"strict\", strict, [bool]) # Remove duplicates if requested if strict: df = df[~df.index.duplicated(keep=\"first\")] # Sort timestamps if direction == \"increasing\": df = df.sort_index() else: df = df.sort_index(ascending=False) # Return the DataFrame return df","title":"sort_timestamps_monotonically()"},{"location":"api/utils/","text":"Utils Miscellaneous internal PyJanitor helper functions. check(varname, value, expected_types) One-liner syntactic sugar for checking types. It can also check callables. Example usage: check('x', x, [int, float]) Parameters: Name Type Description Default varname str The name of the variable (for diagnostic error message). required value The value of the varname . required expected_types list The type(s) the item is expected to be. required Exceptions: Type Description TypeError if data is not the expected type. Source code in janitor/utils.py def check(varname: str, value, expected_types: list): \"\"\" One-liner syntactic sugar for checking types. It can also check callables. Example usage: ```python check('x', x, [int, float]) ``` :param varname: The name of the variable (for diagnostic error message). :param value: The value of the `varname`. :param expected_types: The type(s) the item is expected to be. :raises TypeError: if data is not the expected type. \"\"\" is_expected_type: bool = False for t in expected_types: if t is callable: is_expected_type = t(value) else: is_expected_type = isinstance(value, t) if is_expected_type: break if not is_expected_type: raise TypeError( \"{varname} should be one of {expected_types}\".format( varname=varname, expected_types=expected_types ) ) check_column(df, column_names, present=True) One-liner syntactic sugar for checking the presence or absence of columns. Example usage: check(df, ['a', 'b'], present=True) This will check whether columns 'a' and 'b' are present in df 's columns. One can also guarantee that 'a' and 'b' are not present by switching to present=False . Parameters: Name Type Description Default df DataFrame The name of the variable. required column_names Union[Iterable, str] A list of column names we want to check to see if present (or absent) in df . required present bool If True (default), checks to see if all of column_names are in df.columns . If False , checks that none of column_names are in df.columns . True Exceptions: Type Description ValueError if data is not the expected type. Source code in janitor/utils.py def check_column( df: pd.DataFrame, column_names: Union[Iterable, str], present: bool = True ): \"\"\" One-liner syntactic sugar for checking the presence or absence of columns. Example usage: ```python check(df, ['a', 'b'], present=True) ``` This will check whether columns `'a'` and `'b'` are present in `df`'s columns. One can also guarantee that `'a'` and `'b'` are not present by switching to `present=False`. :param df: The name of the variable. :param column_names: A list of column names we want to check to see if present (or absent) in `df`. :param present: If `True` (default), checks to see if all of `column_names` are in `df.columns`. If `False`, checks that none of `column_names` are in `df.columns`. :raises ValueError: if data is not the expected type. \"\"\" if isinstance(column_names, str) or not isinstance(column_names, Iterable): column_names = [column_names] for column_name in column_names: if present and column_name not in df.columns: # skipcq: PYL-R1720 raise ValueError( f\"{column_name} not present in dataframe columns!\" ) elif not present and column_name in df.columns: raise ValueError( f\"{column_name} already present in dataframe columns!\" ) deprecated_alias(**aliases) Used as a decorator when deprecating old function argument names, while keeping backwards compatibility. Implementation is inspired from StackOverflow . Functional usage example: @deprecated_alias(a='alpha', b='beta') def simple_sum(alpha, beta): return alpha + beta Parameters: Name Type Description Default aliases Dictionary of aliases for a function's arguments. {} Returns: Type Description Callable Your original function wrapped with the kwarg redirection function. Source code in janitor/utils.py def deprecated_alias(**aliases) -> Callable: \"\"\" Used as a decorator when deprecating old function argument names, while keeping backwards compatibility. Implementation is inspired from [`StackOverflow`][stack_link]. [stack_link]: https://stackoverflow.com/questions/49802412/how-to-implement-deprecation-in-python-with-argument-alias Functional usage example: ```python @deprecated_alias(a='alpha', b='beta') def simple_sum(alpha, beta): return alpha + beta ``` :param aliases: Dictionary of aliases for a function's arguments. :return: Your original function wrapped with the `kwarg` redirection function. \"\"\" # noqa: E501 def decorator(func): @functools.wraps(func) def wrapper(*args, **kwargs): rename_kwargs(func.__name__, kwargs, aliases) return func(*args, **kwargs) return wrapper return decorator idempotent(func, df, *args, **kwargs) Raises an error if a function operating on a DataFrame is not idempotent. That is, func(func(df)) = func(df) is not True for all df . Parameters: Name Type Description Default func Callable A Python method. required df DataFrame A pandas DataFrame . required args Positional arguments supplied to the method. () kwargs Keyword arguments supplied to the method. {} Exceptions: Type Description ValueError If func is found to not be idempotent for the given DataFrame ( df ). Source code in janitor/utils.py def idempotent(func: Callable, df: pd.DataFrame, *args, **kwargs): \"\"\" Raises an error if a function operating on a DataFrame is not idempotent. That is, `func(func(df)) = func(df)` is not `True` for all `df`. :param func: A Python method. :param df: A pandas `DataFrame`. :param args: Positional arguments supplied to the method. :param kwargs: Keyword arguments supplied to the method. :raises ValueError: If `func` is found to not be idempotent for the given DataFrame (`df`). \"\"\" if not func(df, *args, **kwargs) == func( func(df, *args, **kwargs), *args, **kwargs ): raise ValueError( \"Supplied function is not idempotent for the given DataFrame.\" ) import_message(submodule, package, conda_channel=None, pip_install=False) Return warning if package is not found. Generic message for indicating to the user when a function relies on an optional module / package that is not currently installed. Includes installation instructions. Used in chemistry.py and biology.py . Parameters: Name Type Description Default submodule str pyjanitor submodule that needs an external dependency. required package str External package this submodule relies on. required conda_channel str conda channel package can be installed from, if at all. None pip_install bool Whether package can be installed via pip . False Source code in janitor/utils.py def import_message( submodule: str, package: str, conda_channel: str = None, pip_install: bool = False, ): \"\"\" Return warning if package is not found. Generic message for indicating to the user when a function relies on an optional module / package that is not currently installed. Includes installation instructions. Used in `chemistry.py` and `biology.py`. :param submodule: `pyjanitor` submodule that needs an external dependency. :param package: External package this submodule relies on. :param conda_channel: `conda` channel package can be installed from, if at all. :param pip_install: Whether package can be installed via `pip`. \"\"\" is_conda = os.path.exists(os.path.join(sys.prefix, \"conda-meta\")) installable = True if is_conda: if conda_channel is None: installable = False installation = f\"{package} cannot be installed via conda\" else: installation = f\"conda install -c {conda_channel} {package}\" else: if pip_install: installation = f\"pip install {package}\" else: installable = False installation = f\"{package} cannot be installed via pip\" print( f\"To use the janitor submodule {submodule}, you need to install \" f\"{package}.\" ) print() if installable: print(\"To do so, use the following command:\") print() print(f\" {installation}\") else: print(f\"{installation}\") is_connected(url) This is a helper function to check if the client is connected to the internet. Example: print(is_connected(\"www.google.com\")) console >> True Parameters: Name Type Description Default url str We take a test url to check if we are able to create a valid connection. required Returns: Type Description bool We return a boolean that signifies our connection to the internet Exceptions: Type Description OSError if connection to URL cannot be established Source code in janitor/utils.py def is_connected(url: str) -> bool: \"\"\" This is a helper function to check if the client is connected to the internet. Example: print(is_connected(\"www.google.com\")) console >> True :param url: We take a test url to check if we are able to create a valid connection. :raises OSError: if connection to `URL` cannot be established :return: We return a boolean that signifies our connection to the internet \"\"\" try: sock = socket.create_connection((url, 80)) if sock is not None: sock.close() return True except OSError as e: warnings.warn( \"There was an issue connecting to the internet. \" \"Please see original error below.\" ) raise e return False refactored_function(message) Used as a decorator when refactoring functions. Implementation is inspired from Hacker Noon . Functional usage example: @refactored_function( message=\"simple_sum() has been refactored. Use hard_sum() instead.\" ) def simple_sum(alpha, beta): return alpha + beta Parameters: Name Type Description Default message str Message to use in warning user about refactoring. required Returns: Type Description Callable Your original function wrapped with the kwarg redirection function. Source code in janitor/utils.py def refactored_function(message: str) -> Callable: \"\"\" Used as a decorator when refactoring functions. Implementation is inspired from [`Hacker Noon`][hacker_link]. [hacker_link]: https://hackernoon.com/why-refactoring-how-to-restructure-python-package-51b89aa91987 Functional usage example: ```python @refactored_function( message=\"simple_sum() has been refactored. Use hard_sum() instead.\" ) def simple_sum(alpha, beta): return alpha + beta ``` :param message: Message to use in warning user about refactoring. :return: Your original function wrapped with the kwarg redirection function. \"\"\" # noqa: E501 def decorator(func): def emit_warning(*args, **kwargs): warnings.warn(message, FutureWarning) return func(*args, **kwargs) return emit_warning return decorator rename_kwargs(func_name, kwargs, aliases) Used to update deprecated argument names with new names. Throws a TypeError if both arguments are provided, and warns if old alias is used. Nothing is returned as the passed kwargs are modified directly. Implementation is inspired from StackOverflow . Parameters: Name Type Description Default func_name str name of decorated function. required kwargs Dict Arguments supplied to the method. required aliases Dict Dictionary of aliases for a function's arguments. required Exceptions: Type Description TypeError if both arguments are provided. Source code in janitor/utils.py def rename_kwargs(func_name: str, kwargs: Dict, aliases: Dict): \"\"\" Used to update deprecated argument names with new names. Throws a `TypeError` if both arguments are provided, and warns if old alias is used. Nothing is returned as the passed `kwargs` are modified directly. Implementation is inspired from [`StackOverflow`][stack_link]. [stack_link]: https://stackoverflow.com/questions/49802412/how-to-implement-deprecation-in-python-with-argument-alias :param func_name: name of decorated function. :param kwargs: Arguments supplied to the method. :param aliases: Dictionary of aliases for a function's arguments. :raises TypeError: if both arguments are provided. \"\"\" # noqa: E501 for old_alias, new_alias in aliases.items(): if old_alias in kwargs: if new_alias in kwargs: raise TypeError( f\"{func_name} received both {old_alias} and {new_alias}\" ) warnings.warn( f\"{old_alias} is deprecated; use {new_alias}\", DeprecationWarning, ) kwargs[new_alias] = kwargs.pop(old_alias) skiperror(f, return_x=False, return_val=nan) Decorator for escaping any error in a function. Example usage: df[column].apply( skiperror(transform, return_val=3, return_x=False)) # Can also be used as shown below @skiperror(return_val=3, return_x=False) def transform(x): pass Parameters: Name Type Description Default f Callable the function to be wrapped. required return_x bool whether or not the original value that caused error should be returned. False return_val the value to be returned when an error hits. Ignored if return_x is True . nan Returns: Type Description Callable the wrapped function. Source code in janitor/utils.py def skiperror( f: Callable, return_x: bool = False, return_val=np.nan ) -> Callable: \"\"\" Decorator for escaping any error in a function. Example usage: ```python df[column].apply( skiperror(transform, return_val=3, return_x=False)) # Can also be used as shown below @skiperror(return_val=3, return_x=False) def transform(x): pass ``` :param f: the function to be wrapped. :param return_x: whether or not the original value that caused error should be returned. :param return_val: the value to be returned when an error hits. Ignored if `return_x` is `True`. :returns: the wrapped function. \"\"\" def _wrapped(x, *args, **kwargs): try: return f(x, *args, **kwargs) except Exception: # skipcq: PYL-W0703 if return_x: return x return return_val return _wrapped skipna(f) Decorator for escaping np.nan and None in a function. Example usage: df[column].apply(skipna(transform)) # Can also be used as shown below @skipna def transform(x): pass Parameters: Name Type Description Default f Callable the function to be wrapped. required Returns: Type Description Callable the wrapped function. Source code in janitor/utils.py def skipna(f: Callable) -> Callable: \"\"\" Decorator for escaping `np.nan` and `None` in a function. Example usage: ```python df[column].apply(skipna(transform)) # Can also be used as shown below @skipna def transform(x): pass ``` :param f: the function to be wrapped. :returns: the wrapped function. \"\"\" def _wrapped(x, *args, **kwargs): if (type(x) is float and np.isnan(x)) or x is None: return np.nan return f(x, *args, **kwargs) return _wrapped","title":"Utils"},{"location":"api/utils/#utils","text":"Miscellaneous internal PyJanitor helper functions.","title":"Utils"},{"location":"api/utils/#janitor.utils.check","text":"One-liner syntactic sugar for checking types. It can also check callables. Example usage: check('x', x, [int, float]) Parameters: Name Type Description Default varname str The name of the variable (for diagnostic error message). required value The value of the varname . required expected_types list The type(s) the item is expected to be. required Exceptions: Type Description TypeError if data is not the expected type. Source code in janitor/utils.py def check(varname: str, value, expected_types: list): \"\"\" One-liner syntactic sugar for checking types. It can also check callables. Example usage: ```python check('x', x, [int, float]) ``` :param varname: The name of the variable (for diagnostic error message). :param value: The value of the `varname`. :param expected_types: The type(s) the item is expected to be. :raises TypeError: if data is not the expected type. \"\"\" is_expected_type: bool = False for t in expected_types: if t is callable: is_expected_type = t(value) else: is_expected_type = isinstance(value, t) if is_expected_type: break if not is_expected_type: raise TypeError( \"{varname} should be one of {expected_types}\".format( varname=varname, expected_types=expected_types ) )","title":"check()"},{"location":"api/utils/#janitor.utils.check_column","text":"One-liner syntactic sugar for checking the presence or absence of columns. Example usage: check(df, ['a', 'b'], present=True) This will check whether columns 'a' and 'b' are present in df 's columns. One can also guarantee that 'a' and 'b' are not present by switching to present=False . Parameters: Name Type Description Default df DataFrame The name of the variable. required column_names Union[Iterable, str] A list of column names we want to check to see if present (or absent) in df . required present bool If True (default), checks to see if all of column_names are in df.columns . If False , checks that none of column_names are in df.columns . True Exceptions: Type Description ValueError if data is not the expected type. Source code in janitor/utils.py def check_column( df: pd.DataFrame, column_names: Union[Iterable, str], present: bool = True ): \"\"\" One-liner syntactic sugar for checking the presence or absence of columns. Example usage: ```python check(df, ['a', 'b'], present=True) ``` This will check whether columns `'a'` and `'b'` are present in `df`'s columns. One can also guarantee that `'a'` and `'b'` are not present by switching to `present=False`. :param df: The name of the variable. :param column_names: A list of column names we want to check to see if present (or absent) in `df`. :param present: If `True` (default), checks to see if all of `column_names` are in `df.columns`. If `False`, checks that none of `column_names` are in `df.columns`. :raises ValueError: if data is not the expected type. \"\"\" if isinstance(column_names, str) or not isinstance(column_names, Iterable): column_names = [column_names] for column_name in column_names: if present and column_name not in df.columns: # skipcq: PYL-R1720 raise ValueError( f\"{column_name} not present in dataframe columns!\" ) elif not present and column_name in df.columns: raise ValueError( f\"{column_name} already present in dataframe columns!\" )","title":"check_column()"},{"location":"api/utils/#janitor.utils.deprecated_alias","text":"Used as a decorator when deprecating old function argument names, while keeping backwards compatibility. Implementation is inspired from StackOverflow . Functional usage example: @deprecated_alias(a='alpha', b='beta') def simple_sum(alpha, beta): return alpha + beta Parameters: Name Type Description Default aliases Dictionary of aliases for a function's arguments. {} Returns: Type Description Callable Your original function wrapped with the kwarg redirection function. Source code in janitor/utils.py def deprecated_alias(**aliases) -> Callable: \"\"\" Used as a decorator when deprecating old function argument names, while keeping backwards compatibility. Implementation is inspired from [`StackOverflow`][stack_link]. [stack_link]: https://stackoverflow.com/questions/49802412/how-to-implement-deprecation-in-python-with-argument-alias Functional usage example: ```python @deprecated_alias(a='alpha', b='beta') def simple_sum(alpha, beta): return alpha + beta ``` :param aliases: Dictionary of aliases for a function's arguments. :return: Your original function wrapped with the `kwarg` redirection function. \"\"\" # noqa: E501 def decorator(func): @functools.wraps(func) def wrapper(*args, **kwargs): rename_kwargs(func.__name__, kwargs, aliases) return func(*args, **kwargs) return wrapper return decorator","title":"deprecated_alias()"},{"location":"api/utils/#janitor.utils.idempotent","text":"Raises an error if a function operating on a DataFrame is not idempotent. That is, func(func(df)) = func(df) is not True for all df . Parameters: Name Type Description Default func Callable A Python method. required df DataFrame A pandas DataFrame . required args Positional arguments supplied to the method. () kwargs Keyword arguments supplied to the method. {} Exceptions: Type Description ValueError If func is found to not be idempotent for the given DataFrame ( df ). Source code in janitor/utils.py def idempotent(func: Callable, df: pd.DataFrame, *args, **kwargs): \"\"\" Raises an error if a function operating on a DataFrame is not idempotent. That is, `func(func(df)) = func(df)` is not `True` for all `df`. :param func: A Python method. :param df: A pandas `DataFrame`. :param args: Positional arguments supplied to the method. :param kwargs: Keyword arguments supplied to the method. :raises ValueError: If `func` is found to not be idempotent for the given DataFrame (`df`). \"\"\" if not func(df, *args, **kwargs) == func( func(df, *args, **kwargs), *args, **kwargs ): raise ValueError( \"Supplied function is not idempotent for the given DataFrame.\" )","title":"idempotent()"},{"location":"api/utils/#janitor.utils.import_message","text":"Return warning if package is not found. Generic message for indicating to the user when a function relies on an optional module / package that is not currently installed. Includes installation instructions. Used in chemistry.py and biology.py . Parameters: Name Type Description Default submodule str pyjanitor submodule that needs an external dependency. required package str External package this submodule relies on. required conda_channel str conda channel package can be installed from, if at all. None pip_install bool Whether package can be installed via pip . False Source code in janitor/utils.py def import_message( submodule: str, package: str, conda_channel: str = None, pip_install: bool = False, ): \"\"\" Return warning if package is not found. Generic message for indicating to the user when a function relies on an optional module / package that is not currently installed. Includes installation instructions. Used in `chemistry.py` and `biology.py`. :param submodule: `pyjanitor` submodule that needs an external dependency. :param package: External package this submodule relies on. :param conda_channel: `conda` channel package can be installed from, if at all. :param pip_install: Whether package can be installed via `pip`. \"\"\" is_conda = os.path.exists(os.path.join(sys.prefix, \"conda-meta\")) installable = True if is_conda: if conda_channel is None: installable = False installation = f\"{package} cannot be installed via conda\" else: installation = f\"conda install -c {conda_channel} {package}\" else: if pip_install: installation = f\"pip install {package}\" else: installable = False installation = f\"{package} cannot be installed via pip\" print( f\"To use the janitor submodule {submodule}, you need to install \" f\"{package}.\" ) print() if installable: print(\"To do so, use the following command:\") print() print(f\" {installation}\") else: print(f\"{installation}\")","title":"import_message()"},{"location":"api/utils/#janitor.utils.is_connected","text":"This is a helper function to check if the client is connected to the internet. Example: print(is_connected(\"www.google.com\")) console >> True Parameters: Name Type Description Default url str We take a test url to check if we are able to create a valid connection. required Returns: Type Description bool We return a boolean that signifies our connection to the internet Exceptions: Type Description OSError if connection to URL cannot be established Source code in janitor/utils.py def is_connected(url: str) -> bool: \"\"\" This is a helper function to check if the client is connected to the internet. Example: print(is_connected(\"www.google.com\")) console >> True :param url: We take a test url to check if we are able to create a valid connection. :raises OSError: if connection to `URL` cannot be established :return: We return a boolean that signifies our connection to the internet \"\"\" try: sock = socket.create_connection((url, 80)) if sock is not None: sock.close() return True except OSError as e: warnings.warn( \"There was an issue connecting to the internet. \" \"Please see original error below.\" ) raise e return False","title":"is_connected()"},{"location":"api/utils/#janitor.utils.refactored_function","text":"Used as a decorator when refactoring functions. Implementation is inspired from Hacker Noon . Functional usage example: @refactored_function( message=\"simple_sum() has been refactored. Use hard_sum() instead.\" ) def simple_sum(alpha, beta): return alpha + beta Parameters: Name Type Description Default message str Message to use in warning user about refactoring. required Returns: Type Description Callable Your original function wrapped with the kwarg redirection function. Source code in janitor/utils.py def refactored_function(message: str) -> Callable: \"\"\" Used as a decorator when refactoring functions. Implementation is inspired from [`Hacker Noon`][hacker_link]. [hacker_link]: https://hackernoon.com/why-refactoring-how-to-restructure-python-package-51b89aa91987 Functional usage example: ```python @refactored_function( message=\"simple_sum() has been refactored. Use hard_sum() instead.\" ) def simple_sum(alpha, beta): return alpha + beta ``` :param message: Message to use in warning user about refactoring. :return: Your original function wrapped with the kwarg redirection function. \"\"\" # noqa: E501 def decorator(func): def emit_warning(*args, **kwargs): warnings.warn(message, FutureWarning) return func(*args, **kwargs) return emit_warning return decorator","title":"refactored_function()"},{"location":"api/utils/#janitor.utils.rename_kwargs","text":"Used to update deprecated argument names with new names. Throws a TypeError if both arguments are provided, and warns if old alias is used. Nothing is returned as the passed kwargs are modified directly. Implementation is inspired from StackOverflow . Parameters: Name Type Description Default func_name str name of decorated function. required kwargs Dict Arguments supplied to the method. required aliases Dict Dictionary of aliases for a function's arguments. required Exceptions: Type Description TypeError if both arguments are provided. Source code in janitor/utils.py def rename_kwargs(func_name: str, kwargs: Dict, aliases: Dict): \"\"\" Used to update deprecated argument names with new names. Throws a `TypeError` if both arguments are provided, and warns if old alias is used. Nothing is returned as the passed `kwargs` are modified directly. Implementation is inspired from [`StackOverflow`][stack_link]. [stack_link]: https://stackoverflow.com/questions/49802412/how-to-implement-deprecation-in-python-with-argument-alias :param func_name: name of decorated function. :param kwargs: Arguments supplied to the method. :param aliases: Dictionary of aliases for a function's arguments. :raises TypeError: if both arguments are provided. \"\"\" # noqa: E501 for old_alias, new_alias in aliases.items(): if old_alias in kwargs: if new_alias in kwargs: raise TypeError( f\"{func_name} received both {old_alias} and {new_alias}\" ) warnings.warn( f\"{old_alias} is deprecated; use {new_alias}\", DeprecationWarning, ) kwargs[new_alias] = kwargs.pop(old_alias)","title":"rename_kwargs()"},{"location":"api/utils/#janitor.utils.skiperror","text":"Decorator for escaping any error in a function. Example usage: df[column].apply( skiperror(transform, return_val=3, return_x=False)) # Can also be used as shown below @skiperror(return_val=3, return_x=False) def transform(x): pass Parameters: Name Type Description Default f Callable the function to be wrapped. required return_x bool whether or not the original value that caused error should be returned. False return_val the value to be returned when an error hits. Ignored if return_x is True . nan Returns: Type Description Callable the wrapped function. Source code in janitor/utils.py def skiperror( f: Callable, return_x: bool = False, return_val=np.nan ) -> Callable: \"\"\" Decorator for escaping any error in a function. Example usage: ```python df[column].apply( skiperror(transform, return_val=3, return_x=False)) # Can also be used as shown below @skiperror(return_val=3, return_x=False) def transform(x): pass ``` :param f: the function to be wrapped. :param return_x: whether or not the original value that caused error should be returned. :param return_val: the value to be returned when an error hits. Ignored if `return_x` is `True`. :returns: the wrapped function. \"\"\" def _wrapped(x, *args, **kwargs): try: return f(x, *args, **kwargs) except Exception: # skipcq: PYL-W0703 if return_x: return x return return_val return _wrapped","title":"skiperror()"},{"location":"api/utils/#janitor.utils.skipna","text":"Decorator for escaping np.nan and None in a function. Example usage: df[column].apply(skipna(transform)) # Can also be used as shown below @skipna def transform(x): pass Parameters: Name Type Description Default f Callable the function to be wrapped. required Returns: Type Description Callable the wrapped function. Source code in janitor/utils.py def skipna(f: Callable) -> Callable: \"\"\" Decorator for escaping `np.nan` and `None` in a function. Example usage: ```python df[column].apply(skipna(transform)) # Can also be used as shown below @skipna def transform(x): pass ``` :param f: the function to be wrapped. :returns: the wrapped function. \"\"\" def _wrapped(x, *args, **kwargs): if (type(x) is float and np.isnan(x)) or x is None: return np.nan return f(x, *args, **kwargs) return _wrapped","title":"skipna()"},{"location":"api/xarray/","text":"XArray Functions to augment XArray DataArrays and Datasets with additional functionality. clone_using(da, np_arr, use_coords=True, use_attrs=False, new_name=None) Given a NumPy array, return an XArray DataArray which contains the same dimension names and (optionally) coordinates and other properties as the supplied DataArray . This is similar to xr.DataArray.copy() with more specificity for the type of cloning you would like to perform - the different properties that you desire to mirror in the new DataArray . If the coordinates from the source DataArray are not desired, the shape of the source and new NumPy arrays don't need to match. The number of dimensions do, however. Usage example - making a new DataArray from a previous one, keeping the dimension names but dropping the coordinates (the input NumPy array is of a different size): .. code-block:: python da = xr.DataArray( np.zeros((512, 512)), dims=['ax_1', 'ax_2'], coords=dict(ax_1=np.linspace(0, 1, 512), ax_2=np.logspace(-2, 2, 1024)), name='original' ) new_da = da.clone_using(np.ones((4, 6)), new_name='new_and_improved', use_coords=False) Parameters: Name Type Description Default da DataArray The DataArray supplied by the method itself. required np_arr <built-in function array> The NumPy array which will be wrapped in a new DataArray given the properties copied over from the source DataArray . required use_coords bool If True , use the coordinates of the source DataArray for the coordinates of the newly-generated array. Shapes must match in this case. If False , only the number of dimensions must match. True use_attrs bool If True , copy over the attrs from the source DataArray . The data inside attrs itself is not copied, only the mapping. Otherwise, use the supplied attrs. False new_name str If set, use as the new name of the returned DataArray . Otherwise, use the name of `da``. None Returns: Type Description DataArray A DataArray styled like the input DataArray containing the NumPy array data. Exceptions: Type Description ValueError if number of dimensions in NumPy array and DataArray do not match. ValueError if shape of NumPy array and DataArray do not match. Source code in janitor/xarray/functions.py @register_xarray_dataarray_method def clone_using( da: xr.DataArray, np_arr: np.array, use_coords: bool = True, use_attrs: bool = False, new_name: str = None, ) -> xr.DataArray: \"\"\" Given a NumPy array, return an XArray `DataArray` which contains the same dimension names and (optionally) coordinates and other properties as the supplied `DataArray`. This is similar to `xr.DataArray.copy()` with more specificity for the type of cloning you would like to perform - the different properties that you desire to mirror in the new `DataArray`. If the coordinates from the source `DataArray` are not desired, the shape of the source and new NumPy arrays don't need to match. The number of dimensions do, however. Usage example - making a new `DataArray` from a previous one, keeping the dimension names but dropping the coordinates (the input NumPy array is of a different size): .. code-block:: python da = xr.DataArray( np.zeros((512, 512)), dims=['ax_1', 'ax_2'], coords=dict(ax_1=np.linspace(0, 1, 512), ax_2=np.logspace(-2, 2, 1024)), name='original' ) new_da = da.clone_using(np.ones((4, 6)), new_name='new_and_improved', use_coords=False) :param da: The `DataArray` supplied by the method itself. :param np_arr: The NumPy array which will be wrapped in a new `DataArray` given the properties copied over from the source `DataArray`. :param use_coords: If `True`, use the coordinates of the source `DataArray` for the coordinates of the newly-generated array. Shapes must match in this case. If `False`, only the number of dimensions must match. :param use_attrs: If `True`, copy over the `attrs` from the source `DataArray`. The data inside `attrs` itself is not copied, only the mapping. Otherwise, use the supplied attrs. :param new_name: If set, use as the new name of the returned `DataArray`. Otherwise, use the name of `da``. :return: A `DataArray` styled like the input `DataArray` containing the NumPy array data. :raises ValueError: if number of dimensions in `NumPy` array and `DataArray` do not match. :raises ValueError: if shape of `NumPy` array and `DataArray` do not match. \"\"\" if np_arr.ndim != da.ndim: raise ValueError( \"Number of dims in the NumPy array and the DataArray \" \"must match.\" ) if use_coords and not all( np_ax_len == da_ax_len for np_ax_len, da_ax_len in zip(np_arr.shape, da.shape) ): raise ValueError( \"Input NumPy array and DataArray must have the same \" \"shape if copying over coordinates.\" ) return xr.DataArray( np_arr, dims=da.dims, coords=da.coords if use_coords else None, attrs=da.attrs.copy() if use_attrs else None, name=new_name if new_name is not None else da.name, ) convert_datetime_to_number(da_or_ds, time_units, dim='time') Convert the coordinates of a datetime axis to a human-readable float representation. Usage example to convert a DataArray 's time dimension coordinates from a datetime to minutes: .. code-block:: python timepoints = 60 da = xr.DataArray( np.random.randint(0, 10, size=timepoints), dims='time', coords=dict(time=np.arange(timepoints) * np.timedelta64(1, 's')) ) da_minutes = da.convert_datetime_to_number('m', dim='time) Parameters: Name Type Description Default da_or_ds Union[xarray.core.dataarray.DataArray, xarray.core.dataset.Dataset] XArray object. required time_units str Numpy timedelta string specification for the unit you would like to convert the coordinates to. required dim str the time dimension whose coordinates are datetime objects. 'time' Returns: Type Description The original XArray object with the time dimension reassigned. Source code in janitor/xarray/functions.py @register_xarray_dataset_method @register_xarray_dataarray_method def convert_datetime_to_number( da_or_ds: Union[xr.DataArray, xr.Dataset], time_units: str, dim: str = \"time\", ): \"\"\" Convert the coordinates of a datetime axis to a human-readable float representation. Usage example to convert a `DataArray`'s time dimension coordinates from a `datetime` to minutes: .. code-block:: python timepoints = 60 da = xr.DataArray( np.random.randint(0, 10, size=timepoints), dims='time', coords=dict(time=np.arange(timepoints) * np.timedelta64(1, 's')) ) da_minutes = da.convert_datetime_to_number('m', dim='time) :param da_or_ds: XArray object. :param time_units: Numpy timedelta string specification for the unit you would like to convert the coordinates to. :param dim: the time dimension whose coordinates are datetime objects. :return: The original XArray object with the time dimension reassigned. \"\"\" times = da_or_ds.coords[dim].data / np.timedelta64(1, time_units) return da_or_ds.assign_coords({dim: times})","title":"XArray"},{"location":"api/xarray/#xarray","text":"Functions to augment XArray DataArrays and Datasets with additional functionality.","title":"XArray"},{"location":"api/xarray/#janitor.xarray.functions.clone_using","text":"Given a NumPy array, return an XArray DataArray which contains the same dimension names and (optionally) coordinates and other properties as the supplied DataArray . This is similar to xr.DataArray.copy() with more specificity for the type of cloning you would like to perform - the different properties that you desire to mirror in the new DataArray . If the coordinates from the source DataArray are not desired, the shape of the source and new NumPy arrays don't need to match. The number of dimensions do, however. Usage example - making a new DataArray from a previous one, keeping the dimension names but dropping the coordinates (the input NumPy array is of a different size): .. code-block:: python da = xr.DataArray( np.zeros((512, 512)), dims=['ax_1', 'ax_2'], coords=dict(ax_1=np.linspace(0, 1, 512), ax_2=np.logspace(-2, 2, 1024)), name='original' ) new_da = da.clone_using(np.ones((4, 6)), new_name='new_and_improved', use_coords=False) Parameters: Name Type Description Default da DataArray The DataArray supplied by the method itself. required np_arr <built-in function array> The NumPy array which will be wrapped in a new DataArray given the properties copied over from the source DataArray . required use_coords bool If True , use the coordinates of the source DataArray for the coordinates of the newly-generated array. Shapes must match in this case. If False , only the number of dimensions must match. True use_attrs bool If True , copy over the attrs from the source DataArray . The data inside attrs itself is not copied, only the mapping. Otherwise, use the supplied attrs. False new_name str If set, use as the new name of the returned DataArray . Otherwise, use the name of `da``. None Returns: Type Description DataArray A DataArray styled like the input DataArray containing the NumPy array data. Exceptions: Type Description ValueError if number of dimensions in NumPy array and DataArray do not match. ValueError if shape of NumPy array and DataArray do not match. Source code in janitor/xarray/functions.py @register_xarray_dataarray_method def clone_using( da: xr.DataArray, np_arr: np.array, use_coords: bool = True, use_attrs: bool = False, new_name: str = None, ) -> xr.DataArray: \"\"\" Given a NumPy array, return an XArray `DataArray` which contains the same dimension names and (optionally) coordinates and other properties as the supplied `DataArray`. This is similar to `xr.DataArray.copy()` with more specificity for the type of cloning you would like to perform - the different properties that you desire to mirror in the new `DataArray`. If the coordinates from the source `DataArray` are not desired, the shape of the source and new NumPy arrays don't need to match. The number of dimensions do, however. Usage example - making a new `DataArray` from a previous one, keeping the dimension names but dropping the coordinates (the input NumPy array is of a different size): .. code-block:: python da = xr.DataArray( np.zeros((512, 512)), dims=['ax_1', 'ax_2'], coords=dict(ax_1=np.linspace(0, 1, 512), ax_2=np.logspace(-2, 2, 1024)), name='original' ) new_da = da.clone_using(np.ones((4, 6)), new_name='new_and_improved', use_coords=False) :param da: The `DataArray` supplied by the method itself. :param np_arr: The NumPy array which will be wrapped in a new `DataArray` given the properties copied over from the source `DataArray`. :param use_coords: If `True`, use the coordinates of the source `DataArray` for the coordinates of the newly-generated array. Shapes must match in this case. If `False`, only the number of dimensions must match. :param use_attrs: If `True`, copy over the `attrs` from the source `DataArray`. The data inside `attrs` itself is not copied, only the mapping. Otherwise, use the supplied attrs. :param new_name: If set, use as the new name of the returned `DataArray`. Otherwise, use the name of `da``. :return: A `DataArray` styled like the input `DataArray` containing the NumPy array data. :raises ValueError: if number of dimensions in `NumPy` array and `DataArray` do not match. :raises ValueError: if shape of `NumPy` array and `DataArray` do not match. \"\"\" if np_arr.ndim != da.ndim: raise ValueError( \"Number of dims in the NumPy array and the DataArray \" \"must match.\" ) if use_coords and not all( np_ax_len == da_ax_len for np_ax_len, da_ax_len in zip(np_arr.shape, da.shape) ): raise ValueError( \"Input NumPy array and DataArray must have the same \" \"shape if copying over coordinates.\" ) return xr.DataArray( np_arr, dims=da.dims, coords=da.coords if use_coords else None, attrs=da.attrs.copy() if use_attrs else None, name=new_name if new_name is not None else da.name, )","title":"clone_using()"},{"location":"api/xarray/#janitor.xarray.functions.convert_datetime_to_number","text":"Convert the coordinates of a datetime axis to a human-readable float representation. Usage example to convert a DataArray 's time dimension coordinates from a datetime to minutes: .. code-block:: python timepoints = 60 da = xr.DataArray( np.random.randint(0, 10, size=timepoints), dims='time', coords=dict(time=np.arange(timepoints) * np.timedelta64(1, 's')) ) da_minutes = da.convert_datetime_to_number('m', dim='time) Parameters: Name Type Description Default da_or_ds Union[xarray.core.dataarray.DataArray, xarray.core.dataset.Dataset] XArray object. required time_units str Numpy timedelta string specification for the unit you would like to convert the coordinates to. required dim str the time dimension whose coordinates are datetime objects. 'time' Returns: Type Description The original XArray object with the time dimension reassigned. Source code in janitor/xarray/functions.py @register_xarray_dataset_method @register_xarray_dataarray_method def convert_datetime_to_number( da_or_ds: Union[xr.DataArray, xr.Dataset], time_units: str, dim: str = \"time\", ): \"\"\" Convert the coordinates of a datetime axis to a human-readable float representation. Usage example to convert a `DataArray`'s time dimension coordinates from a `datetime` to minutes: .. code-block:: python timepoints = 60 da = xr.DataArray( np.random.randint(0, 10, size=timepoints), dims='time', coords=dict(time=np.arange(timepoints) * np.timedelta64(1, 's')) ) da_minutes = da.convert_datetime_to_number('m', dim='time) :param da_or_ds: XArray object. :param time_units: Numpy timedelta string specification for the unit you would like to convert the coordinates to. :param dim: the time dimension whose coordinates are datetime objects. :return: The original XArray object with the time dimension reassigned. \"\"\" times = da_or_ds.coords[dim].data / np.timedelta64(1, time_units) return da_or_ds.assign_coords({dim: times})","title":"convert_datetime_to_number()"}]}